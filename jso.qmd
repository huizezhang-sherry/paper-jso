---
title: New Metrics for Assessing Projection Pursuit Indexes, and Guiding Optimisation Choices
author:
  - name: H. Sherry Zhang
    email: huize.zhang@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Austin
          country: United States
          postal-code: 78751
    attributes:
        corresponding: true
  - name: Dianne Cook
    email: dicook@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
  - name: Nicolas  Langrené
    email: nicolaslangrene@uic.edu.cn
    affiliations:
        - id: 3
          name: Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International College
          department: Department of Mathematical Sciences
          city: Zhuhai
          country: China
          postal-code: 519087
  - name: Jessica Wai Yin Leung
    email: Jessica.Leung@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
abstract: |
  The projection pursuit (PP) guided tour interactively optimises a criterion function known as the PP index, to explore high-dimensional data by revealing interesting projections. Optimisation of some PP indexes can be non-trivial, if they are non-smooth functions, or the optimum has a small "squint angle", detectable only from close proximity. To address these challenges, this study investigates the performance of a recently introduced swarm-based algorithm, Jellyfish Search Optimiser (JSO), for optimising PP indexes. The performance of JSO for visualising data is evaluated across various hyper-parameter settings and compared with existing optimisers. Additionally, methods for calculating the smoothness and squintability properties of the PP index are proposed. They are used to assess the optimiser performance in the presence of PP index complexities. A simulation study illustrates the use of these performance metrics to compare the JSO with existing optimisation methods available for the guided tour. The JSO algorithm has been implemented in the R package, `tourr`, and functions to calculate smoothness and squintability are available in the `ferrn` package. 
keywords: 
  - projection pursuit
  - jellyfish search optimiser (JSO)
  - optimisation
  - grand tour
  - high-dimensional data
  - exploratory data analysis
date: last-modified
bibliography: bibliography.bib
format:
  tandf-pdf:
    keep-tex: true
    fontsize: 12pt
    linestretch: 2
editor: 
  markdown: 
    wrap: 72
crossref: 
  eq-prefix: ""
editor_options: 
  chunk_output_type: console
header-includes:
   - \usepackage{algorithm}
   - \usepackage{float} 
   - \usepackage{amsmath}
   - \usepackage{amsthm}
   - \usepackage{amssymb}
   - \theoremstyle{plain}
   - \newtheorem{defn}{\protect\definitionname}
   - \newtheorem{prop}{\protect\propositionname}
   - \providecommand{\definitionname}{Definition}
   - \providecommand{\propositionname}{Proposition}
nocite: |
   @DLMF
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(ggh4x)
library(broom)
library(kableExtra)
library(tourr)
library(PPtreeViz)
library(ferrn)
library(colorspace)
load(here::here("data/sim_pipe_run_best.rda"))
load(here::here("data/sim_sine_6d_spline_head.rda"))
load(here::here("data/sim_sine_6d_spline_best.rda"))
load(here::here("data/sim_sine_6d_spline_projdist.rda"))
load(here::here("data/pipe_better.rda"))
load(here::here("data/pipe_jellyfish.rda"))
load(here::here("data/smoothness.rda"))
load(here::here("data/squintability.rda"))
load(here::here("data/sq_basis_dist_idx.rda"))
load(here::here("data/sim_df.rda"))
```

# Introduction

Projection pursuit (PP) (@kr69, @FT74, @huber85) is a dimension
reduction technique aimed at identifying informative linear projections
of data. This is useful for exploring high-dimensional data, and
creating plots of the data that reveal the main features to use for
publication. The method involves optimising an objective function known
as the PP index (e.g. @hall1989polynomial, @cook1993projection,
@lee2010projection, @Loperfido2018, @Loperfido2020), which defines the
criterion for what constitutes interesting or informative projections.
Let $X \in \mathbb{R}^{n\times p}$ be the data matrix,
$A \in\mathbb{R}^{p \times d}$ be an orthonormal matrix, where $A$
belongs to the Stiefel manifold $\mathcal{A} = V_d(\mathbb{R}^p)$. The
projection $Y = XA$ is a linear transformation that maps data from a
$p$-dimensional space into a $d$-dimensional space. The index function
$f(XA): \mathbb{R}^{n \times d} \to \mathbb{R}$ is a scalar function
that measures an interesting aspect of the projected data, such as
deviation from normality, presence of clusters, or non-linear structure.
For a fixed sample of data, PP finds the orthonormal basis $A$ that
maximises the index value of the projection, $Y = XA$:

$$
\underset{A \in \mathcal{A}}{\max } \quad f(XA) \quad \text{subject to} \quad A'A = I_d
$$ {#eq-optimization}

It is interesting to note that when using PP visually, one cares less
about $A$ than the plane described by $A$, because the orientation in
the plane is irrelevant. The space of planes belongs to a Grassmann
manifold. This is usually how the projection pursuit guided tour (PPGT)
[@cook1995grand] operates, when using geodesic interpolation between
starting and target planes. It interpolates plane to plane, removing
irrelevant within plane spin, and is agnostic to the basis ($A$) used to
define the plane. Thus, indexes which are used for the PPGT should be
rotationally invariant.

Index functions are quite varied in form, partially depending on the
data that is being projected. @fig-example-functions shows two examples.
Huber plots [@huberplot] of 2D data sets are in (a) and (c), showing the
PP index values for all 1D projections of the 2D data in polar
coordinates, which reveals the form of these functions. The dashed
circle is a baseline set at the average value, and the straight line
marks the optimal projection. Plots (b) and (d) show the respective best
projections of the data as histograms. Indexes like the holes, central
mass and skewness [@cook1993projection] are generally smooth for most
data sets, but capture only large patterns. Many indexes are noisy and
non-convex, requiring an effective and efficient optimisation procedure
to explore the data landscape and achieve a globally optimal viewpoint
of the data. The skewness index computed for trimodal data, in (a), is
smooth with a large squint angle but has three modes, and thus is not
convex. The binned normality index (a simple version of a non-normality
index as described in @huber85) computed on the famous RANDU data, in
(c), is noisier and has a very small squint angle. The discreteness
cannot be seen unless the optimiser is very close to the optimal
projection.

```{r}
#| label: fig-example-functions
#| echo: false
#| fig-width: 9
#| fig-height: 6
#| out-width: 80%
#| fig-cap: "Examples of PP indexes with large (top row) and small (bottom row) squint angles, shown with a Huber plot, and histogram of the projected data corresponding to the optimal projection. A Huber plot shows the PP index values for all 1D data projections in polar coordinates."
#proj <- animate_xy(flea[,1:6], guided_tour(lda_pp(flea$species)))
best_proj <- matrix(c(-0.44157005,  0.80959831,
                      0.14622089, -0.08228193,
                      -0.08454314, -0.23234287,
                      0.70367852,  0.28185021,
                      0.27876825,  0.44819691,
                      0.45123453,  0.05896646), ncol=2, byrow=T)

flea2D <- as.matrix(flea[,1:6]) %*% best_proj
colnames(flea2D) <- c("X1", "X2")
flea2D <- as.data.frame(flea2D)
flea_huber <- prep_huber(flea2D, index = skewness())
sqa1 <- ggplot() +
  geom_huber(data = flea_huber$idx_df, aes(x = x, y = y)) +
  geom_point(data = flea2D, aes(x = X1, y = X2)) +
  geom_abline(slope = flea_huber$slope, intercept = 0) + 
  coord_fixed() +
  theme_huber() +
  ggtitle("(a) Skewness index")

sqa2 <- ggplot(flea_huber$proj_df, aes(x=x)) + 
  geom_histogram(binwidth=0.25, colour="white") +
  xlab("") + ylab("") +
  ggtitle("(b) Optimally projected data") +
  theme_bw() +
  theme(axis.text.y = element_blank())

data(randu)
randu_std <- as.data.frame(apply(randu, 2, function(x) (x-mean(x))/sd(x)))
randu_std$yz <- sqrt(35)/6*randu_std$y-randu_std$z/6
randu_df <- randu_std[c(1,4)]
randu_huber <- prep_huber(randu_df, index = norm_bin(nr = nrow(randu_df)))

sqa3 <- ggplot() +
  geom_huber(data = randu_huber$idx_df, aes(x = x, y = y)) +
  geom_point(data = randu_df, aes(x = x, y = yz)) +
  geom_abline(slope = randu_huber$slope, intercept = 0) +
  theme_huber() +
  coord_fixed() + 
  ggtitle("(c) Binned normality index")

sqa4 <- ggplot(randu_huber$proj_df, aes(x = x)) +
  geom_histogram(breaks = seq(-2.2, 2.4, 0.12)) +
  xlab("") + ylab("") +
  theme_bw() +
  theme(axis.text.y = element_blank()) + 
  ggtitle("(d) Optimally projected data")

sqa1 + sqa2 + sqa3 + sqa4 + plot_layout(ncol=2, widths=c(2,2))

```

Optimisation of PP is often discussed when new indexes are proposed
[@posse95; @marie-sainte2010; @grochowski2011]. @cook1995grand tied the
optimisation more closely to the index, when they introduced the PPGT,
which monitors the optimisation visually so that the user can see the
projected data leading in and out of the optimum. An implementation is
available in the `tourr` package [@tourr] in R [@R]. @RJ-2021-105
illustrated how to diagnose optimisation processes, particularly
focusing on the guided tour, and revealed a need for improved
optimisation. While improving the quality of the optimisation solutions
in the tour is essential, it is also important to be able to view the
data projections as the optimisation progresses. Integrating the guided
tour with a global optimisation algorithm that is efficient in finding
the global optimal and enables viewing of the projected data during the
exploration process is a goal.

Here, the potential for a Jellyfish Search Optimiser (JSO) (see
@chou_novel_2021, and @rajwar_exhaustive_2023) for the PPGT is explored.
JSO, inspired by the search behaviour of jellyfish in the ocean, is a
swarm-based metaheuristic designed to solve global optimisation
problems. Compared to traditional methods, JSO has demonstrated stronger
search ability and faster convergence, and requires fewer tuning
parameters. These practicalities make JSO a promising candidate for
enhancing PP optimisation.

The primary goal of the study reported here is to investigate the
performance of JSO in PP optimisation for the guided tour. It is of
interest to assess how quickly and closely the optimiser reaches a
global optimum, for various PP indexes that may have differing
complexities. To observe the performance of JSO with different types of
PP indexes, metrics are introduced to capture specific properties of the
index including squintability (based on @barnett1981interpreting's
squint angle) and smoothness. Here, we mathematically define metrics for
squintability and smoothness, which is a new contribution for PP
research. A series of simulation experiments using various datasets and
PP indexes are conducted to assess JSO's behaviour and its sensitivity
to hyper-parameter choices (number of jellyfish and maximum number of
tries). The relationship between the JSO performance, hyper-parameter
choices and properties of PP indexes (smoothness and squintability) is
analysed to provide guidance on selecting optimisers for practitioners
using projection pursuit. Additionally, this work should guide the
design of new PP indexes and facilitate better optimization for PP.

The paper is structured as follows. @sec-background introduces the
background of the PPGT, reviews existing optimisers and index functions
in the literature. @sec-PP-properties introduces the metrics that
measure two properties of PP indexes, smoothness and squintability, and
@sec-JSO describes the new JSO to be used for the PPGT. @sec-sim-deets
outlines two simulation experiments to assess JSO's performance: one
comparing JSO's performance improvements relative to an existing
optimiser, Creeping Random Search (CRS), and the other studying the
impact of PP index properties on optimisation performance, and
@sec-sim-res presents the results. @sec-discussion discusses the
implementation of JSO in the `tourr` package and the PP property
calculation in the `ferrn` package. @sec-conclusion summarises the work
and provides suggestions for future directions.

# Projection pursuit, tours, index functions and optimisation {#sec-background}

A tour on high-dimensional data is constructed by geodesically
interpolating between pairs of planes. Any plane is described by an
orthonormal basis, $A_t$, where $t$ represents time in the sequence. The
term "geodesic" refers to maintaining the orthonormality constraint so
that each view shown is correctly a projection of the data. The PP
guided tour operates by geodesically interpolating to target planes
(projections) which have high PP index values, as provided by the
optimiser. The geodesic interpolation means that the viewer sees a
continuous sequence of projections of the data, so they can watch
patterns of interest forming as the function is optimised. There are
five optimisation methods implemented in the `tourr` package:

-   a pseudo-derivative, that searches locally for the best direction,
    based on differencing the index values for very close projections.
-   a brute-force optimisation (CRS).
-   a modified brute force algorithm described in @posse95.
-   an essentially simulated annealing [@Bertsimas93] where the search
    space is reduced during the optimisation.
-   a very localised search, to take tiny steps to get closer to the
    local maximum.

<!-- 
-   `search_geodesic()`: provides a pseudo-derivative optimisation. It
    searches locally for the best direction, based on differencing the
    index values for very close projections. Then it follows the
    direction along the geodesic path between planes, stopping when the
    next index value fails to increase.
-   `search_better()`: also known as Creeping Random Search (CRS), is a
    brute-force optimisation searching randomly for projections with
    higher index values.
-   `search_better_random()`: is essentially simulated annealing
    [@Bertsimas93] where the search space is reduced as the optimisation
    progresses.
-   `search_posse()`: implements the algorithm described in @posse95.
-   `search_polish()`: is a very localised search, to take tiny steps to
    get closer to the local maximum.
-->

There are numerous PP index functions available: introduced in @huber85,
@cook1993projection, @lee2005projection, @lee2010projection, @Grimm2016,
@Laa:2020wkm. Most are relatively simply defined, for any projection
dimension, and implemented because they are relatively easy to optimise.
A goal is to develop PP indexes based on scagnostics (@scag, @WW08), but
the blockage is their optimisation as these tend to be noisy, with
potentially small squint angles.

An initial investigation of PP indexes, and the potential for
scagnostics is described in @laa_using_2020. To be useful here an
optimiser needs to be able to handle index functions that are possibly
not very smooth. In addition, because data structures might be
relatively fine, the optimiser needs to be able to find maxima that
occur with a small squint angle, that can only be seen from very close
by. One last aspect that is useful is for an optimiser to return local
maxima in addition to the global one because data can contain many
different and interesting features.

# Properties of PP indexes {#sec-PP-properties}

@laa_using_2020 has proposed five criteria for assessing projection
pursuit indexes (smoothness, squintability, flexibility, rotation
invariance, and speed). Since not all index properties affect the
optimisation process, the focus here is on the first two properties,
*smoothness* (@sec-smoothness) and *squintability* (@sec-squintability),
for which metrics are proposed to quantify them.

## Smoothness {#sec-smoothness}

This subsection proposes a metric for the smoothness of a projection
pursuit index.

A classical way to describe the smoothness of a function is to identify
how many continuous derivatives of the function exist. This can be
characterized by Sobolev spaces [@adams2003sobolev].

\begin{defn}[sobolev space]\label{def:sobolev_space}
The Sobolev space $W^{k,p}(\mathbb{R})$ for $1\leq p\leq \infty$ is the set of all functions $f$ in $L^p(\mathbb{R})$ for which all weak derivatives $f^{(\ell)}$ of order $\ell\leq k$ exist and have a finite $L^p$ norm.
\end{defn}

The Sobolev index $k$ in Definition \ref{def:sobolev_space} can be used
to characterize the smoothness of a function: if $f\in W^{k,p}$, then
the higher $k$, the smoother $f$. While this Sobolev index $k$ is a
useful measure of smoothness, it can be difficult to compute or even
estimate in practice.

To obtain a computable estimator of the smoothness of the index function
$f$, we propose an approach based on random fields. If a PP index
function $f$ is evaluated at some random bases, as is done at the
initialization stage of JSO, then these random index values can be
interpreted as a random field, indexed by a space parameter, namely the
random projection basis. This analogy suggests to use this random
training sample to fit a spatial model. We propose to use a Gaussian
process equipped with a Matérn covariance function, due to the
connections between this model and Sobolev spaces, see for example
@porcu2024matern.

The distribution of a Gaussian process is fully determined by its mean
and covariance function. The smoothness property comes into play in the
definition of the covariance function: if a PP index is very smooth,
then two close projection bases should produce close index values
(strong correlation); by contrast, if a PP index is not very smooth,
then two close projection bases might give very different index values
(fast decay of correlations with respect to distance between bases).
Popular covariance functions are parametric positive semi-definite
functions. In particular, the Matérn class of covariance functions has a
dedicated parameter to capture the smoothness of the Gaussian field.

\begin{defn}[Matérn covariance function]\label{def:matern}
The Matérn covariance function $K$ is defined by
\begin{equation}
K(u)=K_{\nu,\eta,\ell}(u):=\eta^2\frac{\left(\sqrt{2\nu}\frac{\left\Vert u\right\Vert}{\ell}\right)^{\nu}}{\Gamma(\nu)2^{\nu-1}}\mathcal{K}_{\nu}\left(\sqrt{2\nu}\frac{\left\Vert u\right\Vert}{\ell}\right)\ ,\label{eq:matern}
\end{equation}
where $\left\Vert u\right\Vert$ is the Euclidean norm of $u\in\mathbb{R}^{p{\times}d}$, $\nu>0$ is the smoothness parameter, $\eta$ is the outputscale, $\ell$ is the lengthscale, and $\mathcal{K}_\nu$ is
the modified Bessel function [DLMF 10.25].
\end{defn}

The Matérn covariance function can be expressed analytically when $\nu$
is a half-integer, the most popular values in the literature being
$\frac{1}{2}$, $\frac{3}{2}$ and $\frac{5}{2}$ [@rasmussen2006gaussian].
The parameter $\nu$, called *smoothness parameter*, controls the decay
of the covariance function. As such, it is an appropriate measure of
smoothness of a random field, as shown by the simulations on
@fig-matern-1d and @fig-matern-2d. For example, @karvonen2023asymptotic
showed that if a function $f$ has a Sobolev index of $k$, then the
smoothness parameter estimate $\nu$ in \eqref{eq:matern} cannot be
asymptotically less than $k$. See the survey @porcu2024matern for
additional results on the connection between the Matérn model and
Sobolev spaces. An interesting result is that the asymptotic case
$\nu\rightarrow\infty$ coincides with the Gaussian kernel:
$K_\infty(u)=\exp(-{\left\Vert u\right\Vert}^{2}/2)$.

```{r}
#| label: fig-matern-1d
#| out.width: 100%
#| fig.cap: |
#|   Five random simulations from a Gaussian Process defined on $\mathbb{R}$ with zero mean and Matérn-$\nu$ covariance function, with $\nu=1$ (left), $\nu=2$ (middle), and $\nu=4$ (right), showing that higher values of $\nu$ produce smoother curves.
knitr::include_graphics(here::here("figures", "matern_simulation_1d.png"))
```

```{r}
#| label: fig-matern-2d
#| out.width: 100%
#| fig.cap: |
#|   One random simulation from a Gaussian Process defined on $\mathbb{R}^2$ with zero mean and Matérn-$\nu$ covariance function, with $\nu=1$ (left), $\nu=2$ (middle), and $\nu=4$ (right), showing that higher values of $\nu$ produce smoother surfaces.
knitr::include_graphics(here::here("figures", "matern_simulation_2d.png"))
```

In view of these results, the parameter $\nu$ is suggested as a measure
of the smoothness of the PP index function by fitting a Gaussian process
prior with Matérn covariance on a dataset generated by random
evaluations of the index function, as done at the initialization stage
of the jellyfish search optimization. There exist several R packages,
such as `GpGp` [@guinness2021gpgp] or `ExaGeoStatR` [@abdulah2023large],
to fit the hyper-parameters of a GP covariance function on data, which
is usually done by maximum likelihood estimation. In this project, the
`GpGp` package is used.

\begin{defn}
Let $\mathbf{A}=[A_1, \ldots, A_N] \in (\mathbb{R}^{p \times d})^N$ be d-dimensional projection bases, let $\mathbf{y}=[f(XA_1),\ldots,f(XA_N)]$ be the corresponding PP index values, and let $\mathbf{K}=[K_\theta(A_{i}-A_{j})]_{1\leq i,j\leq N}\in\mathbb{R}^{N\times N}$ be the Matérn covariance matrix evaluated at the input bases, where the vector $\theta$ contains all the parameters of the multivariate Matérn covariance function $K$ (smoothness, outputscale, lengthscales). The log-likelihood of the parameters $\theta$ is defined by 
\begin{equation}
\mathcal{L}(\theta)=\log p(\mathbf{y}\left|\mathbf{A},\theta\right.)=-\frac{1}{2}\mathbf{y}^{\top}(\mathbf{K}+\sigma^{2}\mathbf{I})^{-1}\mathbf{y}-\frac{1}{2}\mathrm{\log}(\det(\mathbf{K}+\sigma^{2}\mathbf{I}))-\frac{N}{2}\log(2\pi)\, \label{eq:gp_log_likelihood}
\end{equation}
where the nugget parameter $\sigma$ is the standard deviation of the intrinsic noise of the Gaussian process.
The optimal parameters (including smoothness) are obtained by maximum log-likelihood
\begin{equation}
\theta^* = \underset{\theta}{\max}\mathcal{L}(\theta)
\end{equation}
The resulting optimal smoothness parameter $\nu$ is chosen as our smoothness metric.
\end{defn}

The value of the optimal smoothness parameter $\nu>0$ can be naturally
interpreted as follows: the higher $\nu$, the smoother the index
function.

## Squintability {#sec-squintability}

This section defines the projection distance and introduces the
squintability metric, followed by a description of two numerical
approaches for computing squintability.

\begin{defn}[projection distance]\label{def:proj-dist}
Let $A \in \mathbb{R}^{p \times d}$ is a $d$-dimensional orthonormal matrix, and let $A^*$ be the optimal matrix that achieves the maximum index value for a given data. The projection distance between $A$ 
and $A^*$, $r(A, A^*)$, is defined by
$r(A, A^*) = \lVert AA^\prime - A^*A^{*\prime}\,\rVert _F$
where $\lVert . \rVert _F$ denotes the Frobenius norm, given by
$\lVert M \rVert _F = \sqrt{\sum_{ij} M_{ij}^2}$. 
\end{defn}
\begin{defn}[squint angle]\label{def:squint-angle}
 
Let $A$ and $B$ be two $d$-dimensional orthonormal matrices in $\mathbb{R}^p$. The squint angle $\theta$ between the subspace spanned by $A$ and $B$ is defined as the smallest principal angle between these subspaces: $\theta = \min_{i \in \{1, \cdots, d\}} \arccos(\tau_i)$, where $\tau_i$ are the singular values of the matrix $M = A^T B$ obtained from its singular value decomposition.
\end{defn}

Squintability can be defined as how the index value $f(XA)$ changes with
respect to the projection distance $r(A, A^*)$, over the course of the
JSO:

\begin{defn}[squintability]\label{def:squintability}
Let $g: \mathbb{R} \mapsto  \mathbb{R}$ be a decreasing function that maps the projection distance $r(A, A^*)$ to the index value $f(XA)$, such that $g(r) = g(r(A, A^*)) = f(XA)$.  The squintability of an index function $f$ is defined by 

\begin{equation}
\varsigma(f) = -4 \times \min_{r} g'(r) \times \arg \min_{r} g'(r)
\label{eq-squintability}
\end{equation}

where $4$ is a constant scaling factor, $-\min_r g'(r)$ represents the 
largest gradient of $-g$ and $\arg \min_{r} g'(r)$ represents the projection distance at which this largest gradient is attained. If the set $\mathcal{D}:=\arg \min_{r} g'(r)$ contains more than one value $r$, the largest value $\max(\mathcal{D})$ is used in equation \eqref{eq-squintability}.

\end{defn}

It is expected that these two values should be both high in the case of
high squintability (fast increase in $g$ early on), and both low in the
case of low squintability (any substantial increase in $g$ happens very
late, close to the optimal angle). This suggests that their product
\eqref{eq-squintability} should provide a sensible measure of
squintability. The multiplicative constant $4$, which can be deemed
arbitrary, does not change the interpretation of the squintability
metric $\varsigma$; it is here to adjust the range of values of
$\varsigma$ and simplify the explicit formula for $\varsigma$ obtained
later on.

From the definition, the following proposition can be derived:

\begin{prop}\label{prop:convex-concave}
Let $g_1$ be a convex, strictly decreasing function and $g_2$ be a concave, strictly decreasing function, both defined on $[0, R]$ where $R>0$, and taking values in $[0, 1]$. Then
$$\varsigma(g_1) < \varsigma(g_2)$$
\end{prop}
\begin{proof}
Let $r_1 := \underset{r}{\arg \min }\,g_1'(r)$ and $r_2 := \underset{r}{\arg \min }\,g_2'(r)$.
Since $g_1$ is convex, $g_1''(r) \geq 0 \; \forall r \in [0, R]$, meaning that $g_1'$ is an increasing function, taking its minimum for $r_1=0$. As a result, $\varsigma(g_1)=-4 \times g'(0) \times 0 =0$.
Since $g_2$ is concave, $g_2''(r) \leq 0 \; \forall r \in [0, R]$, meaning that $g_2'$ is a decreasing function, taking its minimum for $r_2=D>0$. Since $g_2$ is strictly decreasing, $g_2'(r) < 0 \; \forall r \in [0, R]$. In particular $g_2'(R)<0$. As a result $\varsigma(g_2)=-4 \times g'(R) \times R >0$. This proves that $\varsigma(g_1) < \varsigma(g_2)$.
\end{proof}

From @barnett1981interpreting and @laa_using_2020, a large squint angle
implies that the objective function value is close to optimal even when
the perfect view to see the structure is far away. A small squint angle
means that the PP index value improves substantially only when the
perfect view is close by. As such, low squintability implies rapid
improvement in the index value when near the perfect view. For PP, a
small squint angle is considered to be undesirable because it means that
the optimiser needs to be very close to be able to "see" the optimum.
Thus, it could be difficult for the optimiser to find the optimum.
<!--The
mathematical formulation of this intuition is proposed below:-->

It is expected that for a PP index with high squint angle, the
optimization (@eq-optimization) should make substantial progress early
on. Conversely, for a PP index with low squint angle, it might take a
long while for the optimization to make substantial progress, as the
candidate projections would need to be very close to the optimal one for
the structure of the index function to be visible enough to be amenable
to efficient optimization. This observation suggests that the extreme
values of $g'$ and the projection distances for which these values are
attained, are crucial in the mathematical definition of squintability.

To compute the squintability metric \eqref{eq-squintability} in
practice, several approaches are possible. The first one is to propose a
parametric model for $g$, and use it to obtain an explicit formula for
$\varsigma$. Numerical experiments suggest a scaled sigmoid shape as
described below. Define $$
\ell(x):=\frac{1}{1+\exp(\theta_{3}(x-\theta_{2}))}\ ,
$$ {#eq-logistic}

which is a decreasing logistic function depending on two parameters
$\theta_2$ and $\theta_3$, such that $\ell(\theta_{2})=\frac{1}{2}$.
Then, define $$
g(x)=(\theta_{1}-\theta_{4})\frac{\ell(x)-\ell(x_{\max})}{\ell(0)-\ell(x_{\max})}+\theta_{4}\ ,
$$ {#eq-parametric}

which depends on three additional parameters, $\theta_1$, $\theta_2$,
and $x_{\max}$, such that $g(0)=\theta_1$ and $g(x_{\max})=\theta_4$.
Under the parametric model (@eq-parametric), the squintability metric
\eqref{eq-squintability} can be shown to be equal to $$
\varsigma=\frac{(\theta_{1}-\theta_{4})\theta_{2}\theta_{3}}{\ell(0)-\ell(x_{\max})}\ .
$$ {#eq-squintability-parametric}

In practice, the parameters of this model (@eq-parametric) can be
estimated numerically, for example by non-linear least squares, and then
used to evaluate $\varsigma$ as in equation
(@eq-squintability-parametric).

Alternatively, one can estimate \eqref{eq-squintability} in a
nonparametric way, for example by fitting $g$ using kernel regression,
then numerically estimate the angle at which $-g'$ attains its highest
value.

# The jellyfish optimiser {#sec-JSO}

The Jellyfish Search Optimiser (JSO) mimics the natural movements of
jellyfish, which include passive and active motions driven by ocean
currents and their swimming patterns, respectively. In the context of
optimization, these movements are abstracted to explore the search
space, aiming to balance exploration (searching new areas) and
exploitation (focusing on promising areas). The algorithm aims to find
the optimal solution by adapting the behaviour of jellyfish to navigate
towards the best solution over iterations [@chou_novel_2021].

To solve the optimisation problem embedded in the PP guided tour, a
starting projection, an index function, the number of jellyfish, and the
maximum number of trials (tries) are provided as input. Then, the
current projection is evaluated by the index function. The projection is
then moved in a direction determined by a random factor, influenced by
how far along we are in the optimisation process. Occasionally,
completely new directions may be taken like a jellyfish might with ocean
currents. A new projection is accepted if it is an improvement compared
to the current one, rejected otherwise. This process continues and
iteratively improves the projection, until the pre-specified maximum
number of trials is reached.

::: {.callout-note icon="false"}
## Algorithm: Jellyfish Optimizer Pseudo Code

**Input**: `current_projections`, `index_function`, `trial_id`,
`max_trial`

**Output**: `optimised_projection`

**Initialize** `current_best` as the projection with the best index
value from `current_projections`, and `current_idx` as the array of
index values for each projection in `current_projections`

**for** each `trial_id` in 1 to `max_tries` **do**

> Calculate the time control value, $c_t$, based on `current_idx` and
> `max_trial`

> **if** $c_t$ is greater than or equal to $0.5$ **then**
>
> > Define trend based on the `current_best` and `current_projections`
>
> > Update each projection towards the trend using a random factor and
> > orthonormalisation
>
> **else**
>
> > **if** a random number is greater than $1 - c_t$ **then**
> >
> > > Slightly adjust each projection with a small random factor
> > > (passive)
> >
> > **else**
> >
> > > For each projection, compare with a random jellyfish and adjust
> > > towards or away from it (active)
>
> Update the orientation of each projection to maintain consistency
>
> Evaluate the new projections using the index function

> **if** any new projection is worse than the current, revert to the
> `current_projections` for that case
>
> > Determine the projection with the best index value as the new
> > `current_best`

> **if** `trial_id` $\ge$ `max_trial`, print the last best projection
> **exit**

**return** the set of projections with the updated `current_best` as the
`optimised_projection`
:::

<!-- Var names in the function and in the text needs to be consistent??? -->

The JSO implementation involves several key parameters that control its
search process in optimization problems. These parameters are designed
to guide the exploration and exploitation phases of the algorithm. While
the specific implementation details can vary depending on the version of
the algorithm or its application, the focus is on two main parameters
that are most relevant to our application: the number of jellyfish and
the maximum number of tries.

# Assessing optimisers: two simulation studies {#sec-sim-deets}

This section describes the details of two simulation studies: one
compares the JSO performance with an existing optimiser, Creeping Random
Search (CRS) [@RJ-2021-105; @laa_using_2020] to explore JSO's behaviour
under different data dimensions. The second simulation first examines
the JSO performance under different hyper-parameters combinations
(number of jellyfish and the maximum number of tries), and then studies
the effect of index properties (smoothness and squintability), along
with JSO hyper-parameters, and data dimension, on the JSO performance
through a logistic regression.

The performance of JSO is measured by the success rate, defined as the
proportion of simulations that achieves a final index value within 0.05
of the best index value found among all 50 simulations (see
@fig-success-rate for an illustration). This comparison is based on
projection pursuit to find the pipe shape investigated by
@laa_using_2020 using the `holes` index.

## Performance of JSO relative to CRS {#sec-app-1}

The JSO performance is investigated both in comparison to the existing
optimizer, CRS, in a pipe-finding problem using the holes index in the
6, 8, 10, and 12-dimensional space. Fifty simulations are conducted in
four data dimensions ($d = 6, 8, 10, 12$) with each optimiser. JSO uses
100 jellyfish with a maximum of 100 tries, while the CRS allows a
maximum of 1000 samples at each iteration before the algorithm
terminates. The different numbers account for the multiple paths of JSO
to enable fairer comparison with CRS. The results of the simulation are
collected using the data structure proposed in @RJ-2021-105 for
assessing JSO, where the design parameters are stored along with index
value, projection basis, random seed, and computation time.

```{r}
#| label: fig-success-rate
#| fig.cap: "Illustration of success rate calculation: Final projections based on projection pursuit to find the pipe shape in 8D data using the holes index, optimised by CRS, in 50 simulations. The 50 final projections are sorted by their index values. The highest index value found across all simulations is 0.969. Out of the 50 simulations, 43 achieved an index value within 0.05 of the best, resulting in a success rate of 0.86 (43/50)."
# pipe_jellyfish_12d <- pipe_better |> 
#   filter(dim == 2) |> 
#   mutate(dim = as.numeric(dim) * 8, sim = as.numeric(sim)) 
# proj_dt <- map_dfr(pipe_jellyfish_12d |> pull(basis),
#               ~as_tibble(pipe1000_8d %*% .x), .id = "sim") |>
#   mutate(sim = as.numeric(sim)) |>
#   left_join(pipe_jellyfish_12d |> select(sim, index_val), by = "sim")
# 
# idx_val_sorted <- proj_dt |> pull(index_val) |> unique() |> sort()
# proj_dt |>
#   ggplot(aes(x = V1, y = V2)) +
#   geom_point(size = 0.5) +
#   geom_text(aes(label = round(index_val, 3)), x = 0, y = 3.7,
#             size = 8) +
#   xlim(-4, 4) + ylim(-4, 4) +
#   facet_wrap(~fct_reorder(as.factor(sim), -index_val), ncol = 10) +
#   theme_bw() +
#   theme(aspect.ratio = 1, axis.ticks = element_blank(),
#         axis.text = element_blank(), axis.title = element_blank(),
#         panel.grid = element_blank(), strip.background = element_blank(),
#         strip.text = element_blank()) 
knitr::include_graphics(here::here("figures", "success-rate.png"))
```

## Factors affecting JSO success rate: JSO hyper-parameters and index properties {#sec-app-2}

To examine JSO's performance across various hyper-parameter
combinations, the pipe-finding problem using the holes index is repeated
for fifty times in each hyper-parameter combination: 20, 50, and 100
jellyfish and a maximum of 50 and 100 attempts. The success rate is
calculated for each combination.

To assess JSO's performance across various PP problems, two different
data shapes, pipe and a sine wave, are investigated in 6D and 8D spaces
using six different PP indexes: `dcor2d_2`, `loess2d`, `MIC`, `TIC`,
`spline`, and `stringy`, under varied JSO hyper-parameters. This results
in a total of `r nrow(sim_df)` scenarios, comprising of
`r nrow(filter(sim_df, index == "holes"))` computed on the pipe data and
`r nrow(filter(sim_df, index != "holes"))` on the sine-wave data. Again,
JSO is run 50 times for each scenario to calculate the success rate for
each PP problem.

```{r}
#| label: fig-plot-data
#| fig-width: 10
#| fig-height: 5
#| fig-cap: "The sine and pipe structures in the example data, along with index values for these projections."
data("sine1000")
sine1000 <- as_tibble(sine1000)
dp1 <- ggplot(sine1000, aes(x=V5, y=V6)) + geom_point()
data("pipe1000")
pipe1000 <- as_tibble(pipe1000)
dp2 <- ggplot(pipe1000, aes(x=V5, y=V6)) + geom_point()
dp1 + dp2 + plot_layout(ncol=2)
```

Smoothness and squintability are computed following the procedures
outlined in @sec-smoothness and @sec-squintability and as illustrated in
@fig-smoothness and @fig-squintability. To compute smoothness, 500
random bases are simulated. Index values are calculated for each random
basis, followed by fitting a Gaussian process model to obtain the
smoothness measure for the index.

To compute squintability, 50 random bases are sampled and interpolated
to the optimal basis with a step size of 0.005. Index values and
projection distances are calculated for these interpolated bases. A
binning procedure is applied to average the index values within each
0.005 projection distance bin. The four-parameter scaled logistic
function (@eq-parametric) is fitted to the index values against
projection distances, estimated by non-linear least squares to obtain
the squintability measure as Equation @eq-squintability-parametric.

To construct a relationship among success rate, smoothness,
squintability, and JSO hyper-parameters, a generalised linear model is
fitted using a binomial family and a logit link function. The data is
pre-processed by 1) scaling the JSO hyper-parameters by a factor of 10
for interpretation, 2) creating a new binary variable `long_time` to
indicate cases with an average run time over 20 seconds, and 3)
re-coding the success rate for the `stringy` index as 0, because none of
the 50 simulations correctly identified the sine-wave shape.

```{r}
#| label: fig-smoothness
#| out.width: 100%
#| fig.cap: "Steps for calculating smoothness in a projection pursuit problem. Given the target shape, data dimension and the index function: 1) sample random bases given the orthonormality contraint and calculate their corresponding index values, and 2) fit a Gaussian process model of index values against the sampled bases to obtain the smoothness measure."
# basis_smoothness <- sample_bases(idx = "splines2d", n_basis = 1000)
# basis_smoothness |>
#    ggplot(aes(x = dist, y = index)) +
#    geom_point() + 
#    labs(x = "Projection distance", y = "Index value") + 
#    theme_bw() +   
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
knitr::include_graphics(here::here("figures", "smoothness.png"))
```

```{r}
#| label: fig-squintability
#| out.width: 100%
#| fig.cap: |
#|   Steps for calculating squintability in a projection pursuit problem. Given the target shape, data dimension and the index function: 1) sample random bases given the orthonormality and projection distance contraint and calculates their corresponding index values, 2) interpolate the sampled bases to the optimal basis and calculate the projection distance and the index value. 3) bin the index values by projection distances through averaging index value, 4) fit the scaled sigmoid function in equation \eqref{eq-squintability} to the binned index values against projection distances using non-linear least square to obtian the squintability measure using equation (@eq-squintability-parametric).
# p1 <- sim_sine_6d_spline_projdist |> 
#   ggplot(aes(x = proj_dist, y = index_val)) +
#   geom_point(size = 0.1, alpha = 0.5) + 
#   labs(x = "Projection distance", y = "Index value") + 
#   theme_bw() + 
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
# 
# 
# dist_bin <- ceiling(sim_sine_6d_spline_projdist[["proj_dist"]] / 0.1) * 0.1
# dt <- sim_sine_6d_spline_projdist |>
#   dplyr::bind_cols(dist_bin = dist_bin) |>
#   dplyr::group_by(dist_bin) |>
#   dplyr::summarise(index_val = mean(index_val, na.rm = TRUE))
# 
# p2 <-  dt |> 
#   ggplot(aes(x = dist_bin, y = index_val)) + 
#   geom_line() +
#   geom_point() + 
#   labs(x = "Projection distance", y = "Index value") + 
#   theme_bw() + 
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
knitr::include_graphics(here::here("figures", "squintability.png"))
```

# Results {#sec-sim-res}

## Performance of JSO relative to CRS

Performance of JSO is assessed based on the final projections across the
two optimisers: JSO and CRS. The final projections found by the two
optimisers are presented in @fig-proj, broken down by 10th quantile,
faceted by the data dimensions. In the 6-dimensional data scenario, JSO
consistently identifies a clear pipe shape. The CRS also finds the pipe
shape but with a wide rim, suggesting a further polish search may be
required. With increasing dimensions, JSO may not always identify the
pipe shape due to random sampling, but it still finds the pipe shape in
over 50% of cases. Compared to CRS, JSO achieves higher index values and
clearer pipe shapes across all quantiles in data of 8, 10, 12
dimensions, suggesting its advantage in exploring high-dimensional
spaces.

## Factors affecting JSO success rate: JSO hyper-parameters and index properties

The effect of JSO hyper–parameters (number of jellyfish and the maximum
number of tries) on the success rate is presented in @fig-proportion.
The uncertainty is quantified through 500 bootstrap replicates for each
case. As the number of jellyfish and maximum tries increase, the success
rate also increases. For simpler problems (6 dimensions), small
parameter values (20 jellyfish and a maximum of 50 tries) can already
achieve a high success rate. However, larger parameter values (i.e. 100
jellyfish and a maximum of 100 tries) are necessary for
higher-dimensional problems (8, 10, and 12 dimensions). Increasing both
parameters enhances the performance of JSO, but it also extends the
computational time required for the optimisation, which can be
computationally intensive when evaluating the index function (such as
scagnostic indexes) multiple times across numerous iterations.

Smoothness and squintability are calculated across a collection of
pipe-finding and sine-wave finding problems to construct the
relationship between success rate, JSO hyper-parameters, and index
properties. @tbl-smoothness-squintability presents the parameters
estimated from the Gaussian process (outputscale $\eta$, lengthscale
$\ell$, smoothness $\nu$, and nugget $\sigma$) and from the scaled
logistic function ($\theta_1$ to $\theta_4$) for calculating smoothness
and squintability in each PP problem considered. The column $\nu$ is
used as the smoothness metric and the column $\varsigma$ is calculated
as equation (@eq-squintability-parametric) as the squintability metric.

```{r}
#| fig.height: 7.5
#| fig.width: 10
#| label: fig-proj
#| fig.align: "center"
#| fig.cap: "Projections found by the JSO and CRS at each 10th quantile across 50 simulations. The projection pursuit problem is to find the pipe shape using the holes index in the 6, 8, 10, and 12-dimensional spaces. The JSO uses 100 jellyfish and a maximum number of tries of 100. The CRS uses a maximum of 1000 tries in each step of random sampling step before the algorithm terminates. In the 6-D data space, JSO always finds a clear pipe shape while the CRS also finds the pipe shape but with a wide rim. At higher data dimensions, JSO finds a higher index value and a clearer pipe shape across all the quantiles than the CRS"

aaa <- map_dfr(
  list(pipe1000, pipe1000_8d, pipe1000_10d, pipe1000_12d), 
  ~{
    d_dim = dim(.x)[2]
    pipe_jellyfish |> 
      filter(d == d_dim) |> 
      arrange(index_val) |>
      filter(row_number() %in% c(1, seq(5, 50, 5))) |> 
      mutate(id = factor(
        paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
        levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"))) |> 
      compute_projection(data = .x, col = c("d", "index_val", "id"))
  }
)

p1 <- aaa |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = aaa |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x = 0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

bbb <- pipe_better |> 
  group_by(dim) |> 
  arrange(index_val) |> 
  filter(row_number() %in% c(1, seq(5, 50, 5))) |>  
  mutate(id = factor(
    paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
    levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th")), 
    d = 4 + 2 * as.numeric(dim)) |> 
  unnest(proj)

p2 <- bbb |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = bbb |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x = 0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

(p1 + ggtitle("a. JSO"))/(p2 + ggtitle("b. CRS"))
```

```{r}
#| label: fig-proportion
#| fig.width: 8
#| fig.height: 3.5
#| fig.align: "center"
#| fig.cap: "The proportion of simulations that reach near-optimal index values in the pipe-finding problem using the holes index. The proportion is calculated based on the number of simulations, out of 50, that achieve an index value within 0.05 of the best-performing simulation. To quantify uncertainty, 500 bootstrap samples are generated. The thin segments represent the proportion for each of the 500 bootstrap replicates, while the thicker segments represent the mean of these bootstrap samples, connnected by lines. As the dimensionality increases, the proportion of simulations reaching the optimal index value decreases."
pipe_sim_best <- sim_pipe_run_best |> 
  group_by(n_jellies, max_tries, d) |>
  summarise(I_best = max(I_max))
  
pipe_res <- sim_pipe_run_best |> 
  left_join(pipe_sim_best) |> 
  mutate(diff = abs(I_max - I_best) < 0.05) |> 
  group_by(n_jellies, max_tries, d) |> 
  summarise(proportion = sum(diff) / n()) |> 
  mutate(d = as.factor(d), n_jellies = as.factor(n_jellies))

res_df <- sim_pipe_run_best |> 
  left_join(pipe_sim_best |> ungroup() |> mutate(case_id = row_number()),
            by = c("n_jellies", "max_tries", "d")) |> 
  ungroup() |> 
  group_split(case_id)

resampling <- function(dt, size = size, replace = TRUE){
  row_id <- sample(1:50, size = size, replace = replace)
  
  dt |> filter(row_number() %in% row_id) |> 
    mutate(diff = abs(I_max - I_best) < 0.05) |> 
    summarise(proportion = sum(diff) / n()) |> 
    pull(proportion)
  
}

res_interval_df <- tibble(df = res_df) |> 
  crossing(rep = 1:500) |> 
  rowwise() |> 
  mutate(proportion = resampling(df, size = 50)) 

res_quantile <- res_interval_df |> 
  unnest(df) |> 
  group_by(n_jellies, max_tries, case_id, d) |> 
  select(-I_max, -id) |> 
  distinct() |> 
  mutate(d = as.factor(d),
         n_jellies = ifelse(d == 10, n_jellies - 1, n_jellies),
         n_jellies = ifelse(d == 12, n_jellies + 1, n_jellies)) |> 
  rename(`max # of tries` = max_tries) 
  
res_quantile |> 
  ungroup() |> 
  ggplot(aes(x = n_jellies - 0.5, y = proportion, 
             group = d, color = d)) + 
  geom_segment(aes(xend = n_jellies + 0.5), size = 0.05) + 
  geom_segment(aes(x = n_jellies - 1, xend = n_jellies + 1), 
               data = res_quantile |> summarise(proportion = mean(proportion)), 
               size = 2) + 
  geom_line(aes(x = n_jellies), 
               data = res_quantile |> summarise(proportion = mean(proportion)), 
            alpha = 0.3) + 
  scale_color_brewer(palette = "Dark2", name = "dimension") +
  scale_x_continuous(breaks = c(20, 50, 100), labels = c("20", "50", "100")) +
  facet_wrap(vars(`max # of tries`), 
             labeller = "label_both") + 
  theme_bw() +
  theme(panel.grid.minor = element_blank()) + 
  xlab("Number of jellyfish") + 
  ylab("Proportion of success")
```

```{r}
#| label: fit-logistic
sim_df2 <- sim_df |>   
  mutate(n_jellies = n_jellies/10, max_tries = max_tries/10) |> 
  rename(proportion = P_J) 

mod1 <- glm(proportion ~ smoothness + squintability + d + n_jellies + max_tries, data = sim_df2, family = binomial) 
```

```{r}
#| label: fig-idx-proj-dist
#| fig.width: 9
#| fig.height: 5
#| eval: false
#| fig.cap: "Index values versus projection distance for the 12 pipe/sine-wave finding problem, after the binning procedure for calculating the squintability measure. The index values, averaged at bin width of 0.005, are scaled from 0 to 1 for comparison (holes, TIC, and stringy). The `MIC` and `TIC` index curves are convex while others are concave. The stringy curve shows an instantaneous jump to the optimum when aproaching the best basis." 
sq_basis_dist_idx |>
  ggplot(aes(x = dist, y = index, group = index_name)) +
  geom_line() + 
  facet_nested_wrap(~index_name + n, nrow = 3, labeller = label_both) + 
  xlab("projection distance") + ylab("index value") +
  theme_bw()
```

```{r}
#| label: tbl-smoothness-squintability
#| tbl-cap: Parameters estimated from the Gaussian process (outputscale $\eta$, lengthscale $\ell$, smoothness $\nu$, and nugget $\sigma$) and scaled logistic function ($\theta_1$ to $\theta_4$ and $\varsigma$) for the pipe-finding and sine-wave finding problems.  The columns $\nu$ and $\varsigma$ represent the smoothness and squintability measures respectively. 
dt <- tibble(shape = c(rep("pipe", 4), rep("sine", 18)), smoothness) |>
  left_join(squintability) |> 
  rename(d = n, smooth = smoothness) |> 
  mutate(index = ifelse(index == "dcor2d_2", "dcor", index),
         index = ifelse(index == "loess2d", "loess", index),
         index = ifelse(index == "splines2d", "splines", index),
         variance = sqrt(variance)) 
dt |> 
  knitr::kable(
    digits = 2, format = "latex", 
    col.names = c(
      "shape", "index", "d",
      "$\\eta$", "$\\ell$", "$\\nu$", "$\\sigma$",
      "$\\theta_1$", "$\\theta_2$", "$\\theta_3$", "$\\theta_4$", "$\\varsigma$"), linesep = "",
    booktabs = TRUE, escape = FALSE, row.names = FALSE, align = "|llr|rrrr|rrrrr") |> 
  #column_spec(1, border_left = TRUE) |> 
  #column_spec(c(4, 8, 13), border_right = TRUE) |> 
  kable_styling(font_size = 10) 
```

@tbl-mod-output presents the results from fitting a logistic regression
model using the proportion of success as the response and smoothness,
squintability, dimension, running time, number of jellyfish and maximum
number of tries, as predictors. Long runtime is a binary variable, where
greater than 20 seconds is labelled 1. The fit suggests that JSO success
rate is only affected by squintability, dimension and number of jellies.
Interestingly, the smoothness of the index is not a problem for the
JSO - this is promising! Running the optimisation longer and allowing
more tries also does not affect success rate. An increase in
squintability of one unit increases the success rate by
`r round(exp(broom::tidy(mod1)$estimate[3]) * 100, 0)`%. As dimension
increases by one the success rate halves. Increasing the number of
jellies by 10, increases the succes rate by
`r round(100 * (exp(broom::tidy(mod1)$estimate[6])-1), 0)`%.

```{r}
#| label: tbl-mod-output
#| tbl-cap: "Jellyfish success rate relative to index properties and jellyfish hyper-parameters. This is the summary from a logistic regression fit to smoothness, squintability, dimension, running time, number of jellyfish and maximum number of tries. Interestingly, squintability and dimension strongly affect jellyfish optimisation success. The number of jellies marginally affects success, but index smoothness, running longer and increasing the number of tries do not."
#| fig-pos: t
library(gtsummary)
set_gtsummary_theme(theme_gtsummary_journal("jama"))
mod1 |> tbl_regression(exponentiate = TRUE, 
                       pvalue_fun = label_style_pvalue(digits = 3),
                       label = list(smoothness = "Smoothness", 
                                    squintability = "Squintability",
                                    d = "Dimension", 
                                    long_time = "Long runtime", 
                                    n_jellies = "Number of jellyfish", 
                                    max_tries = "Maximum number of tries")) |>
  bold_p(t = 0.1)
```



# Practicalities {#sec-discussion}

The simulation studies have compared the JSO with CRS and shown how
smoothness and squintability affect the JSO success rate. Here we
explain how they can be used for new index and optimiser development.
Using the JSO optimiser for PPGT requires a change in user behaviour.
For all the existing optimisers a single optimisation path is followed.
The JSO has multiple optimisation paths, and needs additional tools to
incorporate into the PPGT, as explained here.

## Using the JSO in a PPGT

To use JSO for PPGT in the `tourr` package, specify
`search_f = search_jellyfish` in the guided tour. Unlike existing
optimisers, the animation function `animate_*()` won't directly render
the tour path for JSO due to the generation of multiple tour paths. To
visualise the path visited by each individual jellyfish, assign the
animation to an object (`res <- animate_xy(...)`). This will save the
bases visited by JSO, along with relevant metadata, as a tibble data
object (see @RJ-2021-105 for more details on the data object). The bases
visited for individual jellyfish can then be extracted and viewed using
the `planned_tour()` function.

## Computing the index properties for your new index

The `ferrn` package [@RJ-2021-105] provides functionality for computing
the smoothness and squintability metrics. Both metrics require sampling
random bases using the `sample_bases()` function, followed by the metric
calculation with `calc_smoothness()` or `calc_squintability()`.

### Smoothness

To sample bases for calculating smoothness, the following arguments are
required: the index function, the dataset, and the number of random
bases to sample. The output of `sample_bases()` is a data frame with a
list-column of sampled basis matrix and index value. Parallellasation is
available to speed up the index value computation through the
`parallel = TRUE` argument.

The `calc_smoothness()` function takes the output from `sample_bases()`
and fits a Gaussian process model to the index values against the
sampled basis, as the location, to obtain the smoothness metric.
Starting parameters and additional Gaussian process arguments can be
specified and details of the fit can be accessed through the `fit_res`
attribute of the output.

### Squintability

Bases sampling for squintability includes an additional step of
interpolating between the sampled bases and the best basis. This step is
performed when the arguments `step_size` and `min_proj_dist` in the
`sample_bases()` function are set to non-NA numerical values. Given the
projection distance typically ranging from 0 to 2, it is recommended to
set `step_size` to 0.1 or lower, and `min_proj_dist` to be at least 0.5
to ensure a meaningful interpolation length.

The `calc_squintability()` function computes squintability using two
methods: 1) parametrically, by fitting a scaled sigmoid function through
non-linear least square (`method = "nls"`), and 2) non-parametrically,
using kernel smoothing (`method = "ks"`). A `bin_width` argument is
required to average the index values over the projection distance before
the fit. For the parametric case, the output provides the estimated
parameters for the scaled sigmoid function ($\theta_1$ to $\theta_4$)
and the calculated squintability metric as Equation
@eq-squintability-parametric. For the non-parametric case, it shows the
maximum gradient attained (`max_d`), the corresponding projection
distance (`max_dist`), and the squintability metric as their products.

# Conclusion {#sec-conclusion}

This paper has presented new metrics to mathematically define desirable
features of PP indexes, squintability and smoothness, and used these to
assess the performance of the new jellyfish search optimiser. The
metrics will be generally useful for characterising PP indexes, and help
with developing new indexes.

In the comparison of the JSO against the currently used CRS, as expected
the JSO vastly outperforms CRS, and provides a high probability of
finding the global optimum. The JSO obtains the maximum more cleanly,
with a slightly higher index value, and plot of the projected data
showing the structure more clearly.

The JSO performance is affected by the hyper-parameters, with a higher
chance of reaching the global optimum when more jellyfish are used and
the maximum number of tries is increased. However, it comes at a
computational cost, as expected. The performance declines if the
projection dimension increases and if the PP index has low
squintability. The higher the squintability the better chance the JSO
can find the optimum. However, interestingly smoothness does not affect
the JSO performance.

One future direction of this work is to test the JSO, along with its
variations and other swarm-based optimisers, on a broader range of
indexes, for example, scagnostic indexes.

<!--
- Summarise the result of the simulation: JSO immense improvement on current methods, ...
- JSO for a guided tour, what changes are needed? Do we follow one tentacle, do we perform off-line and then pull tentacles out to watch, how does it integrate with ferrn


Main findings: 

  - JSO tends to obtian a clearer projection and higher index value when searching higher dimension spaces than CRS (and other current optimisers) - Figure 4
  
  - Increasing JS hyper-parameter enhances its performance, but the computational time required for the optimisation can quickly become intensive, especially when evaluating the index function (such as scagnostic indexes) multiple times across numerous iterations. - Figure 5
  
  - Smoothness and squintability can be calculated and compared across different problems to understand their relative complexity. Once a projection pursuit problem is fully characterised (by the shape-to-find, the index function used, and the data dimension), increasing JSO hyper-parameters can enhance the search effectiveness, however, the resulting increase in computational complexity will also need to be considered. - modelling
-->

# Acknowledgement

The article is created using Quarto [@Allaire_Quarto_2022] in R [@R].
The source code for reproducing the work reported in this paper can be
found at: <https://github.com/huizezhang-sherry/paper-jso>. The
simulation data produced in Section 5 can be found at
<https://figshare.com/articles/dataset/Simulated_raw_data/26039506>.
Nicolas Langrené acknowledges the partial support of the Guangdong
Provincial Key Laboratory IRADS (2022B1212010006, R0400001-22) and the
UIC Start-up Research Fund UICR0700041-22.

# Supplementary materials {.unnumbered}

The supplementary materials available at
<https://github.com/huizezhang-sherry/paper-jso> include: 1) details of
the indexes used in the simulation study, 2) the script to get started
with using JSO in a PPGT and calculating smoothness and squintability,
as explained in @sec-discussion, and (3) the full code to reproduce the
plots and summaries in this paper.

# References
