---
title: Squintability and Other Metrics for Assessing Projection Pursuit Indexes, and Guiding Optimization Choices
author:
  - name: H. Sherry Zhang
    email: huize.zhang@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Austin
          country: United States
          postal-code: 78751
    attributes:
        corresponding: true
  - name: Dianne Cook
    email: dicook@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
  - name: Nicolas  Langrené
    email: nicolaslangrene@uic.edu.cn
    affiliations:
        - id: 3
          name: Guangdong Provincial/Zhuhai Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International College
          department: Department of Mathematical Sciences
          city: Zhuhai
          country: China
          postal-code: 519087
  - name: Jessica Wai Yin Leung
    email: jessica.leung@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
abstract: |
  The projection pursuit (PP) guided tour optimizes a criterion function, known as the PP index, to gradually reveal projections of interest from high-dimensional data through animation. Optimization of some PP indexes can be non-trivial, if they are non-smooth functions, or when the optimum has a small "squint angle", detectable only from close proximity. Here, measures for calculating the smoothness and squintability properties of the PP index are defined. These are used to investigate the performance of a recently introduced swarm-based algorithm, Jellyfish Search Optimizer (JSO), for optimizing PP indexes. The performance of JSO in detecting the target pattern (pipe shape) is compared with existing optimizers in PP. Additionally, JSO's performance on detecting the sine-wave shape is evaluated using different PP indexes (hence different smoothness and squintability) across various data dimensions (d = 4, 6, 8, 10, 12) and JSO hyper-parameters. We observe empirically that higher squintability improves the success rate of the PP index optimization, while smoothness has no significant effect. The JSO algorithm has been implemented in the R package, `tourr`, and functions to calculate smoothness and squintability measures are implemented in the `ferrn` package.
keywords: 
  - Projection Pursuit Guided Tour (PPGT)
  - Jellyfish Search Optimizer (JSO)
date: last-modified
bibliography: bibliography.bib
format:
  tandf-pdf:
    keep-tex: true
    fontsize: 12pt
    linestretch: 2
editor: 
  markdown: 
    wrap: 72
crossref: 
  eq-prefix: ""
editor_options: 
  chunk_output_type: console
header-includes:
   - \usepackage{algorithm}
   - \usepackage{float} 
   - \usepackage{amsmath}
   - \usepackage{amsthm}
   - \usepackage{amssymb}
   - \theoremstyle{plain}
   - \newtheorem{defn}{\protect\definitionname}
   - \newtheorem{prop}{\protect\propositionname}
   - \providecommand{\definitionname}{Definition}
   - \providecommand{\propositionname}{Proposition}
nocite: |
   @DLMF
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(patchwork)
library(ggh4x)
library(broom)
library(broom.helpers)
library(kableExtra)
library(tourr)
library(ferrn)
library(gtsummary)
load(here::here("data/sim_pipe_run_best.rda"))
load(here::here("data/sim_sine_6d_spline_head.rda"))
load(here::here("data/sim_sine_6d_spline_best.rda"))
load(here::here("data/sim_sine_6d_spline_projdist.rda"))
load(here::here("data/pipe_better.rda"))
load(here::here("data/pipe_jellyfish.rda"))
load(here::here("data/sq_sine_basis_df.rda"))
load(here::here("data/smoothness.rda"))
load(here::here("data/squintability.rda"))
load(here::here("data/sq_basis_df_6d_TIC_spline_skinny.rda"))
load(here::here("data/sim_df.rda"))
```

# Introduction

Projection pursuit (PP) [@kr69; @FT74; @huber85] is a dimension
reduction technique aimed at identifying informative linear projections
of data. This is useful for exploring high-dimensional data, and
creating plots of the data that reveal the main features to use for
publication. The method involves optimizing an objective function known
as the PP index [e.g., @hall1989polynomial; @cook1993projection;
@lee2010projection; @Loperfido2018; @Loperfido2020], which defines the
criterion for what constitutes interesting or informative projections.
Let $X \in \mathbb{R}^{n\times p}$ be the data matrix,
$A \in\mathbb{R}^{p \times d}$ be an orthonormal matrix in the Stiefel
manifold $\mathcal{A} = V_d(\mathbb{R}^p)$. The projection $Y = XA$ is a
linear transformation of the $p$-dimensional data to a $d$-dimensional
space. The index function
$f(XA): \mathbb{R}^{n \times d} \to \mathbb{R}$ defines a statistics to
measure a pattern of interest in the projected data, such as deviation
from normality, presence of clusters, or non-linear structure. For a
fixed sample of data, PP finds the orthonormal matrix $A$ (also called
*projection basis* in PP literature) that maximizes the index value of
the projection:

$$
\underset{A \in \mathcal{A}}{\max } \quad f(XA) \quad \text{subject to} \quad A'A = I_d
$$ {#eq-optimization}

where $I_d$ is the identity matrix of dimension $d$.\
It is interesting to note that when using PP visually, one cares less
about $A$ than the plane described by $A$, because the orientation in
the plane is irrelevant. The space of planes belongs to a Grassmann
manifold. This is usually how the projection pursuit guided tour (PPGT)
[@cook1995grand] operates, when using geodesic interpolation between
starting and target planes. It interpolates plane to plane, removing
irrelevant within-plane spin, and is agnostic to the basis ($A$) used to
define the plane. Thus, indexes which are used for the PPGT should be
rotationally invariant.

Index functions are quite varied in form, partially depending on the
data that is being projected. @fig-example-functions shows two examples.
Huber plots [@huberplot] of 2D data sets are in (a) and (c), showing the
PP index values for all 1D projections of the 2D data in polar
coordinates, which reveals the form of these functions. The dashed
circle is a baseline set at the average value, and the straight line
marks the optimal projection. Plots (b) and (d) show the respective best
projections of the data as histograms. Indexes like the holes, central
mass and skewness [@cook1993projection] are generally smooth for most
data sets, but capture only large patterns. Many indexes are noisy and
non-convex, requiring an effective and efficient optimization procedure
to explore the data landscape and achieve a globally optimal viewpoint
of the data. The skewness index computed for trimodal data, in (a), is
smooth with a large squint angle but has three modes, and thus is not
convex. The binned normality index [a simple version of a non-normality
index as described in @huber85] computed on the famous RANDU data, in
(c), is noisier and has a very small squint angle. The discreteness
cannot be seen unless the optimizer is very close to the optimal
projection.

```{r}
#| label: fig-example-functions
#| echo: false
#| fig-width: 9
#| fig-height: 6
#| out-width: 80%
#| fig-cap: "Examples of PP indexes with large (top row) and small (bottom row) squint angles, shown with a Huber plot, and histogram of the projected data corresponding to the optimal projection. A Huber plot shows the PP index values for all 1D data projections in polar coordinates. The skewness index on the trimodal data is also smoother than the binned normality index on RANDU."
#proj <- animate_xy(flea[,1:6], guided_tour(lda_pp(flea$species)))
best_proj <- matrix(c(-0.44157005,  0.80959831,
                      0.14622089, -0.08228193,
                      -0.08454314, -0.23234287,
                      0.70367852,  0.28185021,
                      0.27876825,  0.44819691,
                      0.45123453,  0.05896646), ncol=2, byrow=T)

flea2D <- as.matrix(flea[,1:6]) %*% best_proj
colnames(flea2D) <- c("X1", "X2")
flea2D <- as.data.frame(flea2D)

sqa1 <- ggplot() + 
  geom_huber(data = flea2D, aes(x = X1, y = X2), 
             index.fun = skewness()) + 
  theme_huber() + 
  coord_fixed() + 
  ggtitle("(a) Skewness index")

flea_huber <- prep_huber_best_proj(flea2D, index_fun = skewness())
sqa2 <- ggplot(flea_huber, aes(x=x)) + 
  geom_histogram(binwidth=0.25, colour="white") +
  xlab("") + ylab("") +
  ggtitle("(b) Optimally projected data") +
  theme_bw() +
  theme(axis.text.y = element_blank())


data(randu)
randu_std <- as.data.frame(apply(randu, 2, function(x) (x-mean(x))/sd(x)))
randu_std$yz <- sqrt(35)/6*randu_std$y-randu_std$z/6
randu_df <- randu_std[c(1,4)]

sqa3 <- ggplot() + 
  geom_huber(data = randu_df, aes(x = x, y = yz), 
             index.fun = norm_bin(nr = nrow(randu_df)))  + 
  theme_huber() + 
  coord_fixed() + 
  ggtitle("(c) Binned normality index")

randu_huber <- prep_huber_best_proj(
  randu_df, index_fun = norm_bin(nr = nrow(randu_df)))
sqa4 <- ggplot(randu_huber, aes(x = x)) +
  geom_histogram(breaks = seq(-2.2, 2.4, 0.12)) +
  xlab("") + ylab("") +
  theme_bw() +
  theme(axis.text.y = element_blank()) + 
  ggtitle("(d) Optimally projected data")

sqa1 + sqa2 + sqa3 + sqa4 + plot_layout(ncol=2, widths=c(2,2))

```

Optimization of PP is sometimes discussed when new indexes are proposed
[@posse95; @marie-sainte2010; @grochowski2011]. @cook1995grand tied the
optimization closely to the index with the introduction of the PPGT,
which monitors the optimization visually so that the user can see the
projected data leading in and out of the optimum. An implementation is
available in the `tourr` package [@tourr] in R [@R]. @RJ-2021-105
illustrated how to diagnose optimization processes, particularly
focusing on the guided tour, and revealed a need for improved
optimization. While improving the quality of the optimization solutions
in the tour is essential, it is also important to be able to view the
data projections as the optimization progresses. Integrating the guided
tour with a global optimization algorithm that is efficient in finding
the global optimal and enables viewing of the projected data during the
exploration process is a goal.

In this work, the potential for a Jellyfish Search Optimizer (JSO)
[@chou_novel_2021; @rajwar_exhaustive_2023] for the PPGT is explored.
The JSO, inspired by the search behavior of jellyfish in the ocean, is a
swarm-based metaheuristic designed to solve global optimization
problems. Compared to other metaheuristic methods, JSO has demonstrated
stronger search ability and faster convergence, and requires fewer
tuning parameters. These practicalities make JSO a promising candidate
for enhancing PP optimization.

The primary goal of the study reported here is to investigate the
performance of JSO in PP optimization for the guided tour. It is of
interest to assess how quickly and closely the optimizer reaches a
global optimum, for various PP indexes that may have differing
complexities. To observe the performance of JSO with different types of
PP indexes, metrics are introduced to capture specific properties of the
index including squintability [based on @barnett1981interpreting's
squint angle] and smoothness. We mathematically define metrics for
squintability and smoothness, which is a new contribution for PP
research. A series of simulation experiments are conducted using JSO to
detect different target patterns (pipe and sine-wave) with different PP
indexes (holes, MIC, TIC, dcor, loess, splines, skinny, and stringy), as
well as JSO's hyper-parameter choices (number of jellyfish and maximum
number of iterations). This work should facilitate better optimization for PP
and guide the choice of optimizer when designing new PP indexes.

The paper is structured as follows. @sec-background introduces the
background of the PPGT, reviews existing optimizers and index functions
in the literature. @sec-PP-properties introduces the metrics that
measure two properties of PP indexes, smoothness and squintability.
@sec-JSO describes the new JSO for PP. @sec-sim-deets outlines two
simulation experiments to assess JSO's performance: one comparing JSO's
performance improvements relative to an existing optimizer, Creeping
Random Search (CRS), and the other studying the impact of PP index
properties, controlling for data dimension and JSO hyper-parameters, on
optimization performance, and @sec-sim-res presents the results.
@sec-discussion discusses the implementation of JSO in the `tourr`
package and the PP property calculation in the `ferrn` package.
@sec-conclusion summarizes the work and provides suggestions for future
directions.

# Projection pursuit, tours, index functions and optimization {#sec-background}

A tour on high-dimensional data is constructed by geodesically
interpolating between pairs of planes. Any plane is described by an
orthonormal basis, $A_t$, where $t$ represents time in the sequence. The
term "geodesic" refers to maintaining the orthonormality constraint so
that each view shown is correctly a projection of the data. The PP
guided tour operates by geodesically interpolating to target planes
(projections) which have high PP index values, as provided by the
optimizer. The geodesic interpolation means that the viewer sees a
continuous sequence of projections of the data, so they can watch
patterns of interest forming as the function is optimized. There are
five optimization methods implemented in the `tourr` package:

-   a pseudo-derivative, that searches locally for the best direction,
    based on differencing the index values for very close projections.
-   a brute-force optimization (CRS).
-   a modified brute force algorithm described in @posse95.
-   an essentially simulated annealing [@Bertsimas93] where the search
    space is reduced during the optimization.
-   a very localized search, to take tiny steps to get closer to the
    local maximum.

<!-- 
-   `search_geodesic()`: provides a pseudo-derivative optimization. It
    searches locally for the best direction, based on differencing the
    index values for very close projections. Then it follows the
    direction along the geodesic path between planes, stopping when the
    next index value fails to increase.
-   `search_better()`: also known as Creeping Random Search (CRS), is a
    brute-force optimization searching randomly for projections with
    higher index values.
-   `search_better_random()`: is essentially simulated annealing
    [@Bertsimas93] where the search space is reduced as the optimization
    progresses.
-   `search_posse()`: implements the algorithm described in @posse95.
-   `search_polish()`: is a very localized search, to take tiny steps to
    get closer to the local maximum.
-->

There are numerous PP index functions available: introduced in @huber85,
@cook1993projection, @lee2005projection, @lee2010projection, @Grimm2016,
@Laa:2020wkm. Most are relatively simply defined, for any projection
dimension, and implemented because they are relatively easy to optimize.
A goal is to develop PP indexes based on scagnostics [@scag; @WW08], but
the blockage is their optimization as these tend to be noisy, with
potentially small squint angles.

An initial investigation of PP indexes, and the potential for
scagnostics is described in @laa_using_2020. To be useful here an
optimizer needs to be able to handle index functions that are possibly
not very smooth. In addition, because the target structure in the data
might be relatively fine, the optimizer needs to be able to find maxima
that occur with a small squint angle, that can only be seen from very
close by. One last aspect that is useful is for an optimizer to return
local maxima in addition to the global one because data can contain many
different and interesting features.

# Properties of PP indexes {#sec-PP-properties}

@laa_using_2020 has proposed five criteria for assessing projection
pursuit indexes (smoothness, squintability, flexibility, rotation
invariance, and speed). Since not all index properties affect the
optimization process, the focus of this work is on the first two
properties, *smoothness* (@sec-smoothness) and *squintability*
(@sec-squintability), for which metrics are proposed to quantify them.

## Smoothness {#sec-smoothness}

A classical way to describe the smoothness of a function is to identify
how many continuous derivatives of the function exist. This can be
characterized by Sobolev spaces [@adams2003sobolev].

\begin{defn}[Sobolev space]\label{def:sobolev_space}
The Sobolev space $W^{k,p}(\mathbb{R})$ for $1\leq p\leq \infty$ is the set of all functions $f$ in $L^p(\mathbb{R})$ for which all weak derivatives $f^{(\ell)}$ of order $\ell\leq k$ exist and have a finite $L^p$ norm.
\end{defn}

The Sobolev index $k$ in Definition \ref{def:sobolev_space} can be used
to characterize the smoothness of a function: if $f\in W^{k,p}$, then
the higher $k$, the smoother $f$. While this Sobolev index $k$ is a
useful measure of smoothness, it can be difficult to compute or even
estimate in practice. For example, the loess and splines indexes are not
differentiable due to the use of the $\max$ operator in their definitions.
Scagnostic indexes, such as skinny and stringy, are defined based on
graph elements (area, diameter, perimeter, and length) of alph hull or
minimal spanning tree).

To obtain a computable estimator of the smoothness of the index function
$f$, we propose an approach based on random fields. If a PP index
function $f$ is evaluated at some random bases, then these random index
values can be interpreted as a random field, indexed by a space
parameter, namely the random projection basis. This analogy suggests to
use this random training sample to fit a spatial model. We propose to
use a Gaussian process equipped with a Matérn covariance function, due
to the connections between this model and Sobolev spaces, see for
example @porcu2024matern.

The distribution of a Gaussian process is fully determined by its mean
and covariance function. The smoothness property comes into play in the
definition of the covariance function: if a PP index is very smooth,
then two close projection bases should produce close index values
(strong correlation); by contrast, if a PP index is not very smooth,
then two close projection bases might give very different index values
(fast decay of correlations with respect to distance between bases).
Popular covariance functions are parametric positive semi-definite
functions. In particular, the Matérn class of covariance functions has a
dedicated parameter to capture the smoothness of the Gaussian field.

\begin{defn}[Matérn covariance function]\label{def:matern}
The Matérn covariance function $K$ is defined by
\begin{equation}
K(u)=K_{\nu,\eta,\ell}(u):=\eta^2\frac{\left(\sqrt{2\nu}\frac{\left\Vert u\right\Vert}{\ell}\right)^{\nu}}{\Gamma(\nu)2^{\nu-1}}\mathcal{K}_{\nu}\left(\sqrt{2\nu}\frac{\left\Vert u\right\Vert}{\ell}\right)\ ,\label{eq:matern}
\end{equation}
where $\left\Vert u\right\Vert$ is the Euclidean norm of $u\in\mathbb{R}^{p{\times}d}$, $\nu>0$ is the smoothness parameter, $\eta$ is the outputscale, $\ell$ is the lengthscale, and $\mathcal{K}_\nu$ is
the modified Bessel function [DLMF 10.25].
\end{defn}

The Matérn covariance function can be expressed analytically when $\nu$
is a half-integer, the most popular values in the literature being
$\frac{1}{2}$, $\frac{3}{2}$ and $\frac{5}{2}$ [@rasmussen2006gaussian].
The parameter $\nu$, called *smoothness parameter*, controls the decay
of the covariance function. As such, it is an appropriate measure of
smoothness of a random field, as shown by the simulations on
@fig-matern-1d and @fig-matern-2d. For example, @karvonen2023asymptotic
showed that if a function $f$ has a Sobolev index of $k$, then the
smoothness parameter estimate $\nu$ in \eqref{eq:matern} cannot be
asymptotically less than $k$. See the survey @porcu2024matern for
additional results on the connection between the Matérn model and
Sobolev spaces. An interesting result is that the asymptotic case
$\nu\rightarrow\infty$ coincides with the Gaussian kernel:
$K_\infty(u)=\exp(-{\left\Vert u\right\Vert}^{2}/2)$.

```{r}
#| label: fig-matern-1d
#| out.width: 100%
#| fig.cap: |
#|   Five random simulations from a Gaussian Process defined on $\mathbb{R}$ with zero mean and Matérn-$\nu$ covariance function, with $\nu=1$ (left), $\nu=2$ (middle), and $\nu=4$ (right), showing that higher values of $\nu$ produce smoother curves.
knitr::include_graphics(here::here("figures", "matern_simulation_1d.png"))
```

```{r}
#| label: fig-matern-2d
#| out.width: 100%
#| fig.cap: |
#|   One random simulation from a Gaussian Process defined on $\mathbb{R}^2$ with zero mean and Matérn-$\nu$ covariance function, with $\nu=1$ (left), $\nu=2$ (middle), and $\nu=4$ (right), showing that higher values of $\nu$ produce smoother surfaces.
knitr::include_graphics(here::here("figures", "matern_simulation_2d.png"))
```

In view of these results, the parameter $\nu$ is suggested as a measure
of the smoothness of the PP index function by fitting a Gaussian process
prior with Matérn covariance on a dataset generated by random
evaluations of the index function. There exist several R packages, such
as `GpGp` [@guinness2021gpgp] or `ExaGeoStatR` [@abdulah2023large], to
fit the hyper-parameters of a GP covariance function on data, which is
usually done by maximum likelihood estimation. In this project, the
`GpGp` package is used.

\begin{defn}
Let $\mathbf{A}=[A_1, \ldots, A_N] \in (\mathbb{R}^{p \times d})^N$ be d-dimensional projection bases, $\mathbf{y}=[f(XA_1),\ldots,f(XA_N)]$ be the corresponding PP index values, and $\mathbf{K}=[K_\theta(A_{i}-A_{j})]_{1\leq i,j\leq N}\in\mathbb{R}^{N\times N}$ be the Matérn covariance matrix evaluated at the input bases, where the vector $\theta$ contains all the parameters of the multivariate Matérn covariance function $K$ (smoothness, outputscale, lengthscales). The log-likelihood of the parameters $\theta$ is defined by 
\begin{equation}
\mathcal{L}(\theta)=\log p(\mathbf{y}\left|\mathbf{A},\theta\right.)=-\frac{1}{2}\mathbf{y}^{\top}(\mathbf{K}+\sigma^{2}\mathbf{I})^{-1}\mathbf{y}-\frac{1}{2}\mathrm{\log}(\det(\mathbf{K}+\sigma^{2}\mathbf{I}))-\frac{N}{2}\log(2\pi)\, \label{eq:gp_log_likelihood}
\end{equation}
where the nugget parameter $\sigma$ is the standard deviation of the intrinsic noise of the Gaussian process.
The optimal parameters are obtained by maximum log-likelihood
\begin{equation}
\theta^* = \underset{\theta}{\max}\mathcal{L}(\theta)
\end{equation}
The resulting optimal smoothness parameter $\nu$ is chosen as our smoothness metric.
\end{defn}

The value of the optimal smoothness parameter $\nu>0$ can be naturally
interpreted as follows: the higher $\nu$, the smoother the index
function.

## Squintability {#sec-squintability}


The squintability metric is inspired by the concept of squint angle in Definition \ref{def:squint-angle}, originally introduced in @barnett1981interpreting and later discussed in @laa_using_2020. Consider the space of all projection bases: those near the optimal basis correspond to projections with interesting structure we're researching. The size of this neighborhood depends on the choice of the index function. For some indexes, projections resemble the target even when relatively far from the optimum, corresponding to a large squint angle, while for others, projections need to be close to the optimum to reveal structure we're interested, resulting in a small squint angle. For PP, a small squint angle is considered to be undesirable because it means that the optimizer needs to sample into a be very narrow region to be able to "see" the optimum, making it difficult for the optimizer to find the optimum. 

\begin{defn}[squint angle]\label{def:squint-angle}
Let $A$ and $B$ be two $d$-dimensional orthonormal matrices in $\mathbb{R}^p$. The squint angle $\theta$ between the subspace spanned by $A$ and $B$ is defined as the smallest principal angle between these subspaces: $\theta = \min_{i \in \{1, \cdots, d\}} \arccos(\tau_i)$, where $\tau_i$ are the singular values of the matrix $M = A^T B$ obtained from its singular value decomposition.
\end{defn}

To quantify how close projections need to be before structures of interest can be observed (the optimizer starts hill-climbing), we can track how the index value changes as we move from an initial projection basis towards the optimal one, that is, how index value changes as the projection distance becomes smaller. 


\begin{defn}[projection distance]\label{def:proj-dist}
Let $A \in \mathbb{R}^{p \times d}$ be a $d$-dimensional orthonormal matrix, and let $A^*$ be the optimal matrix that achieves the maximum index value for a given data. The projection distance between $A$ 
and $A^*$, $r(A, A^*)$, is defined by
$r(A, A^*) = \lVert AA^\prime - A^*A^{*\prime}\,\rVert _F$
where $\lVert . \rVert _F$ denotes the Frobenius norm, given by
$\lVert M \rVert _F = \sqrt{\sum_{ij} M_{ij}^2}$. 
\end{defn}

```{r}
#| label: fig-squintability-ill
#| fig.width: 8
#| fig.height: 4
#| fig.cap: "Simulated traces of index value as a function of projection distances for four PP index functions: MIC, splines2d, skinny, and stringy2. The index function MIC and splines2d make early progression towards the optimal index value, indicating a large squint angle, whereas the traces from skinny, and specially stringy2, show improvement only near the optimal projection, suggesting low squintability."
sq_sine_basis_df |> 
  filter(n == 4) |> 
  unnest(basis_df, names_repair = "unique") |> 
  filter(id <= 3) |> 
  rename(index = index...2, value = index...8) |> 
  filter(index %in% c("MIC", "skinny", "stringy2", "splines2d")) |> 
  group_by(id, index, n) |> 
  mutate(value = (value - min(value)) / (max(value) - min(value))) |>
  ggplot(aes(x = dist, y = value, 
             group = interaction(index, id, n), 
             color = index)) +
  geom_line() + 
  scale_color_brewer(palette = "Dark2") +
  xlab("Projection distance") + ylab("Index value") +
  theme_bw() + 
  theme(legend.position = "bottom")
```


@fig-squintability-ill illustrates some simulated traces for different PP index functions: MIC, splines2d, skinny, and stringy2. It is expected that for a PP index with high squint angle, the optimization \eqref{eq-optimization} should make substantial progress early on. Conversely, for a PP index with low squint angle, it might take a long while for the optimization to make substantial progress, as the candidate projections would need to be very close to the optimal one for the structure of the index function to be visible enough to be amenable to efficient optimization. These empirical observations suggest that the half-point index value improvement percentage provides an appropriate
mathematical definition of squintability, which matches the intuition
behind this concept, while being amenable to numerical computation.

\begin{defn}[squintability]\label{def:squintability}
Let $g: \mathbb{R} \mapsto  \mathbb{R}$ be a decreasing function that maps the projection distance $r(A, A^*)$ to the index value $f(XA)$, such that $g(r) = g(r(A, A^*)) = f(XA)$.  The squintability of an index function $f$ is defined by 

\begin{equation}
\varsigma(f) = \frac{g(r_{0}/2)-g(r_{0})}{g(0)-g(r_{0})} \in [0,1]
\label{eq-squintability}
\end{equation}

\end{defn}

where $r_0 = r(A_0, A^*)$ is the projection distance from the initial projection $A_0$ to the optimal one $A^*$. Remark that the amount by which the index value can improve from the
starting matrix $A_0$ to the optimal one $A^*$ is given by
$g(0)-g(r_{0})$. As a result, equation \eqref{eq-squintability}
represents the proportion of this maximum improvement which has been
achieved by the time the distance $r_0$ to the optimal matrix has been
reduced by half ($r_0/2$).

To compute the squintability metric \eqref{eq-squintability} in
practice, several approaches are possible. The first one is to propose a
parametric model for $g$, and use it to obtain an explicit formula for
$\varsigma$. Numerical experiments suggest a scaled sigmoid shape as
described below. Define $$
\ell(x):=\frac{1}{1+\exp(\theta_{3}(x-\theta_{2}))}\ ,
$$ {#eq-logistic}

which is a decreasing logistic function depending on two parameters
$\theta_2$ and $\theta_3$, such that $\ell(\theta_{2})=\frac{1}{2}$.
Then, define $$
g(x)=(\theta_{1}-\theta_{4})\frac{\ell(x)-\ell(r_0)}{\ell(0)-\ell(r_0)}+\theta_{4}\ ,
$$ {#eq-parametric}

which depends on three additional parameters, $\theta_1$, $\theta_4$,
and $r_0$, such that $g(0)=\theta_1$ and $g(r_0)=\theta_4$. Under the
parametric model (@eq-parametric), the squintability metric
\eqref{eq-squintability} can be shown to be equal to $$
\varsigma=\frac{g\left(r_{0}/2\right)-\theta_{4}}{\theta_{1}-\theta_{4}}=\frac{\ell(r_0/2)-\ell(r_0)}{\ell(0)-\ell(r_0)}\ .
$$ {#eq-squintability-parametric}

In practice, the parameters of this model (@eq-parametric) can be
estimated numerically, for example by non-linear least squares, and then
used to evaluate $\varsigma$ as in equation
(@eq-squintability-parametric).

Alternatively, one can estimate \eqref{eq-squintability} in a
nonparametric way, for example by fitting $g$ using kernel regression,
then numerically estimate $\varsigma$ from its definition
\eqref{eq-squintability}.

# The jellyfish search optimizer {#sec-JSO}

The Jellyfish Search Optimizer (JSO) mimics the natural movements of
jellyfish, which include passive and active motions driven by ocean
currents and their swimming patterns, respectively. In the context of
optimization, these movements are abstracted to explore the search
space, aiming to balance exploration (searching new areas) and
exploitation (focusing on promising areas). The algorithm aims to find
the optimal solution by adapting the behavior of jellyfish to navigate
towards the best solution over iterations [@chou_novel_2021].

To solve the optimization problem embedded in the PPGT with JSO, a starting
projection, an index function, the number of jellyfish, and the maximum
number of iterations are provided as input. Then, the current
projection is evaluated by the index function. The projection is then
moved in a direction determined by a random time control function, $c_t$, whose value decreases as 
the number of trials increases (subject to randomness) to guide the exploration and exploitation phases 
of the algorithm. When $c_t \ge 0.5$, new 
directions will be taken like a jellyfish explores with the ocean trend, defined as 
the difference of the current best jellyfish and average of all the current jellyfish.
When $1- c_t$ is smaller than a randomly generated number in $[0, 1]$, jellyfish will move passively with small random perturbations, otherwise, it will move actively towards or away from another jellyfish based on their index values. 
A new projection is accepted if it is an improvement compared to the
current one, rejected otherwise. This process continues and iteratively
improves the projection, until the pre-specified maximum number of
trials is reached.

::: {.callout-note icon="false"}
## Algorithm: Jellyfish Optimizer Pseudo Code

**Input**: `current_projections`, `index_function`, `trial_id`,
`max_trial`

**Output**: `optimized_projection`

**Initialize** `current_best` as the projection with the best index
value from `current_projections`, and `current_idx` as the array of
index values for each projection in `current_projections`

**for** each `trial_id` in 1 to `max_tries` **do**

> Calculate the time control value, $c_t$, based on `trial_id` and
> `max_trial`

> **if** $c_t$ is greater than or equal to $0.5$ **then**
>
> > Define the ocean current trend as the difference of the `current_best` and the average of the `current_projections`
>
> > Update each projection towards the ocean trend using a random factor and
> > orthonormalisation
>
> **else**
>
> > **if** a random number is greater than $1 - c_t$ **then**
> >
> > > Slightly adjust each projection with a small random factor
> > > (passive)
> >
> > **else**
> >
> > > For each projection, compare with a random jellyfish in `current_projections`
> > > and adjust towards or away from it based on their corresponding `current_idx` (active)
>
> Update the orientation of each projection to maintain consistency
>
> Evaluate the new projections using the index function

> **if** any new projection is worse than the current, revert to the
> `current_projections` for that case
>
> > Determine the projection with the best index value as the new
> > `current_best`

> **exit**

**return** the set of projections with the updated `current_best` as the
`optimized_projection`
:::

<!-- Var names in the function and in the text needs to be consistent??? -->

The JSO implementation involves several key parameters that control its
search process in optimization problems. While the specific implementation
details can vary depending on the version of
the algorithm or its application, the focus is on two main parameters
that are most relevant to our application: the number of jellyfish and
the maximum number of iterations.

# Assessing the optimizers {#sec-sim-deets}

This section explains the details of two simulation studies: (1)
a comparison between the JSO and an existing optimizer, creeping random
search [@RJ-2021-105; @laa_using_2020], and (2) a modelling study to examine 
how factors (smoothness, squintability, data dimension, and JSO hyper-parameters)
affect its success rate in detecting holes and sine-wave pattern across 
a collection of index functions.

## Performance of JSO relative to CRS {#sec-app-1}

The CRS is the main optimization routine currently used for the guided
tour. Here the two optimizers are compared on the task of finding the 2D
pipe structure using the `holes` index in data at dimensions
6, 8, 10, and 12. Additional variables are Gaussian noise. JSO uses 100 
jellyfish with 100 iterations, while CRS allows up to 1000 trials per 
iteration terminates if no better projection is found within these 
trials. These choices enable fair comparison between CRS and JSO, 
that conforms to how they are used in practice. Each task is 
repeated 50 times to evaluate the performance between the two optimizers. 

The performance of the optimizers is measured by the success rate, which
is defined as the proportion of simulations that achieves a final index
value within 0.05 of the best index value found among all 50
simulations. @fig-success-rate illustrates why the choice of 0.05 is
reasonable: the pipe is not recognizable in the projected data. This is
motivated by @laa_using_2020's approach to investigating PP indexes.

The results of the simulation are collected using the data structure
used in @RJ-2021-105 for assessing PP optimizers. The design parameters
are stored along with index value, projection basis, and random seed used.

```{r}
#| label: fig-success-rate
#| fig.cap: "How success rate is calculated, illustrated using the optimal projections from 50 optimizations of 8D pipe data, optimised by CRS, sorted by index value. The pipe shape is recognizable in the projection index values between 0.933-0.969. Of the 50 simulations, 43 achieved an index value within 0.05 of the best, resulting in a success rate of 0.86."
#| fig-pos: "!ht"
# pipe_jellyfish_12d <- pipe_better |> 
#   filter(dim == 2) |> 
#   mutate(dim = as.numeric(dim) * 8, sim = as.numeric(sim)) 
# proj_dt <- map_dfr(pipe_jellyfish_12d |> pull(basis),
#               ~as_tibble(pipe1000_8d %*% .x), .id = "sim") |>
#   mutate(sim = as.numeric(sim)) |>
#   left_join(pipe_jellyfish_12d |> select(sim, index_val), by = "sim")
# 
# idx_val_sorted <- proj_dt |> pull(index_val) |> unique() |> sort()
# proj_dt |>
#   ggplot(aes(x = V1, y = V2)) +
#   geom_point(size = 0.5) +
#   geom_text(aes(label = sprintf("%.3f", index_val)), x = 0, y = 3.6,
#             size = 5) +
#   xlim(-4, 4) + ylim(-4, 4) +
#   facet_wrap(~fct_reorder(as.factor(sim), -index_val), ncol = 10) +
#   theme_bw() +
#   theme(aspect.ratio = 1, axis.ticks = element_blank(),
#         axis.text = element_blank(), axis.title = element_blank(),
#         panel.grid = element_blank(), strip.background = element_blank(),
#         strip.text = element_blank()) 
knitr::include_graphics(here::here("figures", "success-rate.png"))
```

## Factors affecting JSO success rate: JSO hyper-parameters and index properties {#sec-app-2}

To examine the performance of JSO relative to hyper-parameter choices,
the `holes` index is used to find the 2D pipe in data of dimensions
4, 6, 8, 10, and 12. The levels of hyper-parameters are 20, 50, and 
100 jellyfish and a maximum of 50 and 100 attempts. Each task is repeated
fifty times to calculate the success rate as described in @sec-app-1.

This set of simulations is expanded to assess the performance of JSO
relative to the index properties, with a second data set type and
additional PP indexes. The second data set has a sine wave in two
dimensions and Gaussian noise in the remaining dimensions. Seven
additional PP indexes that should be able to extract the sine wave are
used (dcor, loess, MIC, TIC, splines, skinny and stringy2) following @laa_using_2020. 
(The supplementary material includes
definitions of these PP indexes, and descriptions of the pipe and sine
wave data simulation.) The JSO hyper-parameters, number of jellyfish and
maximum iterations, are also included. This results in a total of
`r nrow(sim_df)` scenarios, comprising of
`r nrow(filter(sim_df, index == "holes"))` computed on the pipe data and
`r nrow(filter(sim_df, index != "holes"))` on the sine wave data. The
choice of 4-12 dimensions covers the range of easy to relatively
difficult for the optimizers to detect the structured projections.
<!-- A list of combinations of the data shape, dimensions and PP indexes that are considered in this study is provided in @tbl-smoothness-squintability.-->
The same approach is used, that each experiment is repeated 50 times and success
rate is calculated at each level of the simulation design. 

```{r}
#| label: fig-smoothness
#| out.width: 100%
#| fig.cap: "Steps for calculating smoothness in a projection pursuit problem: 1) sample random bases given the orthonormality contraint and calculate their corresponding index values, 2) fit a Gaussian process model of index values against the bases to obtain the smoothness measure, 3) take the smoothness parameter from the Gaussian process as the smoothness metric."
# basis_smoothness <- sample_bases(idx = "splines2d", n_basis = 1000)
# basis_smoothness |>
#    ggplot(aes(x = dist, y = index)) +
#    geom_point() + 
#    labs(x = "Projection distance", y = "Index value") + 
#    theme_bw() +   
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
knitr::include_graphics(here::here("figures", "smoothness.png"))
```

Recall that smoothness and squintability are metrics used to characterize the index 
function for PP tasks and are invariant to the optimizer and its 
hyper-parameters. Thus simulations with different JSO hyper-parameters 
for the same PP task share the same smoothness value. In total, smoothness and
squintability metrics are calculated on the `r nrow(smoothness)` PP tasks in our simulation.

@fig-smoothness describes the procedure to compute smoothness. For each PP task,
500 random bases are simulated and index values are calculated for each. A Gaussian
process model is then fitted to the resulting data to obtain the smoothness
measure for the index, as described in @sec-smoothness. 

```{r}
#| label: fig-squintability
#| out.width: 100%
#| fig.cap: |
#|   Steps for calculating squintability in a projection pursuit problem: 1) sample random bases given the orthonormality and projection distance contraint and calculates their corresponding index values, 2) interpolate the sampled bases to the optimal basis and calculate the projection distance and the index value. 3) average the index values by projection distances at each 0.005 distance bin, 4) fit the scaled sigmoid function in Equation \eqref{eq-logistic} and \eqref{eq-parametric} , 5) calculate the squintability metric using Equation (@eq-squintability-parametric).
# p1 <- sim_sine_6d_spline_projdist |> 
#   ggplot(aes(x = proj_dist, y = index_val)) +
#   geom_point(size = 0.1, alpha = 0.5) + 
#   labs(x = "Projection distance", y = "Index value") + 
#   theme_bw() + 
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
# 
# 
# dist_bin <- ceiling(sim_sine_6d_spline_projdist[["proj_dist"]] / 0.1) * 0.1
# dt <- sim_sine_6d_spline_projdist |>
#   dplyr::bind_cols(dist_bin = dist_bin) |>
#   dplyr::group_by(dist_bin) |>
#   dplyr::summarise(index_val = mean(index_val, na.rm = TRUE))
# 
# p2 <-  dt |> 
#   ggplot(aes(x = dist_bin, y = index_val)) + 
#   geom_line() +
#   geom_point() + 
#   labs(x = "Projection distance", y = "Index value") + 
#   theme_bw() + 
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
knitr::include_graphics(here::here("figures", "squintability.png"))
```

@fig-squintability describes the procedure to compute squintability. For each 
PP task, 50 random bases are generated and interpolated to the optimal basis 
with a step size of 0.005. The index values are averaged over the distance 
window of 0.005. Index values are calculated for all the interpolated bases and 
averaged over the distance bin of 0.005. The logisitic function in
Equation (@eq-logistic) and (@eq-parametric) is fitted with non-linear least squares 
and calculate the squintability measure using Equation 
(@eq-squintability-parametric).

A generalized linear model is fitted using a quasibinomial family and a
logit link function to assess the factors affecting the success rate.
Predictors are smoothness, squintability, data dimension, and JSO
hyper-parameters. For numerical stability related to small squintability values associated with the stringy index (squintability less than 0.002), we decide to use the rank of the two metrics, rather than the nominal values, in the model. 

# Results {#sec-sim-res}

This section summarises the findings from the simulations that compare
the JSO performance with the existing CRS optimizer, and the
relationship between optimization success and hyper-parameter choices
and PP index properties.

```{r}
#| fig.height: 7.5
#| fig.width: 10
#| label: fig-proj
#| fig.align: "center"
#| fig-pos: "!ht"
#| fig.cap: "Visual comparison of JSO and CRS results, using optimal data projections obtained over 50 simulations of the pipe data. Rows correspond to data dimension. Columns correspond to quantiles of the index values, with 0 being the minimum, 100 being the maximum, and 50 the median. JSO achieves the better views of the pipe generally than CRS. As dimension increases both have more difficulty finding the pipe."

aaa <- map_dfr(
  list(pipe1000, pipe1000_8d, pipe1000_10d, pipe1000_12d), 
  ~{
    d_dim = dim(.x)[2]
    pipe_jellyfish |> 
      filter(d == d_dim) |> 
      arrange(index_val) |>
      filter(row_number() %in% c(1, seq(5, 50, 5))) |> 
      mutate(id = factor(
        paste0(c(0, seq(5, 50, 5)) / 50 * 100),
        levels = c(0, seq(5, 50, 5) / 50 * 100))) |> 
      compute_projection(data = .x, col = c("d", "index_val", "id"))
  }
)

p1 <- aaa |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = aaa |> select(-V1, -V2) |> unique(), 
            aes(label = sprintf("%.3f", index_val)),
            x = 0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

bbb <- pipe_better |> 
  group_by(dim) |> 
  arrange(index_val) |> 
  filter(row_number() %in% c(1, seq(5, 50, 5))) |>  
  mutate(id = factor(
    paste0(c(0, seq(5, 50, 5)) / 50 * 100),
        levels = c(0, seq(5, 50, 5) / 50 * 100)), 
    d = 4 + 2 * as.numeric(dim)) |> 
  unnest(proj)

p2 <- bbb |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = bbb |> select(-V1, -V2) |> unique(), 
            aes(label = sprintf("%.3f", index_val)), x = 0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

(p1 + ggtitle("a. JSO"))/(p2 + ggtitle("b. CRS"))
```

## Performance of JSO relative to CRS

The optimal projection is compared between JSO and CRS for the pipe data at
dimensions 6, 8, 10, and 12 in @fig-proj. The columns show the quantile of 
index value in the 50 optimal projections, with 0 being the minimum, 100 being 
the maximum, and 50 the median. The purpose is to summarize the views of the data resulting from
different optimization, and hence compare results between the two
optimizers. Generally, the JSO does a more consistent job of finding the
pipe structure clearly. As the data dimension increases, both optimizers
struggle and less clearly capture the circle shape.

## Effect of hyper-parameters effect on JSO success rate

The effect of JSO hyper-parameters (number of jellyfish and the maximum
number of iteration) on the success rate is summarized in @fig-proportion.
Bootstrap resampling is used to quantify the uncertainty of the success rate through 500 bootstrap samples with replacement from the original 50 simulation results for each task: optimizing a high-dimensional data (d = 4, 6, 8, 10, or 12) with different number of jellyfish ( 20, 50, or 100) and maximum number of iteration (50 or 100). 
As the number of jellyfish and maximum iteration allowed 
increase, the success rate also increases. For problems in lower dimensional (4,
6) search spaces, small parameter values (20 jellyfish and 50 iterations) 
are sufficient to achieve a high success rate. Many more
jellyfish (100 jellyfish and 100 iterations) are needed for finding the 
pipe structure in d = 10, and 12. This suggests
that even JSO will struggle with high dimensions, and require
substantially more computational time to have an acceptable success rate,
especially for difficult PP indexes based on scagnostics.


```{r}
#| label: fig-proportion
#| fig.width: 8
#| fig.height: 3.5
#| fig.align: "center"
#| fig.cap: "Summary of the relationship between JSO hyper-parameters on optimization success rate, using the holes index for pipe data of dimensions 4-12. Bootstrap samples show the variability in success rate. Success rate mostly plateaus by 50 jellyfish. The JSO has some difficulty finding the pipe when dimension is higher than 8 and maximum number of iteration has little effect."
pipe_sim_best <- sim_pipe_run_best |> 
  group_by(n_jellies, max_tries, d) |>
  summarise(I_best = max(I_max))
  
pipe_res <- sim_pipe_run_best |> 
  left_join(pipe_sim_best) |> 
  mutate(diff = abs(I_max - I_best) < 0.05) |> 
  group_by(n_jellies, max_tries, d) |> 
  summarise(proportion = sum(diff) / n()) |> 
  mutate(d = as.factor(d), n_jellies = as.factor(n_jellies))

res_df <- sim_pipe_run_best |> 
  left_join(pipe_sim_best |> ungroup() |> mutate(case_id = row_number()),
            by = c("n_jellies", "max_tries", "d")) |> 
  ungroup() |> 
  group_split(case_id)

resampling <- function(dt, size = size, replace = TRUE){
  row_id <- sample(1:50, size = size, replace = replace)
  
  dt |> filter(row_number() %in% row_id) |> 
    mutate(diff = abs(I_max - I_best) < 0.05) |> 
    summarise(proportion = sum(diff) / n()) |> 
    pull(proportion)
  
}

res_interval_df <- tibble(df = res_df) |> 
  crossing(rep = 1:500) |> 
  rowwise() |> 
  mutate(proportion = resampling(df, size = 50)) 

res_quantile <- res_interval_df |> 
  unnest(df) |> 
  group_by(n_jellies, max_tries, case_id, d) |> 
  select(-I_max, -id) |> 
  distinct() |> 
  mutate(d = as.factor(d),
         n_jellies = ifelse(d == 10, n_jellies - 1, n_jellies),
         n_jellies = ifelse(d == 12, n_jellies + 1, n_jellies)) |> 
  rename(`max # of iteration` = max_tries) 
  
res_quantile |> 
  ungroup() |> 
  ggplot(aes(x = n_jellies - 0.5, y = proportion, 
             group = d, color = d)) + 
  geom_segment(aes(xend = n_jellies + 0.5), size = 0.05) + 
  geom_segment(aes(x = n_jellies - 1, xend = n_jellies + 1), 
               data = res_quantile |> summarise(proportion = mean(proportion)), 
               size = 2) + 
  geom_line(aes(x = n_jellies), 
               data = res_quantile |> summarise(proportion = mean(proportion)), 
            alpha = 0.3) + 
  scale_color_brewer(palette = "Dark2", name = "dimension") +
  scale_x_continuous(breaks = c(20, 50, 100), labels = c("20", "50", "100")) +
  facet_wrap(vars(`max # of iteration`), 
             labeller = "label_both") + 
  theme_bw() +
  theme(panel.grid.minor = element_blank()) + 
  xlab("Number of jellyfish") + 
  ylab("Proportion of success")
```

```{r}
#| label: tbl-smoothness-squintability
#| tbl-cap: The columns $\nu$ and $\varsigma$ show the smoothness and squintability measures for three PP indexes for the sine wave data. The splines index has the largest squint angle and is the smoothest. The skinny index has low smoothness and smallest squint angle. As dimension increases the smoothness becomes smaller but squintability doesn't change much.
dt <- tibble(shape = c(rep("pipe", 5), rep("sine", 18)), smoothness) |>
  left_join(squintability) |> 
  rename(d = n,  smooth = smoothness) |> 
  mutate(index = as.character(index),
         index = ifelse(index == "splines2d", "splines", index),
         variance = sqrt(variance)) 
dt |> 
  filter(shape == "sine", index %in% c("TIC", "splines", "skinny")) |>
  mutate(index = factor(index, levels = c("splines", "TIC", "skinny"))) |> 
  select(shape, index, d, smooth, squint) |>
  arrange(index, d) |>
  knitr::kable(
    digits = 4, format = "latex", 
    col.names = c("shape", "index", "d", "$\\nu$", "$\\varsigma$"), linesep = c("", "", "\\hline"),
    booktabs = TRUE, escape = FALSE, row.names = FALSE, align = "|llr|rr") |> 
  #column_spec(1, border_left = TRUE) |>
  column_spec(c(3,5), border_right = TRUE) 
  #row_spec(c(3, 6), extra_latex_after = "\\cline{1-5}")
  #kable_styling(font_size = 10)   
```

```{r}
#| label: fit-logistic
sim_df2 <- sim_df |>  
  # the original smoothness and squintability score is in the data
  # but we use their rank in the modelling.
  select(-smoothness, -squintability) |> 
  rename(smoothness = smoothness_rank,
         squintability = squintability_rank) |>
  mutate(n_jellies = n_jellies/10, max_tries = max_tries/10) |> 
  rename(proportion = P_J) 

mod1 <- glm(proportion ~ smoothness + squintability + d + n_jellies + max_tries, data = sim_df2, family = quasibinomial) 
mod2 <- glm(proportion ~ squintability + d + n_jellies + max_tries, data = sim_df2, family = quasibinomial)
mod3 <- glm(proportion ~ smoothness + d + n_jellies + max_tries, data = sim_df2, family = quasibinomial)
mod4 <- glm(proportion ~ squintability*d + n_jellies, data = sim_df2, family = quasibinomial)
mod5 <- glm(proportion ~ smoothness*d + squintability*d + n_jellies, data = sim_df2, family = quasibinomial)
```


```{r}
#| label: fig-idx-proj-dist
#| fig.width: 8
#| fig.height: 4
#| fig.cap: "Five traces of index value against projection distance for each of the three indexes (TIC, spline, and skinny) on 6D sine wave data. The traces of the splines index are smooth, with a gradual change between noise and otimal projection. In contrast, the skinny index has a noisy trace that increases rapidly near the optimum. This illustrates the usefulness of the new measures for squintability and smoothness."
sq_basis_df_6d_TIC_spline_skinny |>
  ggplot(aes(x = dist, y = index, group = interaction(index_name, id), color = index_name)) +
  geom_line() + 
  scale_color_brewer(palette = "Dark2", name = "Index function") +
  xlab("Projection distance") + ylab("Index value") +
  theme_bw() + 
  theme(legend.position = "bottom")
```

```{r}
#| tbl-pos: "!ht"
#| label: tbl-mod-output
#| tbl-cap: "Jellyfish success rate relative to index properties and jellyfish hyper-parameters. This is the summary from a logistic regression fit to smoothness, squintability, dimension, number of jellyfish and maximum number of iterations. Interestingly, squintability and dimension strongly affect jellyfish optimization success. The number of jellies marginally affects success, but index smoothness, and increasing the number of iterations do not."
#| fig-pos: t
set_gtsummary_theme(theme_gtsummary_journal("jama"))
mod1 |> tbl_regression(exponentiate = TRUE, 
                       pvalue_fun = label_style_pvalue(digits = 3),
                       label = list(smoothness = "Smoothness", 
                                    squintability = "Squintability",
                                    d = "Dimension", 
                                    n_jellies = "Number of jellyfish", 
                                    max_tries = "Maximum number of iterations")) |>
  bold_p(t = 0.1) 
```

```{r}
#| tbl-pos: "!ht"
#| label: tbl-joint-output
summary_table <- #bind_rows(
  glance(mod1) |> #%>% mutate(Model = "Full"),
  #glance(mod2) %>% mutate(Model = "Squintability"),
  #glance(mod3) %>% mutate(Model = "Smoothness")
#  glance(mod4) %>% mutate(Model = "Interaction")
#) |>
  select(#Model, 
    deviance, df.residual, null.deviance, df.null, #logLik, 
         ) |> #, AIC, BIC) %>% 
  rename(
    `Null Deviance` = null.deviance,
    `DF Null` = df.null,
    `Deviance` = deviance,
    `DF Residual` = df.residual#,
    #`Log-Likelihood` = logLik,
  ) |>
  mutate(across(where(is.numeric), ~ round(.x, 2)))
summary_table |>
  kable(caption = "Fit statistics for the model.") |>
  kable_styling(font_size = 10) |>
  row_spec(0, bold = TRUE) #, width = "5em")
```


## Effect of index properties on JSO success rate


@tbl-smoothness-squintability presents values computed for a subset of
PP indexes on the sine wave data for dimensions 4-8. (A full table of
the parameters estimated from the Gaussian process <!-- (outputscale $\eta$,
lengthscale $\ell$, smoothness $\nu$, and nugget $\sigma$)--> and from the
scaled logistic function <!--($\theta_1$ to $\theta_4$)--> for calculating
smoothness and squintability for all PP indexes and data combinations is
included in the supplementary materials). The column $\nu$ is used as
the smoothness metric and the column $\varsigma$ is calculated as
equation (@eq-squintability-parametric) as the squintability metric.

@fig-idx-proj-dist shows a sample of traces from the indexes in
@tbl-smoothness-squintability for the 6D sine wave data. This traces the
PP index values computed for projections along an interpolated path
between a randomly generated noise projection and the optimal projection (sine wave). (Note
index values were self-standardized to range between 0 and 1, for
comparison.) The splines index is very smooth with a gradual increase
in value between the noise projection and the optimal projection. The
TIC index is slightly less smooth and slightly less gradual change in
value. Least desirable is the skinny index which is noisier and the
optimization needs to be much closer to the optimal angle to have a
higher value. This illustrates how the new measures for squintability
and smoothness work to describe the PP indexes.

@tbl-mod-output presents the results from fitting a logistic regression
model using the proportion of success as the response and smoothness,
squintability, dimension, number of jellyfish and maximum number of
iteration, as predictors. The fit suggests that the JSO success rate is only
affected by squintability, dimension and number of jellyfish. As expected,
the JSO success rate is higher with higher squintability and/or more
jellyfish, and lower when dimension is higher (higher-dimensional
optimization is more difficult). Interestingly, the JSO is unaffected by
the smoothness of the index function. This is consistent with the way
random search algorithms jump from value to value without taking local
regularity into account, as opposed to gradient-based optimizers for
example. Allowing for more iterations also does not affect success rate
significantly. A unit increase in squintability increases the success
rate by `r round(100 * (exp(broom::tidy(mod1)$estimate[3])-1), 0)`%. As
dimension increases by one the success rate almost halves. Increasing
the number of jellies by 10 increases the success rate by
`r round(100 * (exp(broom::tidy(mod1)$estimate[5])-1), 0)`%.
@tbl-joint-output shows the fit statistics supporting that the model is
reasonably strong.

In relation to the model fit, it should be noted that there is moderate correlation between squintability and smoothness, but this does not substantially affect the fit. Fitting smoothness alone results in this variable being weakly significant for explaining success rate, but the effect is dominated by squintability. The addition of interaction terms for squintability and data
dimension, and smoothness and data dimension, improves the model slightly, but not enough to be practically important or change the interpretation above. The CRS optimizer couldn't be used here because it is not efficient (@fig-proj) but we would expect that smoothness does not overly affect it either. Both JSO and CRS are expected to be robust to small local fluctuations.

These results also support the use of the proposed
measures to effectively capture key factors affecting the success
rate of PP.

# Practicalities {#sec-discussion}

Using the JSO optimizer for PPGT in the `tourr` package requires a change in user behavior. For
all the existing optimizers a single optimization path is followed. The
JSO has multiple optimization paths, and needs additional tools to
incorporate into the PPGT, as explained here.

## Using the JSO in a PPGT

To use JSO for PPGT in the `tourr` package, specify
`search_f = search_jellyfish` in the guided tour. 
The animation function `animate_*()` won't directly render the animated 
projection for JSO, as with other existing optimizers since multiple tour 
paths will need to be generated. To
visualize the path visited by each individual jellyfish, assign the
animation to an object (`res <- animate_xy(...)`). This will save all the
bases visited by JSO, along with relevant metadata, as a tibble data
object [see @RJ-2021-105 for more details on the data object]. Users can 
then view the path of individual jellyfish through extracting the relevant 
bases recorded and construct it as a planned tour (using the `planned_tour()` function).
Users may wish to view the 
optimisation path of the most successful jellyfish, or a random selection of jellyfish, 
or some specific runs that find local maximum.

## Computing the index properties for your new index

The `ferrn` package [@RJ-2021-105] has now included functionality for computing
the smoothness and squintability metrics. Both metrics require sampling
random bases using the `sample_bases()` function, followed by the
calculation with `calc_smoothness()` or `calc_squintability()`.

### Smoothness

To sample bases for calculating smoothness, the following inputs are
required: the index function, the dataset, and the number of random
bases to sample. The output of `sample_bases()` is a data frame with a
list-column of sampled bases and index values. Parallelization
is available to speed up the index value calculation through the
`parallel = TRUE` argument.

The `calc_smoothness()` function takes the output from `sample_bases()`
and fits a Gaussian process model to the index values against the
sampled bases, as the location, to obtain the smoothness metric.
Starting parameters and additional Gaussian process arguments can be
specified and details of the fit can be accessed through the `fit_res`
attribute of the output.

### Squintability

Bases sampling for calculating squintability includes an additional step of
interpolating between the sampled bases and the optimal basis. This step is
performed when the arguments `step_size` and `min_proj_dist` in the
`sample_bases()` function are set to non-NA numerical values. Given the
projection distance typically ranging from 0 to 2, it is recommended to
set `step_size` to 0.1 or lower, and `min_proj_dist` to be at least 0.5
to ensure a meaningful interpolation length.

The `calc_squintability()` function computes squintability using two
methods: 1) parametrically, by fitting a scaled sigmoid function through
non-linear least squares (`method = "nls"`), and 2) non-parametrically,
using kernel smoothing (`method = "ks"`). A `bin_width` argument is
required to average the index values over the projection distance before
the fitting. For the parametric case, the output provides the estimated
parameters for the logistic function ($\theta_1$ to $\theta_4$)
and the calculated squintability metric as Equation
(@eq-squintability-parametric). For the non-parametric case, it shows
the maximum gradient attained (`max_d`), the corresponding projection
distance (`max_dist`), and the squintability metric as their products.

# Conclusion {#sec-conclusion}

This paper has presented new metrics to mathematically define desirable
features of PP indexes, squintability and smoothness, and used these to
assess the performance of the new JSO. The
metrics will be generally useful for characterizing PP indexes, and help
with developing new indexes.

In the comparison of the JSO against the currently used CRS, as expected
the JSO vastly outperforms CRS, and provides a high probability of
finding the global optimum. The JSO obtains the maximum more cleanly,
with a slightly higher index value, and plot of the projected data
showing the structure more clearly.

The JSO performance is affected by the hyper-parameters, with a higher
chance of reaching the global optimum when more jellyfish are used and
the maximum number of iteration is increased. However, it comes at a
computational cost, as expected. The performance declines if the
projection dimension increases and if the PP index has low
squintability. The higher the squintability the better chance the JSO
can find the optimum. However, interestingly smoothness does not affect
the JSO performance.

Future work can focus on developing more efficient optimization algorithm for
noise indexes with low squintability metric, which resemble a needle-in-a-haystack 
problem [@siemenn2023fast]. Additionally, if the index function is differentiable, automatic 
differentiation can be used to implement new gradient-based optimizers [@prince2023understanding].




<!--
- Summarise the result of the simulation: JSO immense improvement on current methods, ...
- JSO for a guided tour, what changes are needed? Do we follow one tentacle, do we perform off-line and then pull tentacles out to watch, how does it integrate with ferrn


Main findings: 

  - JSO tends to obtian a clearer projection and higher index value when searching higher dimension spaces than CRS (and other current optimizers) - Figure 4
  
  - Increasing JS hyper-parameter enhances its performance, but the computational time required for the optimization can quickly become intensive, especially when evaluating the index function (such as scagnostic indexes) multiple times across numerous iterations. - Figure 5
  
  - Smoothness and squintability can be calculated and compared across different problems to understand their relative complexity. Once a projection pursuit problem is fully characterised (by the shape-to-find, the index function used, and the data dimension), increasing JSO hyper-parameters can enhance the search effectiveness, however, the resulting increase in computational complexity will also need to be considered. - modelling
-->

# Acknowledgement

The article has been created using Quarto [@Allaire_Quarto_2022] in R
[@R]. The source code for reproducing the work reported in this paper
can be found at: <https://github.com/huizezhang-sherry/paper-jso>. The
simulation data produced in Section 5 can be found at
<https://figshare.com/articles/dataset/Simulated_raw_data/26039506>.
Nicolas Langrené acknowledges the partial support of the Guangdong
Provincial/Zhuhai Key Laboratory IRADS (2022B1212010006) and the UIC
Start-up Research Fund UICR0700041-22.

The R packages used in this work include: `tidyr` [@tidyr], `dplyr`
[@dplyr], `ggplot2` [@ggplot2], `knitr` [@knitr],`gtsummary`
[@gtsummary], `patchwork` [@patchwork], `ggh4x` [@ggh4x], `broom`
[@broom], `kableExtra` [@kabelextra], `ferrn` [@RJ-2021-105], and
`cassowaryr` [@cassowaryr].

# Supplementary materials {.unnumbered}

The supplementary materials available at
<https://github.com/huizezhang-sherry/paper-jso> include: 1) details of
the indexes used in the simulation study, 2) the script to get started
with using JSO in a PPGT and calculating smoothness and squintability,
as explained in @sec-discussion, (3) the table of the Gaussian
process parameters and logistic function parameters for all 23 PP problems investigated, along with (4) the full code to reproduce the plots and summaries in this paper.

# References
