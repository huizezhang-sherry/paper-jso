---
title: Studying the Performance of the Jellyfish Search Optimiser for the Application of Projection Pursuit
author:
  - name: H. Sherry Zhang
    email: huize.zhang@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Austin
          country: United States
          postal-code: 78751
    attributes:
        corresponding: true
  - name: Dianne Cook
    email: dicook@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
  - name: Nicolas  Langrené
    email: nicolaslangrene@uic.edu.cn
    affiliations:
        - id: 3
          name: Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International College
          department: Department of Mathematical Sciences
          city: Zhuhai
          country: China
          postal-code: 519087
  - name: Jessica Wai Yin Leung
    email: Jessica.Leung@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
abstract: |
  The projection pursuit (PP) guided tour interactively optimises a criteria function known as the PP index, to explore high-dimensional data by revealing interesting projections. Optimisation of some PP indexes can be non-trivial, if they are non-smooth functions, or the optima has a small "squint angle", detectable only from close proximity. To address these challenges, this study investigates the performance of a recently introduced swarm-based algorithm, Jellyfish Search Optimiser (JSO), for optimising PP indexes. The performance of JSO for visualising data is evaluated across various hyper-parameter settings and compared with existing optimisers. Additionally, methods for calculating the smoothness and squintability properties of the PP index are proposed. They are used to assess the optimiser performance in the presence of PP index complexities. A simulation study illustrates the use of these performance metrics to compare the JSO with existing optimisation methods available for the guided tour.  The JSO algorithm has been implemented in the R package, `tourr`, and functions to calculate smoothness and squintability are available in the `ferrn` package. 
keywords: 
  - projection pursuit
  - jellyfish search optimiser (JSO)
  - optimisation
  - grand tour
  - high-dimensional data
  - exploratory data analysis
date: last-modified
bibliography: bibliography.bib
format:
  tandf-pdf:
    keep-tex: true
    fontsize: 12pt
    linestretch: 2
editor: 
  markdown: 
    wrap: 72
crossref: 
  eq-prefix: ""
editor_options: 
  chunk_output_type: console
header-includes:
   - \usepackage{algorithm}
   - \usepackage{amsmath}
   - \usepackage{amsthm}
   - \usepackage{amssymb}
   - \theoremstyle{plain}
   - \newtheorem{assumption}{\protect\assumptionname}
   - \newtheorem{claim}{\protect\claimname}
   - \newtheorem{condition}{\protect\conditionname}
   - \newtheorem{conjecture}{\protect\conjecturename}
   - \newtheorem{cor}{\protect\corollaryname}
   - \newtheorem{defn}{\protect\definitionname}
   - \newtheorem{example}{\protect\examplename}
   - \newtheorem{lem}{\protect\lemmaname}
   - \newtheorem{notation}{\protect\notationname}
   - \newtheorem{problem}{\protect\problemname}
   - \newtheorem{prop}{\protect\propositionname}
   - \newtheorem{rem}{\protect\remarkname}
   - \newtheorem{thm}{\protect\theoremname}
   - \providecommand{\assumptionname}{Assumption}
   - \providecommand{\claimname}{Claim}
   - \providecommand{\conditionname}{Condition}
   - \providecommand{\conjecturename}{Conjecture}
   - \providecommand{\corollaryname}{Corollary}
   - \providecommand{\definitionname}{Definition}
   - \providecommand{\examplename}{Example}
   - \providecommand{\lemmaname}{Lemma}
   - \providecommand{\notationname}{Notation}
   - \providecommand{\problemname}{Problem}
   - \providecommand{\propositionname}{Proposition}
   - \providecommand{\remarkname}{Remark}
   - \providecommand{\theoremname}{Theorem}
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(ggh4x)
library(broom)
library(kableExtra)
library(tourr)
library(PPtreeViz)
library(ferrn)
library(colorspace)
load(here::here("data/sim_pipe_run_best.rda"))
load(here::here("data/sim_sine_6d_spline_head.rda"))
load(here::here("data/sim_sine_6d_spline_best.rda"))
load(here::here("data/sim_sine_6d_spline_projdist.rda"))
load(here::here("data/pipe_better.rda"))
load(here::here("data/pipe_jellyfish.rda"))
load(here::here("data/smoothness.rda"))
load(here::here("data/squintability.rda"))
load(here::here("data/sq_basis_dist_idx.rda"))
load(here::here("data/sim_df.rda"))
```

# Introduction

<!--  
```{=html}
\begin{assumption}\label{assum:my_assumption}
This is an assumption.
\end{assumption}
Assumption \ref{assum:my_assumption} is convenient.

\begin{claim}\label{claim:my_claim}
This is a claim.
\end{claim}
Claim \ref{claim:my_claim} is suspicious.

\begin{condition}\label{cond:my_condition}
This is a condition.
\end{condition}
Condition \ref{cond:my_condition} is necessary.

\begin{conjecture}\label{conj:my_conjecture}
This is a conjecture.
\end{conjecture}
Conjecture \ref{conj:my_conjecture} is intriguing.

\begin{cor}\label{cor:my_corollary}
This is a corollary.
\end{cor}
Corollary \ref{cor:my_corollary} is trivial.

\begin{defn}\label{def:my_definition}
This is a definition.
\end{defn}
Definition \ref{def:my_definition} is clear.

\begin{example}\label{ex:my_example}
This is an example.
\end{example}
Example \ref{ex:my_example} is enlightening.

\begin{lem}\label{lem:my_lemma}
This is a lemma.
\end{lem}
Lemma \ref{lem:my_lemma} is helpful.

\begin{notation}\label{not:my_notation}
This is a notation.
\end{notation}
Notation \ref{not:my_notation} is cumbersome.

\begin{problem}\label{prob:my_problem}
This is a problem.
\end{problem}
Problem \ref{prob:my_problem} is difficult.

\begin{proof}\label{proof:my_proof}
This is a proof.
\end{proof}
Proof \ref{proof:my_proof} is compelling.

\begin{prop}\label{prop:my_prop}
This is a proposition.
\end{prop}
Proposition \ref{prop:my_prop} is useful.

\begin{rem}\label{rem:my_remark}
This is a remark.
\end{rem}
Remark \ref{rem:my_remark} is interesting.

\begin{thm}\label{thm:my_theorem}
This is a theorem.
\end{thm}
Theorem \ref{thm:my_theorem} is important.
```
-->

Projection pursuit (PP) (@kr69, @FT74, @huber85) is a dimension reduction technique aimed at identifying informative linear projections of data. This is useful for exploring high-dimensional data, and creating plots of the data that reveal the main features to use for publication. The method involves optimising an objective function known as the PP index (e.g. @hall1989polynomial, @cook1993projection,
@lee2010projection, @Loperfido2018, @Loperfido2020), which defines the criteria for what constitutes interesting or informative projections. Let $X \in \mathbb{R}^{n\times p}$ be the data matrix, $A \in\mathbb{R}^{p \times d}$ be an orthonormal matrix, where $A$ belongs to the Stiefel manifold $\mathcal{A} = V_d(\mathbb{R}^p)$. The projection $Y = XA$ is a linear transformation that maps data from a $p$-dimensional space into a $d$-dimensional space. The index function $f(XA): \mathbb{R}^{n \times d} \to \mathbb{R}$ is a scalar function that measures an interesting aspect of the projected data, such as deviation from normality, presence of clusters, non-linear structure, or other features of interest. For a fixed sample of data, PP finds the orthonormal basis $A$ that maximises the index value of the projection, $Y = XA$:

$$
\underset{A \in \mathcal{A}}{\max } \quad f(XA) \quad \text{subject to} \quad A'A = I_d
$$ {#eq-optimization}

It is interesting to note that when using PP visually, one cares less about $A$ than the plane described by $A$, because the orientation in the plane is irrelevant. The space of planes belongs to a Grassmann manifold. This is usually how the projection pursuit guided tour (PPGT) [@cook1995grand] operates, when using geodesic interpolation between starting and target planes. It interpolates plane to plane, removing irrelevant within plane spin, and is agnostic to the basis ($A$) used to define the plane. Thus, indexes which are used for the PPGT should be rotationally invariant.

Index functions are quite varied in form, partially depending on the data that is being projected. @fig-example-functions shows two examples. Huber plots [@huberplot] of 2D data sets are in (a) and (c), showing the PP index values for all 1D projections of the 2D data in polar coordinates, which reveals the form of these functions. The dashed circle is a baseline set at the average value, and the straight line marks the optimal projection. Plots (b) and (d) show the respective best projections of the data as histograms. Indexes like the holes, central mass and skewness [@cook1993projection] are generally smooth for most data sets, but capture only large patterns. Many indexes noisy and non-convex, requiring an effective and efficient optimisation procedure to explore the data landscape and achieve a globally optimal viewpoint of the data. The skewness index computed for trimodal data, in (a), is smooth with a large squint angle but has three modes, and thus is not convex. The binned normality index (a simple version of a non-normality index as described in @huber85) computed on the famous RANDU data, in (c), is noisier and has a very small squint angle. The discreteness cannot be seen unless the optimiser is very close to the optimal projection.

```{r}
#| label: huber-plot-function
#| echo: false
huber_plot <- function (origdata2D, origclass=NULL, 
  PPmethod = "LDA", weight = TRUE, r = 1, lambda = 0.5, 
  opt.proj = TRUE, UserDefFtn = NULL, ...) 
{
    index <- NULL
    best.proj <- NULL
    best.index <- 0
    origdata2D <- as.matrix(origdata2D)
    for (i in 0:360) {
        theta <- pi/180 * i
        proj.data <- matrix(cos(theta) * origdata2D[, 1] + sin(theta) * 
            origdata2D[, 2])
        proj <- matrix(c(cos(theta), sin(theta)), ncol = 1)
        if (PPmethod == "LDA") {
            newindex <- PPtreeViz::LDAindex(origclass, origdata2D, proj = proj, 
                weight = weight)
        }
        else if (PPmethod == "PDA") {
            newindex <- PPtreeViz::PDAindex(origclass, origdata2D, proj, 
                weight = weight, lambda = lambda)
        }
        else if (PPmethod == "Lr") {
            newindex <- PPtreeViz::Lrindex(origclass, origdata2D, proj, 
                weight = weight, r = r)
        }
        else if (PPmethod == "GINI") {
            newindex <- PPtreeViz::GINIindex1D(origclass, origdata2D, proj)
        }
        else if (PPmethod == "ENTROPY") {
            newindex <- PPtreeViz::ENTROPYindex1D(origclass, origdata2D, 
                proj)
        }
        else if (PPmethod == "holes") {
          newindex <- tourr::holes()(proj.data)
        }
        else if (PPmethod == "skewness") {
          newindex <- tourr::skewness()(proj.data)
        }
        else if (PPmethod == "norm_bin") {
          newindex <- tourr::norm_bin(nrow(proj.data))(proj.data)
        }
        else if (PPmethod == "UserDef") {
            newindex <- UserDefFtn(proj.data, ...)
        }
        index <- c(index, newindex)
    }
    sel.index <- which(index[1:360] > signif(max(index), 6) - 
        1e-06)
    theta.best.all <- pi/180 * (sel.index - 1)
    theta.best <- theta.best.all[1]
    proj.data.best <- matrix(cos(theta.best) * origdata2D[, 1] + 
        sin(theta.best) * origdata2D[, 2])
    index.best <- max(index)
    range <- round(max(index) - min(index), 5)
    if (range == 0) {
      PPindex <- rep(4, length(index))
    }
    else {
      PPindex <- (index - min(index))/range * 2 + 3
    }
    data.circle <- NULL
    data.index <- NULL
    for (i in 1:361) {
        theta <- pi/180 * (i - 1)
        data.index <- rbind(data.index, c(PPindex[i] * cos(theta), 
            PPindex[i] * sin(theta)))
        data.circle <- rbind(data.circle, c(4 * cos(theta), 4 * 
            sin(theta)))
    }
    maxdiff <- max(c(diff(range(origdata2D[, 1])), diff(range(origdata2D[, 
        2]))))
    orig.scaled <- apply(origdata2D, 2, function(x) (x - mean(x))/maxdiff * 
        3.5)
    data.cX <- data.circle[, 1]
    data.cY <- data.circle[, 2]
    data.X <- data.index[, 1]
    data.Y <- data.index[, 2]
    plot.data <- data.frame(data.cX, data.cY, data.X, data.Y)
    x <- orig.scaled[, 1]
    y <- orig.scaled[, 2]
    if (is.null(origclass)) 
      group <- rep(1, nrow(origdata2D))
    else
      group <- origclass
    point.data <- data.frame(x, y, group)
    min.X <- min(unlist(plot.data))
    max.X <- max(unlist(plot.data))
    if (is.null(origclass)) {
      P1 <- ggplot(data = plot.data, aes(x = data.X, y = data.Y)) + 
        geom_path() + geom_path(aes(x = data.cX, y = data.cY), 
        linetype = "dashed") + geom_point(data = point.data, 
        aes(x = x, y = y)) + scale_x_continuous(breaks = NULL) + 
        scale_y_continuous(breaks = NULL) + xlab("") + ylab("") + 
        coord_fixed() + theme_bw() + theme(panel.border = element_blank())
    } else {
      P1 <- ggplot(data = plot.data, aes(x = data.X, y = data.Y)) + 
        geom_path() + geom_path(aes(x = data.cX, y = data.cY), 
        linetype = "dashed") + geom_point(data = point.data, 
        aes(x = x, y = y, color = group, shape = group)) +
        scale_x_continuous(breaks = NULL) + 
        scale_y_continuous(breaks = NULL) + xlab("") + ylab("") + 
        coord_fixed() + theme_bw() + theme(panel.border = element_blank())
    }
    if (opt.proj) {
        P1 <- P1 + geom_abline(intercept = 0, slope = sin(theta.best)/cos(theta.best))
        if (length(theta.best.all) > 1) 
            for (i in 2:length(theta.best.all)) P1 <- P1 + geom_abline(intercept = 0, 
                slope = sin(theta.best.all[i])/cos(theta.best.all[i]), 
                linetype = "dashed")
    }
    best.proj.data <- proj.data.best
    #group <- origclass
    if (is.null(origclass)) {
      hist.data <- data.frame(best.proj.data)
      P2 <- ggplot(data = hist.data, aes(x = best.proj.data)) + 
        geom_histogram()
    } else {
      hist.data <- data.frame(best.proj.data, group)
      P2 <- ggplot(data = hist.data, aes(x = best.proj.data, 
                                         group = group)) + 
        geom_histogram(aes(fill = group), position = "stack")
    }
    #gridExtra::grid.arrange(P1, P2, nrow = 1)
    return(list(plot=P1, proj_data=best.proj.data))
}


```

```{r}
#| label: fig-example-functions
#| echo: false
#| fig-width: 9
#| fig-height: 6
#| out-width: 80%
#| fig-cap: "Examples of PP indexes with large (top row) and small (bottom row) squint angles, shown with a Huber plot, and histogram of the projected data corresponding to the optimal projection. A Huber plot shows the PP index values for all 1D data projections in polar coordinates."
data(randu)
randu_std <- as.data.frame(apply(randu, 2, function(x) (x-mean(x))/sd(x)))
randu_std$yz <- sqrt(35)/6*randu_std$y-randu_std$z/6

#proj <- animate_xy(flea[,1:6], guided_tour(lda_pp(flea$species)))
best_proj <- matrix(c(-0.44157005,  0.80959831,
                      0.14622089, -0.08228193,
                      -0.08454314, -0.23234287,
                      0.70367852,  0.28185021,
                      0.27876825,  0.44819691,
                      0.45123453,  0.05896646), ncol=2, byrow=T)
flea2D <- as.matrix(flea[,1:6]) %*% best_proj
colnames(flea2D) <- c("X1", "X2")
flea2D <- as.data.frame(flea2D)

# Make both plots
large_sqa <- huber_plot(flea2D, PPmethod="skewness")
small_sqa <- huber_plot(randu_std[c(1,4)], PPmethod="norm_bin")

sqa1 <- large_sqa$plot + ggtitle("(a) Skewness index")
lsqa_d <- tibble(d = large_sqa$proj_data[,1])
sqa2 <- ggplot(lsqa_d, aes(x=d)) + 
  geom_histogram(binwidth=0.25, colour="white") +
  xlab("") + ylab("") +
  ggtitle("(b) Optimally projected data") +
  theme_bw() +
  theme(axis.text.y = element_blank())
sqa3 <- small_sqa$plot + ggtitle("(c) Binned normality index")
ssqa_d <- tibble(d = small_sqa$proj_data[,1])
sqa4 = ggplot(ssqa_d, aes(x=d)) + 
  geom_histogram(breaks = seq(-2.2, 2.4, 0.12)) +
  xlab("") + ylab("") +
  ggtitle("(d) Optimally projected data") +
  theme_bw() +
  theme(axis.text.y = element_blank())

sqa1 + sqa2 + sqa3 + sqa4 + plot_layout(ncol=2, widths=c(2,2))
```

Optimisation of PP is often discussed when new indexes are proposed [@posse1995; @marie-sainte2010; @grochowski2011]. @cook1995grand tied the optimisation more closely to the index, when they introduced the PPGT, which monitors the optimisation visually so that the user can see the projected data leading in and out of the optima. An implementation is available in the `tourr` package [@tourr] in R [@R]. @RJ-2021-105 illustrated how to diagnose optimisation processes, particularly focusing on the guided tour, and revealed a need for improved optimisation. While improving the quality of the optimisation solutions in the tour is essential, it is also important to be able to view the data projections as the optimisation progresses. Integrating the guided tour with a global optimisation algorithm that is efficient in finding the global optimal and enables viewing of the projected data during the exploration process is a goal.

Here, the potential for a Jellyfish Search Optimiser (JSO) (see @chou_novel_2021, and @rajwar_exhaustive_2023) for the PPGT is explored. JSO, inspired by the search behaviour of jellyfish in the ocean, is a swarm-based metaheuristic designed to solve global optimisation problems. Compared to traditional methods, JSO has demonstrated stronger search ability and faster
convergence, and requires fewer tuning parameters. These practicalities
make JSO a promising candidate for enhancing PP optimisation.
<!--The study reported here explores JSO's potential in enhancing projection pursuit and examines the characteristics
and advantages of such implementation.-->

The primary goal of the study reported here is to investigate the
performance of JSO in PP optimisation for the guided tour. It is of
interest to assess how quickly and closely the optimiser reaches a gobal
optima, for various PP indexes that may have differing complexities. To
observe the performance of JSO with different types of PP indexes,
metrics are introduced to capture specific properties of the index
including squintability (based on @barnett1981interpreting's squint
angle) and smoothness. Here, we mathematically define metrics for squintability and smoothness, which is a new contribution for PP research. A series of simulation experiments using various datasets and PP indexes are conducted to assess JSO's behaviour and its sensitivity to hyper-parameter choices (number of jellyfish and maximum number of tries). The relationship between the JSO performance, hyper-parameter choices and properties of PP indexes (smoothness and squintability) is analysed to provide guidance on selecting optimisers for practitioners using projection pursuit. Additionally, this work should guide the design of new PP indexes and facilitate better optimization for PP.

The paper is structured as follows. @sec-background introduces the
background of the PPGT, reviews existing optimisers and index
functions in the literature. @sec-theory describes JSO and introduces the 
metrics that measure different properties of PP indexes, smoothness and
squintability. @sec-sim-deets summarises the results of two simulation experiments done to assess JSO's performance: one comparing JSO's performance improvements relative to an existing optimiser, Creeping Random Search (CRS), and the other studying the impact of different PP index properties on optimisation performance. @sec-sim-res presents the results. @sec-discussion discusses implementation details and insights for practitioners. @sec-conclusion summarises the work and provides suggestions for future
directions.

# Projection pursuit, tours, index functions and optimisation {#sec-background}

<!-- Need to assume that the special issue will have the general introduction to PP, so the focus here will be on background needed for this paper, tours, guidance using PP, optimisation, visualisation, ... -->

A tour on high-dimensional data is constructed by geodesically
interpolating between pairs of planes. Any plane is described by an
orthonormal basis, $A_t$, where $t$ represents time in the sequence. The
term "geodesic" refers to maintaining the orthonormality constraint so
that each view shown is correctly a projection of the data. The PP
guided tour operates by geodesically interpolating to target planes
(projections) which have high PP index values, as provided by the
optimiser. The geodesic interpolation means that the viewer sees a
continuous sequence of projections of the data, so they can watch
patterns of interest forming as the function is optimised. There are
five (unsophisticated) optimisation methods implemented in the `tourr`
package:

-   `search_geodesic()`: provides a pseudo-derivative optimisation. It
    searches locally for the best direction, based on differencing the
    index values for very close projections. Then it follows the
    direction along the geodesic path between planes, stopping when the
    next index value fails to increase.
-   `search_better()`: also known as Creeping Random Search (CRS), is a
    brute-force optimisation searching randomly for projections with
    higher index values.
-   `search_better_random()`: is essentially simulated annealing
    [@Bertsimas93] where the search space is reduced as the optimisation
    progresses.
-   `search_posse()`: implements the algorithm described in @posse95.
-   `search_polish()`: is a very localised search, to take tiny steps to
    get closer to the local maximum.

There are several PP index functions available: `holes()` and `cmass()`
[@cook1993projection]; `lda_pp()` [@lee2005projection]; `pda_pp()`
[@lee2010projection]; `dcor2d()` and `splines2d()` [@Grimm2016];
`norm_bin()` and `norm_kol()` [@huber85]; `slice_index()`
[@Laa:2020wkm]. Most are relatively simply defined, for any projection
dimension, and implemented because they are relatively easy to optimise.
A goal is to be able to incorporate more complex PP indexes, for
example, based on scagnostics (@scag, @WW08).

An initial investigation of PP indexes, and the potential for
scagnostics is described in @laa_using_2020. To be useful here an
optimiser needs to be able to handle index functions which are possibly
not very smooth. In addition, because data structures might be
relatively fine, the optimiser needs to be able to find maxima that
occur with a small squint angle, that can only be seen from very close
by. One last aspect that is useful is for an optimiser to return local
maxima in addition to global because data can contain many different and
interesting features.

# The jellyfish optimiser and properties of PP indexes {#sec-theory}

JSO mimics the natural movements of jellyfish, which include passive and
active motions driven by ocean currents and their swimming patterns,
respectively. In the context of optimization, these movements are
abstracted to explore the search space, aiming to balance exploration
(searching new areas) and exploitation (focusing on promising areas).
The algorithm aims to find the optimal solution by adapting the
behaviour of jellyfish to navigate towards the best solution over
iterations [@chou_novel_2021].

To solve the optimisation problem embedded in the PP guided tour, a
starting projection, an index function, the number of jellyfish, and the
maximum number of trials (tries) are provided as input. Then, the
current projection is evaluated by the index function. The projection is
then moved in a direction determined by a random factor, influenced by
how far along we are in the optimisation process. Occasionally,
completely new directions may be taken like a jellyfish might with ocean
currents. A new projection is accepted if it is an improvement compared
to the current one, rejected otherwise. This process continues and
iteratively improves the projection, until the pre-specified maximum
number of trials is reached.

::: {.callout-note icon="false"}
## Algorithm: Jellyfish Optimizer Pseudo Code

**Input**: `current_projections`, `index_function`, `trial_id`,
`max_trial`

**Output**: `optimised_projection`

**Initialize** `current_best` as the projection with the best index
value from `current_projections`, and `current_idx` as the array of
index values for each projection in `current_projections`

**for** each `trial_id` in 1 to `max_tries` **do**

> Calculate the time control value, $c_t$, based on `current_idx` and
> `max_trial`

> **if** $c_t$ is greater than or equal to $0.5$ **then**
>
> > Define trend based on the `current_best` and `current_projections`
>
> > Update each projection towards the trend using a random factor and
> > orthonormalisation
>
> **else**
>
> > **if** a random number is greater than $1 - c_t$ **then**
> >
> > > Slightly adjust each projection with a small random factor
> > > (passive)
> >
> > **else**
> >
> > > For each projection, compare with a random jellyfish and adjust
> > > towards or away from it (active)
>
> Update the orientation of each projection to maintain consistency
>
> Evaluate the new projections using the index function

> **if** any new projection is worse than the current, revert to the
> `current_projections` for that case
>
> > Determine the projection with the best index value as the new
> > `current_best`

> **if** `trial_id` $\ge$ `max_trial`, print the last best projection
> **exit**

**return** the set of projections with the updated `current_best` as the
`optimised_projection`
:::

<!-- Var names in the function and in the text needs to be consistent??? -->

The JSO implementation involves several key parameters that control its
search process in optimization problems. These parameters are designed
to guide the exploration and exploitation phases of the algorithm. While
the specific implementation details can vary depending on the version of
the algorithm or its application, the focus is on two main parameters
that are most relevant to our application: the number of jellyfish and
the maximum number of tries.

@laa_using_2020 has proposed five criteria for assessing projection
pursuit indexes (smoothness, squintability, flexibility, rotation
invariance, and speed). Since not all index properties affect the
optimisation process, the focus here is on the first two properties,
*smoothness* (@sec-smoothness) and *squintability* (@sec-squintability),
for which metrics are proposed to quantify them.

## Smoothness {#sec-smoothness}

This subsection proposes a metric for the smoothness of a projection
pursuit index.

A classical way to describe the smoothness of a function is to identify
how many continuous derivatives of the function exist. This can be
characterized by Sobolev spaces [@adams2003sobolev].

```{=tex}
\begin{defn}\label{def:sobolev_space}
The Sobolev space $W^{k,p}(\mathbb{R})$ for $1\leq p\leq \infty$ is the set of all functions $f$ in $L^p(\mathbb{R})$ for which all weak derivatives $f^{(\ell)}$ of order $\ell\leq k$ exist and have a finite $L^p$ norm.
\end{defn}
```
The Sobolev index $k$ in Definition \ref{def:sobolev_space} can be used
to characterize the smoothness of a function: if $f\in W^{k,p}$, then
the higher $k$, the smoother $f$. While this Sobolev index $k$ is a
useful measure of smoothness, it can be difficult to compute or even estimate
in practice.

To obtain a computable estimator of the smoothness of the index function
$f$, we propose an approach based on random fields. If a PP index
function $f$ is evaluated at some random bases, as is done at the
initialization stage of JSO, then these random index values can be
interpreted as a random field, indexed by a space parameter, namely the
random projection basis. This analogy suggests to use this random
training sample to fit a spatial model. We propose to use a Gaussian
process equipped with a Matérn covariance function, due to the
connections between this model and Sobolev spaces, see for example
@porcu2024matern.

The distribution of a Gaussian process is fully determined by its mean
and covariance function. The smoothness property comes into play in the
definition of the covariance function: if a PP index is very smooth,
then two close projection bases should produce close index values
(strong correlation); by contrast, if a PP index is not very smooth,
then two close projection bases might give very different index values
(fast decay of correlations with respect to distance between bases). Popular covariance functions are parametric positive semi-definite
functions.  In particular, the Matérn class of
covariance functions has a dedicated parameter to capture the smoothness of
the Gaussian field.

```{=tex}
\begin{defn}
The Matérn covariance function $K$ is defined by
\begin{equation}
K(u)=K_{\nu,\eta,\ell}(u):=\eta^2\frac{\left(\sqrt{2\nu}\frac{u}{\ell}\right)^{\nu}}{\Gamma(\nu)2^{\nu-1}}\mathcal{K}_{\nu}\left(\sqrt{2\nu}\frac{u}{\ell}\right)\ , u\geq0\label{eq:matern}
\end{equation}
where $\nu>0$ is the smoothness parameter, $\eta$ is the outputscale, $\ell$ is the lengthscale, and $\mathcal{K}_\nu$ is
the modified Bessel function @DLMF. A multivariate extension $K(u)$, $u\in\mathbb{R}^d$ can be obtained by products of univariate covariance functions \eqref{eq:matern}.
\end{defn}
```
The Matérn covariance function can be expressed analytically when $\nu$
is a half-integer, the most popular values in the literature being
$\frac{1}{2}$, $\frac{3}{2}$ and $\frac{5}{2}$ [@rasmussen2006gaussian]. The parameter $\nu$,
called *smoothness parameter*, controls the decay of the covariance
function. As such, it is an appropriate measure of smoothness of a
random field, as shown by the simulations on @fig-matern-1d and @fig-matern-2d. For example, @karvonen2023asymptotic showed that if a
function $f$ has a Sobolev index of $k$, then the smoothness parameter
estimate $\nu$ in \eqref{eq:matern} cannot be asymptotically less than
$k$. See the survey @porcu2024matern for additional results on the connection between the Matérn model and Sobolev spaces. An interesting result is that the asymptotic case
$\nu\rightarrow\infty$ coincides with the Gaussian kernel:
$K_\infty(u)=\exp(-u^{2}/2)$.



```{r}
#| label: fig-matern-1d
#| out.width: 100%
#| fig.cap: |
#|   Five random simulations from a Gaussian Process defined on $\mathbb{R}$ with zero mean and Matérn-$\nu$ covariance function, with $\nu=1$ (left), $\nu=2$ (middle), and $\nu=4$ (right), showing that higher values of $\nu$ produce smoother curves.
knitr::include_graphics(here::here("figures", "matern_simulation_1d.png"))
```

```{r}
#| label: fig-matern-2d
#| out.width: 100%
#| fig.cap: |
#|   One random simulation from a Gaussian Process defined on $\mathbb{R}^2$ with zero mean and Matérn-$\nu$ covariance function, with $\nu=1$ (left), $\nu=2$ (middle), and $\nu=4$ (right), showing that higher values of $\nu$ produce smoother surfaces.
knitr::include_graphics(here::here("figures", "matern_simulation_2d.png"))
```


In view of these results, the parameter $\nu$ is suggested as a measure
of the smoothness of the PP index function by fitting a Gaussian process
prior with Matérn covariance on a dataset generated by random
evaluations of the index function, as done at the initialization stage
of the jellyfish search optimization. There exist several R packages,
such as `GpGp` [@guinness2021gpgp] or `ExaGeoStatR` [@abdulah2023large],
to fit the hyperparameters of a GP covariance function on data, which is
usually done by maximum likelihood estimation. In this project, the
`GpGp` package is used.

```{=tex}
\begin{defn}
Let $\mathbf{A}=[A_1, \ldots, A_N] \in (\mathbf{R}^{p \times d})^N$ be d-dimensional projection bases, let $\mathbf{y}=[f(XA_1),\ldots,f(XA_N)]$ be the corresponding PP index values, and let $\mathbf{K}=[K_\theta(A_{i},A_{j})]_{1\leq i,j\leq N}\in\mathbb{R}^{N\times N}$ be the Matérn covariance matrix evaluated at the input bases, where the vector $\theta$ contains all the parameters of the multivariate Matérn covariance function $K$ (smoothness, outputscale, lengthscales). The log-likelihood of the parameters $\theta$ is defined by 
\begin{equation}
\mathcal{L}(\theta)=\log p(\mathbf{y}\left|\mathbf{A},\theta\right.)=-\frac{1}{2}\mathbf{y}^{\top}(\mathbf{K}+\sigma^{2}\mathbf{I})^{-1}\mathbf{y}-\frac{1}{2}\mathrm{\log}(\det(\mathbf{K}+\sigma^{2}\mathbf{I}))-\frac{N}{2}\log(2\pi)\,.\label{eq:gp_log_likelihood}
\end{equation}
where the nugget parameter $\sigma$ is the standard deviation of the intrinsic noise of the Gaussian process.
The optimal parameters (including smoothness) are obtained by maximum log-likelihood
\begin{equation}
\theta^* = \underset{\theta}{\max}\mathcal{L}(\theta)
\end{equation}
The resulting optimal smoothness parameter $\nu$ is chosen as our smoothness metric.
\end{defn}
```

The value of the optimal smoothness parameter $\nu>0$ can be
naturally interpreted as follows: the higher $\nu$, the smoother the
index function.

## Squintability {#sec-squintability}

Here the formal definition of projection distance and squint angle are given, before the definition of squintability. Two approaches to compute this metric numerically,
are described.


```{=tex}
\begin{defn}[projection distance]\label{def:proj-dist}
Recall that $A \in \mathbf{R}^{p \times d}$ is a $d$-dimensional orthonormal matrix. Let $A^*$ be the optimal matrix that achieves the maximum index value for a given data. The projection distance $d(A, A^*)$ between $A$ 
and $A^*$ is defined as 
$d(A, A^*) = \lVert AA^\prime - A^*A^{*\prime}\  \rVert _F$
where $\lVert . \rVert _F$ denotes the Frobenius norm, given by
$\lVert M \rVert _F = \sqrt{\sum_{ij} M_{ij}^2}$. 
\end{defn}
```


```{=tex}
\begin{defn}[squint angle]\label{def:squint-angle}
Let $A$ and $B$ be two $d$-dimensional orthonormal matrices in $\mathbb{R}^p$. The squint angle $\theta$ between the subspace spanned by $A$ and $B$ is defined as the smallest principal angle between these subspaces: $\theta = \min_{i \in \{1, \cdots, d\}} \arccos(\tau_i)$, where $\tau_i$ are the singular values of the matrix $M = A^T B$ obtained from its singular value decomposition.

\end{defn}
```

Squintability can be defined as how the index value $f(XA)$ changes with
respect to the projection distance $d(A, A^*)$, over the course of the
JSO:

```{=tex}
\begin{defn}[squintability]\label{def:squintability}
Let $g: \mathbb{R} \mapsto  \mathbb{R}$ be a decreasing function that maps the projection distance $d(A, A^*)$ to the index value $f(XA)$, such that $g(d(A, A^*)) = f(XA)$. For brevity, denote $g(d)$ as $g(d(A, A^*))$. The squintability of an index function $f(XA)$ is defined as 

\begin{equation}
\varsigma(f) = -c \times \max_{d} g'(d) \times \arg \max_{d} g'(d)
\label{eq-squintability}
\end{equation}

where $c$ is a constant scaling factor, $-\max_d g'(d)$ represents the 
largest gradient of $-g$ and $\arg \max_{d} g'(d)$ represents the projection distance at which this largest gradient is attained.

\end{defn}
```

It is expected that these two
values should be both high in the case of high squintability (fast
increase in $f$ early on), and both low in the case of low squintability
(any substantial increase in $f$ happens very late, close to the optimal
angle). This suggests that their product \eqref{eq-squintability} should
provide a sensible measure of squintability. The multiplicative constant
$4$, which can be deemed arbitrary, does not change the interpretation
of the squintability metric $\varsigma$; it is here to adjust the range
of values of $\varsigma$ and simplify the explicit formula for
$\varsigma$ obtained later on.

From the definition, the following proposition can be derived:

\begin{prop}\label{prop:convex-concave}
Let $g_1(d)$ be a convex decreasing function and $g_2(d)$ be a concave decreasing function both defined on [0, 1] where $d \in [0, D]$. Let $d_1 := \underset{d}{\arg \max} |g_1'(d)|$ and $d_2 := \underset{d}{\arg \max} |g_2'(d)|$.

$$\varsigma(g_1) < \varsigma(g_2)$$

For a convex function, $g_1'(d) < 0 \; \forall d$ and is non-decreasing. For a concave function, $g_2'(d) < 0 \; \forall d$ and is non-increasing. As such, it follows that $d_1 < d_2$.

Suppose both functions achieve the same maximum gradient $-g_{\max}$, i.e., $\max g'_1(d_1) = \max g'_2(d_2) = -g_{\max}$,

$$
\varsigma(g_1(d_1)) = -c  \max g'_1(d_1)  d_1 = -c  (-g_{\max}) d_1 \leq -c  (-g_{\max}) d_2 = -c \max g'_2(d_2)  d_2 = \varsigma(g_2(d_2))
$$

\end{prop}

From @barnett1981interpreting and @laa_using_2020, a large squint angle
implies that the objective function value is close to optimal even when
the perfect view to see the structure is far away. A small squint angle
means that the PP index value improves substantially only when the
perfect view is close by. As such, low squintability implies rapid
improvement in the index value when near the perfect view. For PP, a
small squint angle is considered to be undesirable because it means that
the optimiser needs to be very close to be able to "see" the optima.
Thus, it could be difficult for the optimiser to find the optima. The
mathematical formulation of this intuition is proposed below:

It is expected that for a PP index with high squint angle, the
optimization (@eq-optimization) should make substantial progress early
on. Conversely, for a PP index with low squint angle, it might take a
long while for the optimization to make substantial progress, as the
candidate projections would need to be very close to the optimal one for
the structure of the index function to be visible enough to be amenable
to efficient optimization. This observation suggests that the extreme
values of $f'$ (the ones for which $f''=0$, assuming that $f$ is twice
differentiable), and the projection distances for which these values are
attained, are crucial in the mathematical definition of squintability.  


<!-- Noticing that $f$ is expected to be a decreasing function, the -->
<!-- squintability metric is proposed as follows: -->

<!-- $$ -->
<!-- \varsigma:=-4\min_{x}f'(x)\times\underset{x}{\arg\min}f'(x)\ . -->
<!-- $$ {#eq-squintability} -->

<!-- $-\min_{x}f'(x)$ represents the largest value of the gradient of $-f$ -->
<!-- and $\underset{x}{\arg\min}f'(x)$ represents the projection distance at -->
<!-- which this largest gradient is attained.  -->

To compute the squintability metric \eqref{eq-squintability} in practice,
several approaches are possible. The first one is to propose a
parametric model for $f$, and use it to obtain an explicit formula for
$\varsigma$. Numerical experiments suggest a scaled sigmoid shape as
described below. Define

$$
\ell(x):=\frac{1}{1+\exp(\theta_{3}(x-\theta_{2}))}\ ,
$$ {#eq-logistic}

which is a decreasing logistic function depending on two parameters
$\theta_2$ and $\theta_3$, such that $\ell(\theta_{2})=\frac{1}{2}$.
Then, define

$$
f(x)=(\theta_{1}-\theta_{4})\frac{\ell(x)-\ell(x_{\max})}{\ell(0)-\ell(x_{\max})}+\theta_{4}\ ,
$$ {#eq-parametric}

which depends on three additional parameters, $\theta_1$, $\theta_2$,
and $x_{\max}$, such that $f(0)=\theta_1$ and $f(x_{\max})=\theta_4$.
Under the parametric model (@eq-parametric), the squintability metric
\eqref{eq-squintability} can be shown to be equal to

$$
\varsigma=\frac{(\theta_{1}-\theta_{4})\theta_{2}\theta_{3}}{\ell(0)-\ell(x_{\max})}\ .
$$ {#eq-squintability-parametric}

In practice, the parameters of this model (@eq-parametric) can be
estimated numerically, for example by non-linear least squares, and then
used to evaluate $\varsigma$ as in equation
(@eq-squintability-parametric).

Alternatively, one can estimate \eqref{eq-squintability} in a nonparametric
way, for example by fitting $f$ using kernel regression, then
numerically estimate the angle at which $-f'$ attains its highest value.

<!-- \textcolor{orange}{See whether further details are needed for the nonparametric approach.} -->

<!--To the best of our knowledge, -->

<!-- ## Speed -->

<!-- The speed of optimizing an index function can be measured empirically by -->

<!-- recording the duration of the optimisation. Alternatively, one can also -->

<!-- gauge the speed in terms of computational complexity of evaluating the -->

<!-- index function (for instance, in big O notation, with respect to the -->

<!-- sample size) of computing the index function. -->

# Simulation study {#sec-sim-deets}

The JSO performance is compared with an existing optimiser, Creeping
Random Search (CRS) [@RJ-2021-105; @laa_using_2020] used in the PP
guided tour to explore JSO's behaviour under different hyper-parameter
and data dimension combinations. The second simulation studies the
effect of index properties (smoothness and squintability), along with
JSO hyper-parameters, and data dimension, on the success rate of the JSO
performance. This section describes the simulation details, with the
results deferred to @sec-sim-res.

## Performance of JSO relative to CRS {#sec-app-1}

he performance of JSO is investigated both in comparison to the existing
optimizer, CRS, and across various hyper-parameter values. The
performance is measured by the success rate, defined as the proportion
of simulations that achieves a final index value within 0.05 of the best
index value found among all 50 simulations (see @fig-success-rate for an
illustration). This comparison is based on projection pursuit to find
the pipe shape investigated by @laa_using_2020 using the `holes` index.

Fifty simulations are conducted with both JSO and CRS, in four different
data dimensions ($d = 6, 8, 10, 12$). JSO uses 100 jellyfishes with a
maximum of 100 tries, while the CRS allows a maximum of 1000 samples at
each iteration before the algorithm terminates. The different numbers
account for the multiple paths of JSO to enable fairer comparison with
CRS. The results of the simulation are collected using the data
structure proposed in @RJ-2021-105 for assessing JSO, where the design
parameters are stored along with index value, projection basis, random
seed, and computation time.

Fifty additional simulations are conducted for each hyper-parameter
combination to analyze how they affect the JSO success rate. This
includes variations in the number of jellyfish (20, 50, and 100) and the
maximum number of tries (50 and 100).

```{r}
#| label: fig-success-rate
#| fig.cap: "Illustration of success rate calculation: Final projections based on projection pursuit to find the pipe shape in 8D data using the holes index, optimised by CRS, in 50 simulations. The 50 final projections are sorted by their index values. The highest index value found across all simulations is 0.969. Out of the 50 simulations, 43 achieved an index value within 0.05 of the best, resulting in a success rate of 0.86 (43/50)."
# pipe_jellyfish_12d <- pipe_better |> 
#   filter(dim == 2) |> 
#   mutate(dim = as.numeric(dim) * 8, sim = as.numeric(sim)) 
# proj_dt <- map_dfr(pipe_jellyfish_12d |> pull(basis),
#               ~as_tibble(pipe1000_8d %*% .x), .id = "sim") |>
#   mutate(sim = as.numeric(sim)) |>
#   left_join(pipe_jellyfish_12d |> select(sim, index_val), by = "sim")
# 
# idx_val_sorted <- proj_dt |> pull(index_val) |> unique() |> sort()
# proj_dt |>
#   ggplot(aes(x = V1, y = V2)) +
#   geom_point(size = 0.5) +
#   geom_text(aes(label = round(index_val, 3)), x = 0, y = 3.7,
#             size = 8) +
#   xlim(-4, 4) + ylim(-4, 4) +
#   facet_wrap(~fct_reorder(as.factor(sim), -index_val), ncol = 10) +
#   theme_bw() +
#   theme(aspect.ratio = 1, axis.ticks = element_blank(),
#         axis.text = element_blank(), axis.title = element_blank(),
#         panel.grid = element_blank(), strip.background = element_blank(),
#         strip.text = element_blank()) 
knitr::include_graphics(here::here("figures", "success-rate.png"))
```

## Factors affecting JSO success rate: index properties and jellyfish hyper-parameters {#sec-app-2}

To assess JSO's performance across various scenarios, two different data
shapes, pipe and a sine wave, are investigated in 6D and 8D spaces using
six different PP indexes: `dcor2d_2`, `loess2d`, `MIC`, `TIC`, `spline`,
and `stringy`, with varied JSO hyper-parameters. A total of
`r nrow(sim_df)` combinations result, comprising of
`r nrow(filter(sim_df, index == "holes"))` computed on the pipe data and
`r nrow(filter(sim_df, index != "holes"))` on the sine-wave data. Again,
JSO is run 50 times to calculate the success rate for each projection
pursuit.

Smoothness and squintability are computed following the procedures
outlined in @sec-smoothness and @sec-squintability and as illustrated in
@fig-smoothness and @fig-squintability.

To compute smoothness, 300 random bases are simulated. Index values and
projection distance (to the optimal basis) are calculated for each
random basis before fitting the Gaussian process model to obtain the
smoothness measure for the index.

To compute squintability, 50 random bases are sampled and interpolated
to the optimal basis with a step size of 0.005. Index values and
projection distances are calculated for these interpolated bases and the
index values are averaged with a bin width of 0.005. A four-parameter
scaled logistic function is fitted to the index values against
projection distances, estimated by non-linear least squares. The
squintability measure is then calculated as
(@eq-squintability-parametric).

To construct a relationship among success rate, index properties
(smoothness and squintability), and jellyfish hyper-parameters, a
generalised linear model is fitted using a binomial family and a logit
link function. The data is pre-processed by 1) scaling the JSO
hyper-parameters by a factor of 10 for interpretation, 2) creating a new
binary variable `long_time` to indicate cases with an average run time
over 30 seconds, and 3) re-coding the success rate for the `stringy`
index as 0, because none of the 50 simulations correctly identified the
sine-wave shape.

<!-- **Projection distance**: The projection distance between projection -->

<!-- bases and the optimal best basis (or the empirical best basis if -->

<!-- the optimal is unknown) measures how close a visited basis is to  -->

<!-- the best basis and it is calculated as the Frobenius norm in equation -->

<!-- (@eq-proj-dist). Plotting projection distances against index values for the  -->

<!-- bases visited reveals how the index value changes as the basis -->

<!-- approaches the optimal basis, helping to calculate the squintability -->

<!-- measure. -->

```{r}
#| label: fig-smoothness
#| out.width: 100%
#| fig.cap: "Illustration of steps to calculate smoothness. For a given projection pursuit problem defined by the shape to find, data dimension and the index function, 1) sample random bases given the orthonormality contraint, 2) calculate the projection distance and the index value for each random basis, and 3) fit a Gaussian process model of index values against projection distances to obtain the smoothness measure. "
# basis_smoothness <- sample_bases(idx = "splines2d", n_basis = 1000)
# basis_smoothness |>
#    ggplot(aes(x = dist, y = index)) +
#    geom_point() + 
#    labs(x = "Projection distance", y = "Index value") + 
#    theme_bw() +   
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
knitr::include_graphics(here::here("figures", "smoothness.png"))
```

```{r}
#| label: fig-squintability
#| out.width: 100%
#| fig.cap: |
#|   Illustration of steps to calculate squintability. For a given projection pursuit problem defined by the shape to find, data dimension and the index function, 1) sample random bases given the orthonormality and projection distance contraint, 2) interpolate the sampled bases to the optimal basis and calculate the projection distance and the index value for each interpolated basis. 3) bin the index values by projection distances to obtain the average index value for each bin, 4) fit the scaled sigmoid function in equation \eqref{eq-squintability} to the binned index values against projection distances using non-linear least square, 5) calculate the squintability measure using equation (@eq-squintability-parametric) with parameters estimated from the model.
# p1 <- sim_sine_6d_spline_projdist |> 
#   ggplot(aes(x = proj_dist, y = index_val)) +
#   geom_point(size = 0.1, alpha = 0.5) + 
#   labs(x = "Projection distance", y = "Index value") + 
#   theme_bw() + 
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
# 
# 
# dist_bin <- ceiling(sim_sine_6d_spline_projdist[["proj_dist"]] / 0.1) * 0.1
# dt <- sim_sine_6d_spline_projdist |>
#   dplyr::bind_cols(dist_bin = dist_bin) |>
#   dplyr::group_by(dist_bin) |>
#   dplyr::summarise(index_val = mean(index_val, na.rm = TRUE))
# 
# p2 <-  dt |> 
#   ggplot(aes(x = dist_bin, y = index_val)) + 
#   geom_line() +
#   geom_point() + 
#   labs(x = "Projection distance", y = "Index value") + 
#   theme_bw() + 
#   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))
knitr::include_graphics(here::here("figures", "squintability.png"))
```

# Results {#sec-sim-res}

The results from the first simulation described in @sec-sim-deets are
analysed based on the final projections across two optimisers (JSO and
CRS) and the success rate across JSO hyper-parameters. In the second
simulation, smoothness and squintability are calculated across a
collection of pipe-finding and sine-wave finding problems to construct
the relationship between success rate, JSO hyper-parameters, and index
properties.

The final projections found by the two optimisers (JSO and CRS) are
presented in @fig-proj, broken down by 10th quantile, faceted by the
data dimensions. In the 6-dimensional data scenario, JSO consistently
identifies a clear pipe shape. The CRS also finds the pipe shape but
with a wide rim, suggesting a further polish search may be required.
With increasing dimensions, JSO may not always identify the pipe shape
due to random sampling, but it still finds the pipe shape in over 50% of
cases. Compared to CRS, JSO achieves higher index values and clearer
pipe shapes across all quantiles in data of 8, 10, 12 dimensions,
suggesting its advantage in exploring high-dimensional spaces.

The success rate calculated at each hyper-parameter combination (number
of jellyfish and the maximum number of tries) is presented in
@fig-proportion. As the number of jellyfish and maximum tries increase,
the success rate also increases. For simpler problems (6 dimensions),
small parameter values (20 jellyfishes and a maximum of 50 tries) can
already achieve a high success rate. However, larger parameter values
(i.e. 100 jellyfishes and a maximum of 100 tries) are necessary for
higher-dimensional problems (8, 10, and 12 dimensions). Increasing both
parameters enhances the performance of JSO, but it also extends the
computational time required for the optimisation, which can be
computationally intensive when evaluating the index function (such as
scagnostic indexes) multiple times across numerous iterations.

```{r}
#| fig.height = 7.5,
#| fig.width = 10,
#| label = fig-proj,
#| fig.align = "center",
#| fig.cap = "Projections found by the JSO and CRS at each 10th quantile across 50 simulations. The projection pursuit problem is to find the pipe shape using the holes index in the 6, 8, 10, and 12-dimensional spaces. The JSO uses 100 jellyfishes and a maximum number of tries of 100. The CRS uses a maximum of 1000 tries in each step of random sampling step before the algorithm terminates. In the 6-D data space, JSO always finds a clear pipe shape while the CRS also finds the pipe shape but with a wide rim. At higher data dimensions, JSO finds a higher index value and a clearer pipe shape across all the quantiles than the CRS"

aaa <- map_dfr(
  list(pipe1000, pipe1000_8d, pipe1000_10d, pipe1000_12d), 
  ~{
    d_dim = dim(.x)[2]
    pipe_jellyfish |> 
      filter(d == d_dim) |> 
      arrange(index_val) |>
      filter(row_number() %in% c(1, seq(5, 50, 5))) |> 
      mutate(id = factor(
        paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
        levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"))) |> 
      compute_projection(data = .x, col = c("d", "index_val", "id"))
  }
)

p1 <- aaa |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = aaa |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x = 0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

bbb <- pipe_better |> 
  group_by(dim) |> 
  arrange(index_val) |> 
  filter(row_number() %in% c(1, seq(5, 50, 5))) |>  
  mutate(id = factor(
    paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
    levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th")), 
    d = 4 + 2 * as.numeric(dim)) |> 
  unnest(proj)

p2 <- bbb |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = bbb |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x = 0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

(p1 + ggtitle("a. JSO"))/(p2 + ggtitle("b. CRS"))
```

```{r}
#| label = fig-proportion,
#| fig.width = 6,
#| fig.height = 2.5,
#| fig.align = "center",
#| fig.cap = "Proportion of simulations reaches near-optimal index values in the pipe-finding problem using the holes index. The proportion is calculated based on the number of simulations, out of 50, that achieve an index value within 0.05 of the best-performing simulation. As the dimensionality increases, the proportion of simulations reaching the optimal index value increases."
pipe_sim_best <- sim_pipe_run_best |> 
  group_by(n_jellies, max_tries, d) |>
  summarise(I_best = max(I_max))
  
pipe_res <- sim_pipe_run_best |> 
  left_join(pipe_sim_best) |> 
  mutate(diff = abs(I_max - I_best) < 0.05) |> 
  group_by(n_jellies, max_tries, d) |> 
  summarise(proportion = sum(diff) / n()) |> 
  mutate(d = as.factor(d), n_jellies = as.factor(n_jellies))

pipe_res |> 
  rename(`max # of tries` = max_tries) |>
  ggplot(aes(x = n_jellies, y = proportion, 
             group = interaction(d, `max # of tries`),
             color = d)) + 
  geom_line() + 
  scale_color_brewer(palette = "Dark2", name = "dimension") +
  facet_wrap(vars(`max # of tries`), 
             labeller = "label_both") + 
  theme_bw() + 
  xlab("Number of jellyfish") + 
  ylab("Proportion of success")

```

```{r}
sim_df2 <- sim_df |>   
  mutate(n_jellies = n_jellies/10,
         max_tries = max_tries/10,
         long_time = ifelse(time > 20, 1, 0),
         P_J = ifelse(index == "stringy", 0, P_J))

mod1 <- glm(P_J ~ smoothness + squintability + d + long_time + n_jellies + max_tries, data = sim_df2, family = binomial)
```

The index properties, including smoothness and squintability, offer
numerical metrics to characterise the complexity of projection pursuit
optimisation problems. @tbl-smoothness-squintability presents the
parameters for calculating both metrics estimated from the Gaussian
process (variance, range, smooth, and nugget) and the scaled logistic
function ($\theta_1$ to $\theta_4$) for each case considered in the
simulation. The column "smooth" is used as the smoothness metric and the
column "squint" is calculated as equation (@eq-squintability-parametric)
as the squintability metric. @tbl-mod-output presents the results of
fitting a generalised linear model with a binomial family and a logit
link function to a sample of data in @tbl-mod-data, where all the three
components (success rate, JS hyper-parameters, and index properties) are
combined. The model suggests that JSO success rate is positively
associated with the two hyper-parameters, as well as with the index
properties: smoothness and squintability. Specifically, using 10 more
jellyfish and 10 more tries increases the odd ratio of success by
`r round(100 * (exp(broom::tidy(mod1)$estimate[6])-1), 2)`% and
`r round(100 * (exp(broom::tidy(mod1)$estimate[7])-1), 2)`%,
respectively. However, being flagged with long runtime and an increase
of data dimension reduce the success rate by
`r round(exp(broom::tidy(mod1)$estimate[5]) * 100, 2)`% and
`r round(exp(broom::tidy(mod1)$estimate[4]) * 100, 2)`%, respectively.
The variable `squintability` and `dimension` are significant, suggesting
their importance relative to JSO hyper-parameters in the optimisation
success.

```{r fig-idx-proj-dist, eval = FALSE}
#| fig.width = 9,
#| fig.height = 5,
#| fig.cap = "Index values versus projection distance for the 12 pipe/sine-wave finding problem, after the binning procedure for calculating the squintability measure. The index values, averaged at bin width of 0.005, are scaled from 0 to 1 for comparison (holes, TIC, and stringy). The `MIC` and `TIC` index curves are convex while others are concave. The stringy curve shows an instantaneous jump to the optimum when aproaching the best basis." 
sq_basis_dist_idx |>
  ggplot(aes(x = dist, y = index, group = index_name)) +
  geom_line() + 
  facet_nested_wrap(~index_name + n, nrow = 3, labeller = label_both) + 
  xlab("projection distance") + ylab("index value") +
  theme_bw()
```

```{r}
#| label: tbl-smoothness-squintability
#| tbl-cap: Parameters estimated from the Gaussian process (including variance, range, smoothness, and nugget) and scaled logistic function ($\theta_1$ to $\theta_4$) for the pipe-finding and sine-wave finding problems.  The column "smooth" and "squint" represent the smoothness and squintability measures. 
dt <- tibble(shape = c(rep("pipe", 4), rep("sine", 8)), smoothness) |>
  left_join(squintability) |> 
  rename(d = n, smooth = smoothness) |> 
  mutate(index = ifelse(index == "dcor2d_2", "dcor2d", index)) 
dt |> 
  knitr::kable(
    digits = 2, format = "latex", 
    col.names = c(
      "shape", "index", "d",
      "variance", "range", "smooth", "nugget",
      "$\\theta_1$", "$\\theta_2$", "$\\theta_3$", "$\\theta_4$", "squint"), linesep = "",
    booktabs = T, escape = FALSE, row.names = TRUE) |> 
  column_spec(1, border_left = TRUE) |> 
  column_spec(c(4, 8, 13), border_right = TRUE) |> 
  kable_styling(font_size = 10) 
```

```{r}
#| label: tbl-mod-data
#| tbl-cap: The first 7 rows of the datasets processed for modelling. 
sim_df |> 
  dplyr::select(index, d, smoothness, 
    squintability, n_jellies, max_tries, 
    -I_max, P_J, time) |> 
  mutate(time = as.numeric(time)) |>
  arrange(index, d, n_jellies, max_tries) |> 
  head(7) |> 
  kable(digits = 2, format = "latex", 
        col.names = c("index", "d", 
          "smoothness", "squintability", 
          "n. jellyfish", 
          "max. tries", 
          "success rate", "time (sec)"),
        linesep = "",
        booktabs = T) |> 
  kable_styling(#latex_options = "hold_position",
    font_size = 10)
```

```{r}
#| label: tbl-mod-output
#| tbl-cap: Model estimates of proportion of jellyfish success on index properties and jellyfish hyper-parameters from the generalised linear model with a binomial family and a logit link function. The variable smoothness, squintability, number of jellyfish and maximum number of tries are positively associated with JSO success rate while data dimension and being flagged as long runtime are negatively associated with the success rate. The variable squintability and dimension are significant, suggesting their importance relative to jellyfish hyper-parameters in the optimisation success.
broom::tidy(mod1) |> 
  mutate(term = c("Intercept", "Smoothness",
   "Squintability", "Dimension (d)", 
   "Long time", "N. jellyfish", 
   "Max. tries")) |>
  kable(digits = 2, format = "latex", 
    linesep = "", booktabs = T) |> 
  column_spec(c(1,5), border_right = TRUE) |> 
  column_spec(1, border_left = TRUE) |> 
  kable_styling(font_size = 10) 
```

# Practicalities {#sec-discussion}

## Computing the metrics for your new index

## Using the JSO in a PPGT

```{r}
#| eval: false
#| echo: false
jso_optima <- save_history(flea[,1:6], 
                           guided_tour(holes()), 
                           search_f = search_jellyfish)
```
<!--
This section discusses the insights in implementation yielded from the previous sections to guide the usage of projection pursuit guided tour amongst practitioners. Smoothness and squintability in a projection pursuit problem can now be measured given the target shape, the chosen index function, and the data dimension. In practice, these metrics can be used to characterise the difficulty of the optimisation task and inform the choice of optimiser. For instance, a large squintability value indicates a distinct difference in index values between optimal regions and others. When one jellyfish enters the optimal region, the high index value it generates will be clearly distinguishable from spurious values due to this pronounced difference and lead other jellyfish to move toward the optimal region. In contrast, a smaller squintability value results in a less distinct separation between desirable and undesirable regions, making it more difficult for other jellyfish to accurately navigate toward the target area. In addition, a larger smoothness value implies a smoother index function, which suggests the use of both gradient-based and random sampling optimisers. 

This study enables comparison of new indices with existing indices with the proposed smoothness and squintability measures, as well as benchmarking with existing well-known data sets. Note that when comparing between multiple optimisers, the smoothness and squintability metrics proposed in this paper are relative. Comparison should be made with common parameters, i.e. the same seed for generating random bases and the same binning width for squintability. Furthermore, the range of the index values should be standardised to the [0, 1] interval as demonstrated for the `holes` and `TIC` indexes.

As a population-based metaheuristic method, JSO requires evaluating multiple jellyfish in each iteration, which increased its computational complexity compared to single-candidate random sampling methods. This additional computational costs should be taken into consideration when choosing optimisers. Tuning JSO hyperparameters, such as the number of jellyfish and the maximum number of attempts, can enhance the likelihood of finding the optimal region at the expense of computational costs. However, in some cases, increasing these parameters may not be effective if the index surface is complex or has low squintability.

The new JSO is integrated with the current implementation of the projection pursuit guided tour in the `tourr` package, and can be further examined using PP optimisation diagnostics in the `ferrn` package. The recommended approach is to conduct the optimisation off-line, extract the bases of a selected jellyfish, and then use the planned tour to follow selected jellyfish. These tools are also available in the `tourr` package.
-->

# Conclusion {#sec-conclusion}

This paper has presented new metrics to mathematically define desirable
features of PP indexes, squintability and smoothness, and used these to
assess the performance of the new jellyfish search optimiser. The
metrics will be generally useful for characterising PP indexes, and help
with developing new indexes.

In the comparison of the JSO against the currently used CRS, as expected
the JSO vastly out-performs CRS, and provides a high probability of
finding the global optima. The JSO obtains the maximum more cleanly,
with a slightly higher index value, and plot of the projected data
showing the structure more clearly.

The JSO performance is affected by the hyper-parameters, with a higher
chance of reaching the global optima when more jellyfish are used and
the maximum number of tries is increased. However, it comes at a
computational cost, as expected. The performance declines if the
projection dimension increases and if the PP index has low
squintability. The higher the squintability the better chance the JSO
can find the optima. However, interestingly smoothness does not affect
the JSO performance.



```{=html}
<!--
- Summarise the result of the simulation: JSO immense improvement on current methods, ...
- JSO for a guided tour, what changes are needed? Do we follow one tentacle, do we perform off-line and then pull tentacles out to watch, how does it integrate with ferrn


Main findings: 

  - JSO tends to obtian a clearer projection and higher index value when searching higher dimension spaces than CRS (and other current optimisers) - Figure 4
  
  - Increasing JS hyper-parameter enhances its performance, but the computational time required for the optimisation can quickly become intensive, especially when evaluating the index function (such as scagnostic indexes) multiple times across numerous iterations. - Figure 5
  
  - Smoothness and squintability can be calculated and compared across different problems to understand their relative complexity. Once a projection pursuit problem is fully characterised (by the shape-to-find, the index function used, and the data dimension), increasing JSO hyper-parameters can enhance the search effectiveness, however, the resulting increase in computational complexity will also need to be considered. - modelling
-->
```

# Acknowledgement

The article is created using Quarto [@Allaire_Quarto_2022] in R [@R].
The source code for reproducing the work reported in this paper can be
found at: <https://github.com/huizezhang-sherry/paper-jso>. Nicolas
Langrené acknowledges the partial support of the Guangdong Provincial
Key Laboratory IRADS (2022B1212010006, R0400001-22) and the UIC Start-up
Research Fund UICR0700041-22.

# References
