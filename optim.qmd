---
title: Studying the Performance of the Jellyfish Optimiser for the Application of Projection Pursuit
header-includes:
   - \usepackage{algorithm}
   - \usepackage{algpseudocode}
author:
  - name: Alice Anonymous
    email: alice@example.com
    affiliations: 
        - id: some-tech
          name: Some Institute of Technology
          department: Department Name
          address: Street Address
          city: City
          state: State
          postal-code: Postal Code
    attributes:
        corresponding: true
  - name: Bob Security
    email: bob@example.com
    affiliations:
        - id: another-u
          name: Another University
          department: Department Name
          address: Street Address
          city: City
          state: State
          postal-code: Postal Code
  - name: Cat Memes
    email: cat@example.com
    affiliations:
        - ref: another-u
  - name: Derek Zoolander
    email: derek@example.com
    affilations:
        - ref: some-tech
abstract: |
  This is the abstract. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum augue turpis, dictum non malesuada a, volutpat eget velit. Nam placerat turpis purus, eu tristique ex tincidunt et. Mauris sed augue eget turpis ultrices tincidunt. Sed et mi in leo porta egestas. Aliquam non laoreet velit. Nunc quis ex vitae eros aliquet auctor nec ac libero. Duis laoreet sapien eu mi luctus, in bibendum leo molestie. Sed hendrerit diam diam, ac dapibus nisl volutpat vitae. Aliquam bibendum varius libero, eu efficitur justo rutrum at. Sed at tempus elit.
keywords: 
  - projection pursuit
  - optimization
  - jellyfish optimiser
  - data visualisation
  - high-dimensional data
date: last-modified
bibliography: bibliography.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal of Multivariate Analysis
      formatting: preprint
      model: 3p
      cite-style: numbername
editor: 
  markdown: 
    wrap: 72
---

*Let's use British English ("American or British usage is accepted, but
not a mixture of these")*

```{r setup, echo = FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(ggh4x)
library(broom)
```

# Introduction \[Nicolas and Jessica\]

The artificial jellyfish search (JS) algorithm [@chou_novel_2021] is a
swarm-based metaheuristic optimisation algorithm inspired by the search
behaviour of jellyfish in the ocean. It is one of the newest swarm
intelligence algorithms [@rajwar_exhaustive_2023], which was shown to
have stronger search ability and faster convergence with few algorithmic
parameters compared to classic optimization methods
[@chou_novel_2021]-[@chou_recent_2022].

Effective optimisation is an important aspect of many methods employed
for visualising high-dimensional data ($X$). Here we are concerned about
computing informative linear projections of high-dimensional ($p$) data
using projection pursuit (PP) (@kr69, @FT74). This involves optimising a
function (e.g. @hall1989polynomial, @cook1993projection,
@lee2010projection, @Loperfido2018, @Loperfido2020), called the
projection pursuit index (PPI), that defines what is interesting or
informative in a projection.

These PPI are defined on projections ($XA$), which means that there is a
constraint that needs to be considered when optimising. A projection of
data is defined by a $p\times d$ orthonormal matrix $A$, and this
imposes the constraint on the elements of $A$, that columns need have
norm equal to 1 and the product of columns need to sum to zero.

@cook1995grand introduced the PP guided tour, which enabled interactive
visualisation of the optimisation in order to visually explore
high-dimensional data. It is implemented in the R [@R] package `tourr`
[@tourr]. The optimisation that is implemented is fairly basic, and
potential problems were highlighted by @RJ-2021-105. Implementing better
optimisation functionality is a goal, but it needs to be kept in mind
that the guided tour also has places importance on watching the
projected data as the optimisation progresses.

Here we explore the potential for a jellyfish optimisation to be
integrated with the guided tour. @sec-background explains the
optimisation that is used in the current the projection pursuit guided
tour. @sec-theory provides more details on the jellyfish optimiser and
formalises several characteristics of projection pursuit indexes that
are help to measure optimisaer performance. @sec-simulation describes a
simulation study on performance of the jellyfish for several types of
data and index functions. @sec-conclusion summarises the work and
provides suggestions for future directions.

# Projection pursuit, index functions and optimisation \[Di and Sherry\] {#sec-background}

<!-- Need to assume that the special issue will have the general introduction to PP, so the focus here will be on background needed for this paper, tours, guidance using PP, optimisation, visualisation, ... -->

A tour on high-dimensional data is constructed by geodesically
interpolating between pairs of planes. Any plane is described by an
orthonormal basis, $A_t$, where $t$ represents time in the sequence. The
term "geodesic" refers to maintaining the orthonormality constraint so
that each view shown is correctly a projection of the data. The PP
guided tour operates by geodesically interpolating to target planes
(projections) which have high PP index values, as provided by the
optimiser. The geodesic interpolation means that the viewer sees a
continuous sequence of projections of the data, so they can watch
patterns of interest forming as the function is optimised. There are
five optimisation methods implemented in the `tourr` package:

-   `search_geodesic()`: provides a pseudo-derivative optimisation. It
    searches locally for the best direction, based on differencing the
    index values for very close projections. Then it follows the
    direction along the geodesic path between planes, stopping when the
    next index value fails to increase.
-   `search_better()`: is a brute-force optimisation searching randomly
    for projections with higher index values.
-   `search_better_random()`: is essentially simulated annealing
    [@Bertsimas93] where the search space is reduced as the optimisation
    progresses.
-   `search_posse()`: implements the algorithm described in @posse95.
-   `search_polish()`: is a very localised search, to take tiny steps to
    get closer to the local maximum.

There are several PP index functions available: `holes()` and `cmass()`
[@cook1993projection]; `lda_pp()` [@lee2005projection]; `pda_pp()`
[@lee2010projection]; `dcor2d()` and `splines2d()` [@Grimm2016];
`norm_bin()` and `norm_kol()` [@huber85]; `slice_index()`
[@Laa:2020wkm]. Most are relatively simply defined, for any projection
dimension, and implemented because they are relatively easy to optimise.
A goal is to be able to incorporate more complex PP indexes, for example
based on scagnostics (@scag, @WW08).

An initial investigation of PP indexes, and the potential for
scagnostics is described in @laa_using_2020. To be useful here an
optimiser needs to be able to handle functions which are not very
smooth. In addition, because data structures might be relatively fine,
the optimiser needs to be able to find maxima that occur with a small
squint angle, that can only be seen from very close by. One last aspect
that is useful is for an optimiser to return local maxima in addition to
global because data can contain many different and interesting features.

# The jellyfish optimiser and properties of PP indexes \[Nicolas and Jessica\] {#sec-theory}

The jellyfish optimiser (JSO) mimics the natural movements of jellyfish,
which include passive and active motions driven by ocean currents and
their swimming patterns, respectively. In the context of optimization,
these movements are abstracted to explore the search space in a way that
balances exploration (searching new areas) and exploitation (focusing on
promising areas). The algorithm aims to find the optimal solution by
adapting the jellyfish's behaviour to navigate towards the best solution
over iterations [@chou_novel_2021].

To understand what the jellyfish optimizer is doing in the context of
Projection Pursuit, we first start with a current projection (the
starting point). Then, we evaluate this projection using an index
function, which tells us how good the current projection is. We then
move the projection in a direction determined by the 'best jelly' and
random factors, influenced by how far along we are in the optimization
process (the trial $i$ and `max.tries`). Occasionally, we might explore
completely new directions like a jellyfish might with ocean currents.
Then, we compare new potential projections to our current one. If
they're better, we adopt them; if not, we stick with our current
projection. This process continues and iteratively improves the
projection, until we reach the maximum number of trials.

::: {.callout-note icon="false"}
## Algorithm: Jellyfish Optimizer Pseudo Code

**Input**: `current_projections`, `index_function`, `tries`, `max_tries`

**Output**: `optimized_projection`

**Initialize** `best_jelly` as the projection with the best index value
from `current_projections`, and `current_index` as the array of index
values for each projection in `current_projections`

**for** each try in 1 to max_tries **do**

> Calculate $c_t$ based on the current try and max_tries

> **if** $c_t$ is greater than or equal to $0.5$ **then**
>
> > Define trend based on the best jelly and current projections
>
> > Update each projection towards the trend using a random factor and
> > orthonormalisation
>
> **else**
>
> > **if** a random number is greater than $1 - c_t$ **then**
> >
> > > Slightly adjust each projection with a small random factor (Type A
> > > passive)
> >
> > **else**
> >
> > > For each projection, compare with a random jelly and adjust
> > > towards or away from it (Type B active)
>
> Update the orientation of each projection to maintain consistency
>
> Evaluate the new projections using the index function

> **if** any new projection is worse than the current, revert to the
> `current_projection` for that case
>
> > Determine the projection with the best index value as the new
> > best_jelly

> **if** the try is the last one, print the final best projection and
> **exit**

**return** the set of projections with the updated best_jelly as the
optimized_projection
:::

<!-- Var names in the function and in the text needs to be consistent??? -->

The JSO implementation involves several key parameters that control its
search process in optimization problems. These parameters are designed
to guide the exploration and exploitation phases of the algorithm. While
the specific implementation details can vary depending on the version of
the algorithm or its application, we focus on two main parameters that
are most relevant to our application: the number of jellyfish and drift.

@laa_using_2020 has proposed five criteria for assessing projection
pursuit indexes (smoothness, squintability, flexibility, rotation
invariance, and speed). Since not all the properties affects the
execution of the optimisation, here we consider the three relevant
properties (smoothness, squintability, and speed), and propose three
metrics to evaluate these three properties.

## Smoothness {#sec-smoothness}

```{=html}
<!--
An intuitive way to measure smoothness of a function would be to count
how many continuous derivatives exist. 
To help define smoothness in our
context, we can make use of the Sobolev spaces: functions $f$ in the
Sobolev space $W^{p,\infty}$ have all derivatives of order less than $p$
continuous. Smoothness would then be the highest $p$ such that the index
function belongs to $W^{p,\infty}$. We can relax this and consider
$W^{p,q}$ Sobolev spaces.

Consider the following definition of partial derivative. Let $U$ be an
open subset of $\mathbf{R}^n$ and $f: U \rightarrow \mathbf{R}$. The
partial derivative of function $f$ at the point
$\mathbf{a} = (a_1, \ldots, a_n) \in U$ with respective to the $i$-th
variable $x_i$ is defined as

$$
\frac{\delta}{\delta x_i} f(\mathbf{a}) =  \lim_{h \rightarrow 0} \frac{f(a_1, \ldots, a_{i-1}, a_i +h, a_{i+1}, \ldots, a_n) - f(a_1 , \ldots, a_i, \ldots, a_n)}{h} = \lim_{h\rightarrow 0} \frac{f(\mathbf{a} + h\mathbf{e_i}) - f(\mathbf{a})}{h}.
$$ where $\mathbf{e_i}$ is the unit vector of the $i$-th variable $x_i$.

If the derivative is not well-defined, we propose to approximate the
derivative using the following expression:

$$\frac{1}{n_i}\sum_{i} \frac{\|f(\mathbf{a} + h_i\mathbf{e_i}) - f(\mathbf{a})\|^p}{h_i}$$

where $p \in [0,1]$. The choice of $p$ reflects the penalty behaviour.
For the application in this paper, we choose $p = 1$.

-   $h_i$ is an neighbourhood area around $a$. We can insert different
    values of $h$ to approximate the above quantity and observe how this
    quantity changes.
-->
```
If we evaluate the index function at some random points (like the random
initialization of the jellyfish optimizer), then we can interpret these
random index values as a random field, indexed by a space parameter: the
random projection angle. This analogy suggests to use this random
training sample to fit a spatial model, a simple one being a (spatial)
Gaussian process.

```{=html}
<!--
There exist R packages to fit the hyperparameters of a Gaussian process, for example ExaGeoStatR (https://github.com/ecrc/exageostatR), which is an R wrapper for the ExaGeoStat library (https://arxiv.org/pdf/1708.02835.pdf).
-->
```
How can we define a measure of smoothness from this? The distribution of
a Gaussian process is fully determined by its mean function and
covariance function. The way the covariance function is defined is where
smoothness comes into play: if an index is very smooth, then two close
projection angles should produce close index values (strong
correlation); by contrast, if an index is not smooth, then two close
projection angles might give very different index values (fast decay of
correlations with respect to distance between angles).

Popular covariance functions are parametric positive semi-definite
functions, some of which have a parameter to capture the smoothness of
the Gaussian field. In particular, consider the Matérn class of
covariance functions, defined by

$$
K(u):=\frac{(\sqrt{2\nu}u)^{\nu}}{\Gamma(\nu)2^{\nu-1}}\mathcal{K}_{\nu}(\sqrt{2\nu}u)
$$

where $\nu>0$ is the smoothness parameter and where $\mathcal{K}_\nu$ is
the modified Bessel function. The Matérn covariance function can be
expressed analytically when $\nu$ is a half-integer, the most popular
values in the literature being $1/2$, $3/2$ and $5/2$ . The parameter
$\nu$, called smoothness parameter, controls the decay of the covariance
function. As such, it is an appropriate measure of smoothness of a
random field.

In our context, we suggest to use this parameter as a measure of the
smoothness of the index function by fitting a Gaussian process prior
with Matérn covariance on a dataset generated by random evaluations of
the index function, as in the initial stage of the jellyfish random
search. There exist several R packages, such as GpGp or ExaGeoStatR, to
fit the hyperparameters of a GP covariance function on data. In this
project, we make use of the GpGp package.

The fitted value $\nu>0$ can be interpreted as follows: the higher
$\nu$, the smoother the index function.

## Squintability {#sec-squintability}

From the literature, it is commonly understood that a large squint angle
implies that the objective function value is close to optimal even when
we are not very close to the perfect view to see the structure. A small
squint angle means that index function value improves substantially only
when we are very close to the perfect view. As such, low squintability
implies rapid improvement in the index value when near the perfect view.

In this study, we propose two metrics to capture the notion of
squintability.

\[We generate random points that is beyond 1.5 projection distance and
interpolate. Then we fit a kernel or use nonlinear least squares.\]

First, parametric model.

\[Nicolas's pdf\]

Second, we consider the product of the largest absolute magnitude of
rate of change of $f$ and the corresponding projection angle as a second
measure of squintability. Since $f$ is decreasing, the rate of change of
$f$ is negative and thus $|\underset{x}{\min} f('x)|$ gives the absolute
magnitude of the most negative rate of change.

\[Nicolas's pdf\]

To the best of our knowledge, this is the first attempt to measure the
notion of squintability.

## Speed

The speed of optimizing an index function can be calculated/measured
using the computational complexity (in big O notation, with respect to
the sample size) of computing the index function.

# Visualisation of jellyfish optimiser

Information of the jellyfish optimiser is available in a tabular format
and below is an example data collected from finding the sine-wave
structure in 6D data using a distance correlation index (`docr2d_2`):

```{r}
load(here::here("data/sim_example.rda"))
```

```{r}
dplyr::glimpse(sim_example |> select(-id)) 
```

Information recorded can be categorised into the following categories:

-   projection pursuit variables: the index function used (`idx_f`), the
    data dimension (`d`)
-   jellyfish optimiser parameters: the number of jellies (`n_jellies`),
    the maximum number of tries (`max_tries`)
-   simulation variables: the simulation number (`sim`), the seed used
    (`seed`)
-   optimisation variables: the projection basis in a matrix format
    (`basis`), the index value (`index_val`), a description of the
    status - one of "initiation", "current_best", and "jellyfish_update"
    (`info`), current iteration ID (`tries`), current jelly ID (`loop`),
    and the time taken to find the optimum (`time`).

The basis column records every basis *visited* by the jellyfish
optimiser prior to comparing with the current basis. In each iteration,
if the index value of a visited basis is smaller than that of the
current one, the jellyfish optimiser will retain the current basis for
the next iteration, while still documenting the visited basis.

Numerical information to compute:

-   angular distance between the projection basis and the theoretical
    best basis,
-   the proportion of simulation that found the optimal basis,
-   the proportion of jellies, within each simulation, that found the
    optimal basis,

Visualisation to inspect:

-   inspect the basis visited by each jellyfish in the reduced PCA
    space,
-   inspect the final 2D projections reached by each jellyfish,
-   plot the index value against the angular distance between the
    projection basis and the theoretical best basis

Plotting the basis in the space and the projected data can help to
understand 1) whether each simulation finds the same optimum or some
simulations find local optima; and 2) whether the index function used
can detect the structure in the data and the projection contains the
structure of interest.

The visualisations above can be faceted by the projection pursuit
variables and jellyfish optimiser parameters to compare the performance
of different indexes to detect the same structure and how the jellyfish
optimiser parameters affect the optimisation process.

\[example plots\]

```{r}
load(here::here("data/sine_dcor2d_best.rda"))
set.seed(123456)
sin1000 <- spinebil::sinData(6, 1000) %>% scale() %>% as_tibble()
colnames(sin1000) <- paste0("V", 1:6)


proj_dt <- map_dfr(sine_dcor2d_best |> pull(basis), 
              ~as_tibble(as.matrix(sin1000) %*% .x), .id = "sim") 

p1 <- proj_dt |>
  ggplot(aes(x = V1, y = V2)) + 
  geom_point(size = 0.1) + 
  xlim(-4, 4) + ylim(-4, 4) +
  facet_wrap(~as.numeric(sim), ncol = 10) + 
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())
```

```{r}
#| fig.height: 5 
#| fig.width: 10
#| fig.cap: sdfjsflk
p1 
```

```{r eval = FALSE}
theo_best <- matrix(c(rep(0, 8), 1, 0, 0, 1), nrow = 6, byrow = TRUE)
angle_dcor2d <- sine_dcor2d_5050 |>
    rowwise() |>
    mutate(angle = tourr::proj_dist(theo_best, basis))

p2 <- angle_dcor2d |>
  ggplot(aes(x = angle, y = index_val)) +
  geom_point(size = 0.1) +
  theme(aspect.ratio = 1) 

```

# Application \[Di and Sherry\] {#sec-simulation}

The jellyfish optimiser has been implemented in the tourr package
[@wickham_tourr_2011] and we will use the diagnostic plots proposed in
the ferrn package [@RJ-2021-105] to visualise the optimisation process.

## Going beyond 10D

The pipe-finding problem is initially used to investigate indexes and
optimisers in @laa_using_2020, and we extend it from a 6D problem to a
12D problem.

Jellyfish optimiser, as a multi-start algorithm, is efficient in \[...\]
for high-dimensional problems

```{r echo = FALSE}
load(here::here("data/pipe_better.rda"))
load(here::here("data/pipe_jellyfish.rda"))
```

```{r}
#| fig.width = 6,
#| fig.height = 3,
#| fig.align = "center",
#| fig.cap = "sthis sdfaksdlf"
pipe_better |>
  bind_rows(pipe_jellyfish) |>
  mutate(dim = as.factor(6 + (as.numeric(dim) - 1) * 2)) |>
  ggplot(aes(x = dim, y = index_val, group = interaction(optimiser, dim), color = optimiser)) +
  geom_violin(width = 2, adjust = 0.5, position = position_dodge(width = 0.4)) +
  ggbeeswarm::geom_quasirandom(size = 0.1, width = 0.1, dodge.width = 0.4) +
  theme_bw() +
  scale_y_continuous(breaks = seq(0.85, 1, 0.01)) +
  scale_color_brewer(palette = "Accent") + 
  labs(y = "Index", x = "Dimension")

```

```{r}
#| fig.width = 8,
#| fig.height = 5,
#| fig.align = "center",
#| fig.cap = "sthis sdfaksdlf"
get_proj <- function(data){
  quantiles <- map(1:4, ~data |>
                     filter(dim == .x) |>
                     pull(index_val) |>
                     quantile(seq(0, 1, 0.5), type = 3) |>
                     rev() |>
                     unname())

  res <- map2_dfr(
    unlist(quantiles),
    rep(1:4, each = 3),
    ~ data |>
      filter(dim == .y, index_val == .x) |>
      select(dim, proj, index_val),
    .id = "id"
    ) |>
    unnest(proj) |>
    mutate(index_val = sprintf("%.3f", index_val),
           optimiser = "jellyfish",
           id = (as.numeric(id) - 1) %%3 + 1,
           dim = as.factor(6 + (as.numeric(dim) - 1) * 2)) |>
    mutate(id = case_when(id == 1 ~ "best",
                          id == 2 ~ "mean",
                          id == 3 ~ "worst",
                          TRUE ~ "0thers"))
  return(res)
}


p1 <- get_proj(pipe_jellyfish)  |>
  ggplot(aes(x = V1, y = V2)) +
  geom_point(size = 0.1) +
  geom_text(data = ~. |> distinct(id, dim, index_val),
           aes(label = index_val), x =0, y = 3.7, size = 3) +
  facet_grid(dim ~ id) +
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())
p2 <- p1 %+% get_proj(pipe_better)
p1 + ggtitle("The Jellyfish Optimiser") | p2 + ggtitle("The Better Optimiser")


```


\newpage 


## Another data example

[TODO: overall summary]construct a relationship between jellyfish success and jellyfish parameters and the optimisation properties defined in @sec-theory. This can inform the choice of jellyfish parameters for a given
optimisation problem.

In addition to the pipe-finding problem, the sine-wave finding problem is also investigated in 6D and 8D spaces. Six indices are considered: `dcor2d_2`, `loess2d`, `MIC`, `TIC`, `spline`, and `stringy`, which gives the 12 problems printed as rows in @tbl-smoothness-squintability. Combining these problems with different jellyfish optimiser parameters (`n_jellies` and `max_tries`), a total of 51 observations is produced, comprising of 24 observations for the pipe-finding and 27 observations for the sine-wave finding.

For each observation, fifty (50) runs of the jellyfish optimisation is performed. In each run, summary statistics are calculated to find the best index across all the jellyfishes (`I_max`) and the proportion of jellyfishes found (`P_J`). At the observation level, the best index value across all the 50 runs  (`I_max_max`) and the proportion of trials found (`P_J_hat`) is calculated.

Smoothness and squintability measures are computed for each problem, following the procedures outlined in @sec-smoothness and @sec-squintability. To calculate smoothness, 300 random bases are used to estimate Gaussian process parameters (variance, range, smoothness, and nugget). For squintability, 50 random bases are sampled and interpolated to the optimal basis using a step size of 0.005. The index value and projection distance are calculated for all the interpolated bases. A binning procedure is then applied to average the index value at each bin width of 0.005. Finally, the scaled logistic function with four parameters (theta1 - theta4) is estimated by non-linear least square to obtain the squintability measure.

[TODO: discuss what the values of smooth and theta3 mean for the optimisation]

@tbl-smoothness-squintability presents all the parameters calculated for smoothness and squintability, with the column "smooth" for smoothness and "theta3" for squintability. @fig-idx-proj-dist shows the relationship between the index value against the projection distance for each problem following the binning procedure. 

<!-- @tbl-all-sim displays the first five rows after combining all calculations.  -->

```{r}
#| label: tbl-smoothness-squintability
#| tbl-cap: Parameters estimated from the Gaussian process (including variance, range, smoothness, and nugget) and scaled logistic function (theta1 - theta4) for the pipe-finding and sine-wave finding problems. The squint column is calculated as theta1 * theta2 * theta3 / 4, as described in @sec-squintability. The "smooth" and "squint" column represent the smoothness and squintability measures. 
load(here::here("data/smoothness.rda"))
load(here::here("data/squintability.rda"))
library(kableExtra)
tibble(shape = c(rep("pipe", 4), rep("sine", 8)), smoothness) |>
  left_join(squintability) |> 
  rename(d = n, smooth = smoothness) |> 
  knitr::kable(digits = 3) |> 
  column_spec(1, border_left = TRUE) |> 
  column_spec(c(3, 7, 12), border_right = TRUE) |> 
  kable_styling(latex_options = "hold_position")
```

```{r fig-idx-proj-dist}
#| fig.width = 9,
#| fig.height = 5,
#| fig.cap = "Index values against projection distance for the 12 pipe/sine-wave finding problem after the binning procedure during the estimation of the squintability measure. The index value is averaged at each bin width of 0.005 and the TIC index is scaled to 0-1 for comparison. When finding the sine-wave structure using the MIC and TIC index, a convex curved is observed, as opposed to the pipe-finding problem or the sine-wave finding problem with the dcor2d\\_2, loess2d, and splines2d index. When finding the sine-wave with the stringy index, the index shows an instantaneous jump to the optimum when close to the best basis."
load(here::here("data/sq_basis_dist_idx.rda"))
sq_basis_dist_idx |>
  ggplot(aes(x = dist, y = idx, group = index)) +
  geom_line() +
  facet_nested_wrap(~index + n, nrow = 3, labeller = label_both, scales = "free_y") + 
  xlab("projection distance") + ylab("index value")

```

```{r}
#| label: tbl-all-sim
#| tbl-cap: The first few rows of the datasets processed for modelling. The smoothness and squintability variable are uniquely characterised by the index function used and the data dimension, and thus do not vary across n\\_jellies and max\\_tries. The variable P\\_J\\_hat, and time are calculated at each observation.
load(here::here("data/sim_df.rda"))
sim_df |> 
  dplyr::select(index, d, smoothness, squintability, n_jellies, max_tries, 
                -I_max_max, P_J_hat, time) |> 
  arrange(index, d, n_jellies, max_tries) |> 
  head(7) |> 
  kable(digits = 3)
```



```{r}
sim_df2 <- sim_df |> 
  mutate(n_jellies = n_jellies/10,
         max_tries = max_tries/10,
         long_time = ifelse(time > 30, 1, 0),
         P_J_hat = ifelse(index == "stringy", 0, P_J_hat))

mod1 <- glm(P_J_hat ~ smoothness + squintability + d + long_time + n_jellies + max_tries, data = sim_df2, family = binomial)
broom::tidy(mod1)
```


# Conclusion \[Di and Sherry\] {#sec-conclusion}

<!-- # Bibliography styles -->

<!-- Here are two sample references:  @Feynman1963118 @Dirac1953888. -->

<!-- By default, natbib will be used with the `authoryear` style, set in `classoption` variable in YAML.  -->

<!-- You can sets extra options with `natbiboptions` variable in YAML header. Example  -->

<!-- ``` -->

<!-- natbiboptions: longnamesfirst,angle,semicolon -->

<!-- ``` -->

<!-- There are various more specific bibliography styles available at -->

<!-- <https://support.stmdocs.in/wiki/index.php?title=Model-wise_bibliographic_style_files>.  -->

<!-- To use one of these, add it in the header using, for example, `biblio-style: model1-num-names`. -->

<!-- ## Using CSL  -->

<!-- If `cite-method` is set to `citeproc` in `elsevier_article()`, then pandoc is used for citations instead of `natbib`. In this case, the `csl` option is used to format the references. By default, this template will provide an appropriate style, but alternative `csl` files are available from <https://www.zotero.org/styles?q=elsevier>. These can be downloaded -->

<!-- and stored locally, or the url can be used as in the example header. -->

<!-- # Equations -->

<!-- Here is an equation: -->

<!-- $$  -->

<!--   f_{X}(x) = \left(\frac{\alpha}{\beta}\right) -->

<!--   \left(\frac{x}{\beta}\right)^{\alpha-1} -->

<!--   e^{-\left(\frac{x}{\beta}\right)^{\alpha}};  -->

<!--   \alpha,\beta,x > 0 . -->

<!-- $$ -->

<!-- Inline equations work as well: $\sum_{i = 2}^\infty\{\alpha_i^\beta\}$ -->

<!-- # Figures and tables -->

<!-- @fig-meaningless is generated using an R chunk. -->

<!-- ```{r} -->

<!-- #| label: fig-meaningless -->

<!-- #| fig-cap: A meaningless scatterplot -->

<!-- #| fig-width: 5 -->

<!-- #| fig-height: 5 -->

<!-- #| fig-align: center -->

<!-- #| out-width: 50% -->

<!-- #| echo: false -->

<!-- plot(runif(25), runif(25)) -->

<!-- ``` -->

<!-- # Tables coming from R -->

<!-- Tables can also be generated using R chunks, as shown in @tbl-simple example. -->

<!-- ```{r} -->

<!-- #| label: tbl-simple -->

<!-- #| tbl-cap: Caption centered above table -->

<!-- #| echo: true -->

<!-- knitr::kable(head(mtcars)[,1:4]) -->

<!-- ``` -->

# References
