---
title: Studying the Performance of the Jellyfish Search Optimiser for the Application of Projection Pursuit
header-includes:
   - \usepackage{algorithm}
author:
  - name: H. Sherry Zhang
    email: huize.zhang@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Austin
          country: United States
          postal-code: 78751
    attributes:
        corresponding: true
  - name: Dianne Cook
    email: dicook@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
  - name: Nicolas  Langren√©
    email: nicolaslangrene@uic.edu.cn
    affiliations:
        - id: 3
          name: Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International College
          department: Department of Mathematical Sciences
          city: Zhuhai
          country: China
          postal-code: 519087
  - name: Jessica Wai Yin Leung
    email: Jessica.Leung@monash.edu
    affiliations:
        - id: 2
          name: Monash University
          department: Department of Econometrics and Business Statistics
          city: Melbourne
          country: Australia
          postal-code: 3800
abstract: |
  Projection pursuit (PP) is a dimension reduction technique that identifies low-dimensional projections in high-dimensional data by optimising a criteria function known as the PP index. The optimisation in PP can be non-smooth and requires identifying optima with a small "squint angle", detectable only from close proximity. To address these challenges, this study investigates the performance of a recent swarm-based algorithm, Jellyfish Search Optimiser (JSO), for optimising PP indexes. The performance of JSO is evaluated across various hyper-parameter settings and compared with existing optimisers. Additionally, this work proposes novel methods to quantify two properties of the PP index -- smoothness and squintability -- that capture the complexities inherent in PP optimisation problems. These two metrics are evaluated with JSO hyper-parameters to determine their effect on JSO success rate. The JSO algorithm has been implemented in the `tourr` package, while calculations for smoothness and squintability are available in the `ferrn` package. 
keywords: 
  - projection pursuit
  - Jellyfish Search Optimiser (JSO)
  - Optimisation
date: last-modified
bibliography: bibliography.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal of Multivariate Analysis
      formatting: preprint
      model: 3p
      cite-style: numbername
editor: 
  markdown: 
    wrap: 72
crossref: 
  eq-prefix: ""
---

**README**:

-   British English ("American or British usage is accepted, but not a
    mixture of these")\*

-   No "we" - always third person

-   Check if the affiliation information is correct

-   word use:

    -   Use jellyfish search optimiser, or JSO
    -   "hyper-parameter" rather than "hyperparameter"
    -   PP index rather than PPI

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
#install the latest version of ferrn from: https://github.com/huizezhang-sherry/ferrn
library(tidyverse)
library(ggplot2)
library(patchwork)
library(ggh4x)
library(broom)
library(kableExtra)
library(ferrn)
load(here::here("data/sim_pipe_run_best.rda"))
load(here::here("data/sim_sine_6d_spline_head.rda"))
load(here::here("data/sim_sine_6d_spline_best.rda"))
load(here::here("data/sim_sine_6d_spline_projdist.rda"))
load(here::here("data/pipe_better.rda"))
load(here::here("data/pipe_jellyfish.rda"))
load(here::here("data/smoothness.rda"))
load(here::here("data/squintability.rda"))
load(here::here("data/sq_basis_dist_idx.rda"))
load(here::here("data/sim_df.rda"))
```

# Introduction \[Nicolas and Jessica\]

Projection Pursuit (PP) is a high-dimensional data visualisation
approach that involves computing informative linear projections of data
by optimising a variety of objective functions, namely *PP index*
(@hall1989polynomial, @cook1993projection, @lee2010projection,
@Loperfido2018, @Loperfido2020). Given high-dimensional data
$X \in \mathbf{R}^{n\times p}$ and the index function $f(\cdot)$, PP
finds the orthonormal projection matrix $A \in \mathbf{R}^{p \times d}$
by solving the following optimisation problem:

$$
\underset{A}{\max } \quad f(XA) \quad \text{subject to} \quad A'A = I
$$ {#eq-optimization}

These indices $f(\cdot)$ are characterised by the informativeness or
"interestingness" of a projection and are often non-linear and
non-convex. As such, an effective and efficient optimisation procedure
is essential to enable sufficient exploration of the data landscape
during the visualisation process and to arrive at a global optimal
viewpoint of the data.

Significant efforts have been devoted to enhancing the functionality of
PP and facilitating a better experience in the visualisation process.
@cook1995grand introduced the PP guided tour, which enabled interactive
visualisation of the optimisation to visually explore high-dimensional
data. It is implemented in the R [@R] package `tourr`[@tourr].
@RJ-2021-105 highlighted potential problems of the optimisation routine
implemented. While improving the quality of the optimisation solutions
in the tour is essential, it is also important to be able to watch the
projected data as the optimisation progresses. As such, it is pertinent
to integrate the guided tour with a global optimisation algorithm that
is efficient in finding the global optimal and enables viewing of the
projected data during the exploration process.

The artificial Jellyfish Search Optimiser (JSO) [@chou_novel_2021] is a
swarm-based metaheuristic designed to solve global optimisation
problems. Inspired by the search behaviour of jellyfish in the ocean,
JSO is one of the latest swarm intelligence algorithms
[@rajwar_exhaustive_2023], which was shown to have stronger search
ability and faster convergence with fewer parameters compared to classic
optimisation methods [@chou_novel_2021]-[@chou_recent_2022]. These
practical properties makes JSO a strong candidate in integration with
the PP guided tour. As such, it is of interest to explore the potential
of JSO in enhancing the PP guided tour and examine the characteristics
and advantages of such implementation.

The primary goal of this study is to investigate the performance of JSO
in the context of the Projection pursuit. A series of simulation
experiments using well-recognised data sets and PP indexes are conducted
to yield insights regarding the behaviour of JSO under different
settings and the sensitivity of hyper-parameter in the optimisation.
Second, to observe the performance of JSO with different types of PP
indexes, a collection of metrics is introduced to capture specific
properties of the index including squintability and smoothness as
introduced in @laa_using_2020. To the best of the authors' knowledge,
this is the first attempt to quantitatively measure squintability and
smoothness in this context. Finally, the relationship between the JSO
performance, hyper-parameters tuning and various properties of PP
indexes is analysed to provide helpful guidance for practitioners that
are using the guided tour for high dimensional visualisation.

The rest of this paper is structured as follows. @sec-background
introduces the background of the PP guided tour and reviews existing
methods in the literature. @sec-theory describes the details of JSO and
introduces metrics that measure different properties of PP indexes.
@sec-vis visualises the behaviour of the JSO in terms of describes a
simulation study on the performance of the JSO using well-known data
sets and index functions. @sec-app displays two sets of simulation
experiments comparing the JSO with the search-better optimiser and
studies the impact of different PP index properties on the optimisation
performance. @sec-conclusion summarises the work and provides
suggestions for future directions.

# Projection pursuit, index functions and optimisation \[Di and Sherry\] {#sec-background}

<!-- Need to assume that the special issue will have the general introduction to PP, so the focus here will be on background needed for this paper, tours, guidance using PP, optimisation, visualisation, ... -->

A tour on high-dimensional data is constructed by geodesically
interpolating between pairs of planes. Any plane is described by an
orthonormal basis, $A_t$, where $t$ represents time in the sequence. The
term "geodesic" refers to maintaining the orthonormality constraint so
that each view shown is correctly a projection of the data. The PP
guided tour operates by geodesically interpolating to target planes
(projections) which have high PP index values, as provided by the
optimiser. The geodesic interpolation means that the viewer sees a
continuous sequence of projections of the data, so they can watch
patterns of interest forming as the function is optimised. There are
five optimisation methods implemented in the `tourr` package:

-   `search_geodesic()`: provides a pseudo-derivative optimisation. It
    searches locally for the best direction, based on differencing the
    index values for very close projections. Then it follows the
    direction along the geodesic path between planes, stopping when the
    next index value fails to increase.
-   `search_better()`: is a brute-force optimisation searching randomly
    for projections with higher index values.
-   `search_better_random()`: is essentially simulated annealing
    [@Bertsimas93] where the search space is reduced as the optimisation
    progresses.
-   `search_posse()`: implements the algorithm described in @posse95.
-   `search_polish()`: is a very localised search, to take tiny steps to
    get closer to the local maximum.

There are several PP index functions available: `holes()` and `cmass()`
[@cook1993projection]; `lda_pp()` [@lee2005projection]; `pda_pp()`
[@lee2010projection]; `dcor2d()` and `splines2d()` [@Grimm2016];
`norm_bin()` and `norm_kol()` [@huber85]; `slice_index()`
[@Laa:2020wkm]. Most are relatively simply defined, for any projection
dimension, and implemented because they are relatively easy to optimise.
A goal is to be able to incorporate more complex PP indexes, for example
based on scagnostics (@scag, @WW08).

An initial investigation of PP indexes, and the potential for
scagnostics is described in @laa_using_2020. To be useful here an
optimiser needs to be able to handle index functions which are possibly
not very smooth. In addition, because data structures might be
relatively fine, the optimiser needs to be able to find maxima that
occur with a small squint angle, that can only be seen from very close
by. One last aspect that is useful is for an optimiser to return local
maxima in addition to global because data can contain many different and
interesting features.

# The jellyfish optimiser and properties of PP indexes \[Nicolas and Jessica\] {#sec-theory}

JSO mimics the natural movements of jellyfish, which include passive and
active motions driven by ocean currents and their swimming patterns,
respectively. In the context of optimization, these movements are
abstracted to explore the search space in a way that balances
exploration (searching new areas) and exploitation (focusing on
promising areas). The algorithm aims to find the optimal solution by
adapting the behaviour of jellyfish to navigate towards the best
solution over iterations [@chou_novel_2021].

To solve the optimisation problem embedded in the PP guided tour, a
starting projection, an index function and the maximum number of
iterations are provided as input. Then, the current projection is
evaluated by the index function. The projection is then moved in a
direction determined by a random factor, influenced by how far along we
are in the optimization process. Occasionally, completely new directions
may be taken like a jellyfish might with ocean currents. A new
projection is accepted if it is an improvement compared to the current
one, rejected otherwise. This process continues and iteratively improves
the projection, until the pre-specified maximum number of trials is
reached.

::: {.callout-note icon="false"}
## Algorithm: Jellyfish Optimizer Pseudo Code

**Input**: `current_projections`, `index_function`, `trial_id`,
`max_trial`

**Output**: `optimized_projection`

**Initialize** `current_best` as the projection with the best index
value from `current_projections`, and `current_idx` as the array of
index function values for each projection in `current_projections`

**for** each trial_id in 1 to max_tries **do**

> Calculate $c_t$ based on the current_idx and max_trial

> **if** $c_t$ is greater than or equal to $0.5$ **then**
>
> > Define trend based on the current_best and current_projections
>
> > Update each projection towards the trend using a random factor and
> > orthonormalisation
>
> **else**
>
> > **if** a random number is greater than $1 - c_t$ **then**
> >
> > > Slightly adjust each projection with a small random factor
> > > (passive)
> >
> > **else**
> >
> > > For each projection, compare with a random jelly and adjust
> > > towards or away from it (active)
>
> Update the orientation of each projection to maintain consistency
>
> Evaluate the new projections using the index function

> **if** any new projection is worse than the current, revert to the
> `current_projections` for that case
>
> > Determine the projection with the best index value as the new
> > current_best

> **if** trial_id $\geq$ max_trial, print the last best projection
> **exit**

**return** the set of projections with the updated current_best as the
optimized_projection
:::

<!-- Var names in the function and in the text needs to be consistent??? -->

The JSO implementation involves several key parameters that control its
search process in optimization problems. These parameters are designed
to guide the exploration and exploitation phases of the algorithm. While
the specific implementation details can vary depending on the version of
the algorithm or its application, we focus on two main parameters that
are most relevant to our application: the number of jellyfish and the
maximum number of trials.

@laa_using_2020 has proposed five criteria for assessing projection
pursuit indexes (smoothness, squintability, flexibility, rotation
invariance, and speed). Since not all the properties affects the
execution of the optimisation, here we consider the three relevant
properties (smoothness, squintability, and speed), and propose three
metrics to evaluate these three properties.

## Smoothness {#sec-smoothness}

```{=html}
<!--
An intuitive way to measure smoothness of a function would be to count
how many continuous derivatives exist. 
To help define smoothness in our
context, we can make use of the Sobolev spaces: functions $f$ in the
Sobolev space $W^{p,\infty}$ have all derivatives of order less than $p$
continuous. Smoothness would then be the highest $p$ such that the index
function belongs to $W^{p,\infty}$. We can relax this and consider
$W^{p,q}$ Sobolev spaces.

Consider the following definition of partial derivative. Let $U$ be an
open subset of $\mathbf{R}^n$ and $f: U \rightarrow \mathbf{R}$. The
partial derivative of function $f$ at the point
$\mathbf{a} = (a_1, \ldots, a_n) \in U$ with respective to the $i$-th
variable $x_i$ is defined as

$$
\frac{\delta}{\delta x_i} f(\mathbf{a}) =  \lim_{h \rightarrow 0} \frac{f(a_1, \ldots, a_{i-1}, a_i +h, a_{i+1}, \ldots, a_n) - f(a_1 , \ldots, a_i, \ldots, a_n)}{h} = \lim_{h\rightarrow 0} \frac{f(\mathbf{a} + h\mathbf{e_i}) - f(\mathbf{a})}{h}.
$$ where $\mathbf{e_i}$ is the unit vector of the $i$-th variable $x_i$.

If the derivative is not well-defined, we propose to approximate the
derivative using the following expression:

$$\frac{1}{n_i}\sum_{i} \frac{\|f(\mathbf{a} + h_i\mathbf{e_i}) - f(\mathbf{a})\|^p}{h_i}$$

where $p \in [0,1]$. The choice of $p$ reflects the penalty behaviour.
For the application in this paper, we choose $p = 1$.

-   $h_i$ is an neighbourhood area around $a$. We can insert different
    values of $h$ to approximate the above quantity and observe how this
    quantity changes.
-->
```
If we evaluate the index function at some random points (like the random
initialization of the jellyfish optimizer), then we can interpret these
random index values as a random field, indexed by a space parameter: the
random projection angle. This analogy suggests to use this random
training sample to fit a spatial model, a simple one being a (spatial)
Gaussian process.

```{=html}
<!--
There exist R packages to fit the hyperparameters of a Gaussian process, for example ExaGeoStatR (https://github.com/ecrc/exageostatR), which is an R wrapper for the ExaGeoStat library (https://arxiv.org/pdf/1708.02835.pdf).
-->
```
How can we define a measure of smoothness from this? The distribution of
a Gaussian process is fully determined by its mean function and
covariance function. The way the covariance function is defined is where
smoothness comes into play: if an index is very smooth, then two close
projection angles should produce close index values (strong
correlation); by contrast, if an index is not smooth, then two close
projection angles might give very different index values (fast decay of
correlations with respect to distance between angles).

Popular covariance functions are parametric positive semi-definite
functions, some of which have a parameter to capture the smoothness of
the Gaussian field. In particular, consider the Mat√©rn class of
covariance functions, defined by

$$
K(u):=\frac{(\sqrt{2\nu}u)^{\nu}}{\Gamma(\nu)2^{\nu-1}}\mathcal{K}_{\nu}(\sqrt{2\nu}u)
$$

where $\nu>0$ is the smoothness parameter and where $\mathcal{K}_\nu$ is
the modified Bessel function. The Mat√©rn covariance function can be
expressed analytically when $\nu$ is a half-integer, the most popular
values in the literature being $1/2$, $3/2$ and $5/2$ . The parameter
$\nu$, called smoothness parameter, controls the decay of the covariance
function. As such, it is an appropriate measure of smoothness of a
random field.

In our context, we suggest to use this parameter as a measure of the
smoothness of the index function by fitting a Gaussian process prior
with Mat√©rn covariance on a dataset generated by random evaluations of
the index function, as in the initial stage of the jellyfish random
search. There exist several R packages, such as GpGp or ExaGeoStatR, to
fit the hyperparameters of a GP covariance function on data. In this
project, we make use of the GpGp package.

The fitted value $\nu>0$ can be interpreted as follows: the higher
$\nu$, the smoother the index function.

## Squintability {#sec-squintability}

From the literature, it is commonly understood that a large squint angle
implies that the objective function value is close to optimal even when
we are not very close to the perfect view to see the structure. A small
squint angle means that index function value improves substantially only
when we are very close to the perfect view. As such, low squintability
implies rapid improvement in the index value when near the perfect view.

In this subsection, a metric to capture the notion of squintability is
proposed, and two approaches to compute this metric numerically are
detailed.

```{=tex}
\textcolor{orange}{Here we need to add a precise, mathematical
definition of the $x$ and $y$ variables ("angle distance" to optimal?
and PP index values) used in the figures of this Section.}
```
\[We generate random points that is beyond 1.5 projection distance and
interpolate. Then we fit a kernel or use nonlinear least squares.\]

Let $f$ represent the average PP index value with respect to the
projection angle distance to optimal, over the course of the JSO. It is
expected that for a PP index with high squint angle, the optimization
(@eq-optimization) should make substantial progress early on.
Conversely, for a PP index with low squint angle, it might take a long
while for the optimization to make substantial progress, as the
candidate projections would need to be very close to the optimal one for
the structure of the index function to be visible enough to be amenable
to efficient optimization. This observation suggests that the extreme
values of $f'$ (the ones for which $f''=0$, assuming that $f$ is twice
differentiable), and the angles for which these values are attained,
should play an important role in the mathematical definition of
squintability. Noticing that $f$ is expected to be a decreasing
function, we propose the following squintability metric:

$$
\varsigma:=-4\min_{x}f'(x)\times\underset{x}{\arg\min}f'(x)
$$ {#eq-squintability}

On the one hand, $-\min_{x}f'(x)$ represents the largest value of the
gradient of $-f$. On the other hand, $\underset{x}{\arg\min}f'(x)$
represents the angle value at which this largest gradient is attained.
Based on the above discussion, it is expected that these two values
should be both high in the case of high squintability (fast increase in
$f$ early on), and both low in the case of low squintability (any
substantial increase in $f$ happens very late, close to the optimal
angle). This suggest that their product (@eq-squintability) should
provide a sensible measure of squintability. The multiplicative constant
$4$, which can be deemed arbitrary, does not change the interpretation
of the squintability metric $\varsigma$; it is here to adjust the range
of values of $\varsigma$ and simplify the explicit formula for
$\varsigma$ obtained later on.

To compute the squintability metric (@eq-squintability) in practice,
several approaches are possible. The first one is to propose a
parametric model for $f$, and use it to obtain an explicit formula for
$\varsigma$. Numerical experiments suggest a scaled sigmoid shape as
described below. Define

$$
\ell(x):=\frac{1}{1+\exp(\theta_{3}(x-\theta_{2}))}\ ,
$$ {#eq-logistic}

which is a decreasing logistic function depending on two parameters
$\theta_2$ and $\theta_3$ , such that $\ell(\theta_{2})=\frac{1}{2}$.
Then, define

$$
f(x)=(\theta_{1}-\theta_{4})\frac{\ell(x)-\ell(x_{\max})}{\ell(0)-\ell(x_{\max})}+\theta_{4}\ ,
$$ {#eq-parametric}

which depends on three additional parameters, $\theta_1$, $\theta_2$ ,
and $x_{\max}$, such that $f(0)=\theta_1$ and $f(x_{\max})=\theta_4$.
Under the parametric model (@eq-parametric), the squintability metric
(@eq-squintability) can be shown to be equal to

$$
\varsigma=\frac{(\theta_{1}-\theta_{4})\theta_{2}\theta_{3}}{\ell(0)-\ell(x_{\max})}
$$ {#eq-squintability-parametric}

In practice, the parameters of this model (@eq-parametric) can be
estimated numerically, for example by non-linear least squares, and then
used to evaluate $\varsigma$ as in equation
(@eq-squintability-parametric).

Alternatively, one can estimate (@eq-squintability) in a nonparametric
way, for example by fitting $f$ using kernel regression, then
numerically estimate the angle at which $-f'$ attains its highest value.

```{=tex}
\textcolor{orange}{See whether further details are needed for the nonparametric approach.}
```
TODO: add equation reference to main text line 705

To the best of our knowledge, this is the first attempt to measure the
notion of squintability.

## Speed

The speed of optimizing an index function can be measured empirically by
recording the duration of the optimisation. Alternatively, one can also
gauge the speed in terms of computational complexity of evaluating the
index function (for instance, in big O notation, with respect to the
sample size) of computing the index function.

# Visualisation of jellyfish optimiser {#sec-vis}

The data structure proposed in @RJ-2021-105 is implemented for JS
optimisers and additional information such as index function used, data
dimension, and jellyfish hyper-parameters can be included as additional
columns. An example data collected from finding the sine-wave structure
in 6D data (`d = 6`) using a spline index function(`splines2d`) is
printed below:

```{r}
dplyr::glimpse(sim_sine_6d_spline_head |> select(-id, -info)) 
```

The jellyfish hyper-parameters used are 50 jellyfishes (`n_jellies`) and
a maximum of 50 tries (`max_tries`). The full data contains 50
simulations, recorded by the variable `sim`, with different seeds
(`seed`). Recorded in columns `basis`, `index_val`, `tries`, `loop` and
`time`, are the sampled projection basis, calculated index value,
iteration ID, jellyfish ID, and time taken to find the optimum,
respectively. The basis column records every visited basis before
comparing with the current (last) basis. If a visited basis has a
smaller index value than the current (last) one, JSO will keep the
current basis for the next iteration in the search but still records the
visited inferior basis. This data can be used to derive numerical
summaries and plots:

-   **Success rate**: The success rate is defined as the proportion of
    simulations that achieve a final index value within 0.05 of the best
    index value found among all 50 simulations. In the dataset above,
    the optimal index value is
    `r sim_sine_6d_spline_best |> pull(index_val) |> max() |> round(3) -> aa; aa`.
    Out of the 50 simulations,
    `r sim_sine_6d_spline_best |> filter(index_val >= aa - 0.05) |> nrow() -> bb; bb`
    achieved an index value within \[`r aa - 0.05`, `r aa`\], resulting
    in a success rate of
    `r sim_sine_6d_spline_best |> mutate(a = index_val >= aa - 0.05) |> pull(a) -> cc; round(sum(cc)/length(cc), 3)`.
    @fig-projection shows the best projection found in each simulation,
    sorted by the index value. Most of the projections show a clear
    sine-wave structure, while the last six show only a straight line
    with clusters at the end. When local optima exist in the projection
    pursuit problem, plotting the projections found in each simulation
    or by each jellyfish can help identify these local optima.

-   **Projection distance**: The projection distance between projection
    bases and the theoretical best basis (or the empirical best basis if
    the theoretical best is unknown) is calculated as the Frobenius
    norm, which measures how close a visited basis is to the best basis.
    Plotting this projection distance against the index value for each
    basis visited reveals how the index value changes as the basis
    approaches the optimal basis, helping to identify the squintability
    of the index function. @fig-index-value-proj-dist shows the index
    value against the projection distance for the given example and its
    binned version. As the projection distance decreases, the index
    value initially increases with an increasing slope and then levels
    off. The index value shows greater variation at medium projection
    distances compared to small and large distances.

```{r}
#| label: fig-projection
#| fig.height: 6
#| fig.width: 10
#| fig.cap: Best projections found by the JS opitmiser in each simulation, sorted by the index value. The projection pursuit problem is to detect the 6D sine-wave structure using the spline index. Most simulations have detected a clear sine-wave pattern, while the last six show a straight line with clustering at the ends.. In this simulation, the success rate is 0.88 (44/50).
proj_dt <- map_dfr(sim_sine_6d_spline_best |> pull(basis), 
              ~as_tibble(as.matrix(sine1000) %*% .x), .id = "sim") |> 
  mutate(sim = as.numeric(sim)) |> 
  left_join(sim_sine_6d_spline_best |> select(sim, index_val), by = "sim")

proj_dt |>
  ggplot(aes(x = V1, y = V2)) + 
  geom_point(size = 0.1) + 
  geom_text(aes(label = round(index_val, 3)), x = 0, y = 3.7, 
            size = 3) +
  xlim(-4, 4) + ylim(-4, 4) +
  facet_wrap(~fct_reorder(as.factor(sim), -index_val), ncol = 10) + 
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())
```

```{r}
#| label: fig-index-value-proj-dist
#| fig.height: 3
#| fig.width: 8
#| fig.cap: "Projection distance plotted against the index value for detecting the 6D sine-wave structure using the spline index function. a) The index values start around 0 with low variance at large projection distances. As the projection distance decreases, the index values increase with a larger variance. Towards the minimum projection distance, the index values increase and cluster around 1, indicating the optimiser is approaching to the optimum. b)  Projection distances are binned with a bin width of 0.1, and index values are averaged over each bin. The relationship between the two variables forms a sigmoid curve."
p1 <- sim_sine_6d_spline_projdist |> 
  ggplot(aes(x = proj_dist, y = index_val)) +
  geom_point(size = 0.1, alpha = 0.5) + 
  labs(x = "Projection distance", y = "Index value")

p2 <- sim_sine_6d_spline_projdist |> 
  group_by(proj_bin = proj_dist %/% 0.1) |> 
  summarise(index_val = mean(index_val)) |> 
  ggplot(aes(x = proj_bin, y = index_val)) + 
  geom_line() +
  geom_point() + 
  labs(x = "Projection distance", y = "Index value")

(p1 | p2) + plot_annotation(tag_levels = "a")
```

# Application \[Di and Sherry\] {#sec-app}

The JSO provides a new swarm-based search strategy to optimise
projection pursuit problems. Two examples are provided in this section
to demonstrate its performance. The first example compares its
performance with the search-better optimiser and explores how it behaves
with different combinations of hyper-parameters. The second example
studies the effect of optimisation properties defined in @sec-theory,
along with jellyfish hyper-parameters, on the outcome of the
optimisation.

## Performance of JSO in pipe-finding problems {#sec-app-1}

The performance of JSO is investigated in two folds: 1) compare it with
a commonly used optimiser: the search-better optimiser [@RJ-2021-105;
@laa_using_2020], and 2) examine its success rate under different
hyper-parameters (number of jellyfishes and maximum number of tries).
The projection pursuit problem used is finding the pipe shape using the
holes index, investigated by @laa_using_2020.

```{r}
#| fig.height = 7.5,
#| fig.width = 10,
#| label = fig-proj,
#| fig.align = "center",
#| fig.cap = "Projections found by the jellyfish and search better optimisers at each 10th quantile across 50 simulations. The projection pursuit problem is to find the pipe shape using the holes index in the 6, 8, 10, and 12-dimensional spaces. The JSO uses 100 jellyfishes and a maximum number of tries of 100. The search better optimiser uses a maximum of 1000 tries in each step of random sampling step before the algorithm terminates. In the 6-D data space, JSO always finds a clear pipe shape while the search better optimiser also finds the pipe shape but with a wide rim. At higher data dimensions, JSO finds a higher index value and a clearer pipe shape across all the quantiles than the search better optimiser."

aaa <- map_dfr(
  list(pipe1000, pipe1000_8d, pipe1000_10d, pipe1000_12d), 
  ~{
    d_dim = dim(.x)[2]
    pipe_jellyfish |> 
      filter(d == d_dim) |> 
      arrange(index_val) |>
      filter(row_number() %in% c(1, seq(5, 50, 5))) |> 
      mutate(id = factor(
        paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
        levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"))) |> 
      compute_projection(data = .x, col = c("d", "index_val", "id"))
  }
)

p1 <- aaa |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = aaa |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x =0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

bbb <- pipe_better |> 
  group_by(dim) |> 
  arrange(index_val) |> 
  filter(row_number() %in% c(1, seq(5, 50, 5))) |>  
  mutate(id = factor(
    paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
    levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th")), 
    d= 4 + 2 * as.numeric(dim)) |> 
  unnest(proj)

p2 <- bbb |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = bbb |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x =0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

(p1 + ggtitle("The jellyfish search optimiser"))/(p2 + ggtitle("The search better optimiser"))
```

```{r}
#| label = fig-proportion,
#| fig.width = 6,
#| fig.height = 3,
#| fig.align = "center",
#| fig.cap = "Proportion of simulations reaches near-optimal index values in the pipe-finding problem using the holes index. The proportion is calculated based on the number of simulations, out of 50, that achieve an index value within 0.05 of the best-performing simulation. As the dimensionality increases, the proportion of simulations reaching the optimal index value increases."
pipe_sim_best <- sim_pipe_run_best |> 
  group_by(n_jellies, max_tries, d) |>
  summarise(I_best = max(I_max))
  
pipe_res <- sim_pipe_run_best |> 
  left_join(pipe_sim_best) |> 
  mutate(diff = abs(I_max - I_best) < 0.05) |> 
  group_by(n_jellies, max_tries, d) |> 
  summarise(proportion = sum(diff) / n()) |> 
  mutate(d = as.factor(d), n_jellies = as.factor(n_jellies))

pipe_res |> 
  rename(`max # of tries` = max_tries) |>
  ggplot(aes(x = n_jellies, y = proportion, 
             group = interaction(d, `max # of tries`),
             color = d)) + 
  geom_line() + 
  scale_color_brewer(palette = "Dark2", name = "dimension") +
  facet_wrap(vars(`max # of tries`), 
             labeller = "label_both") + 
  theme_bw() + 
  xlab("Number of jellyfish") + 
  ylab("Proportion of success")

```

Fifty simulations are conducted with both JSO and the search-better , in
four dimensions ($d = 6, 8, 10, 12$). The JSO uses 100 jellyfishes and
100 maximum number of tries, while the search-better optimiser allows a
maximum of 1000 samples at each iteration before the algorithm
terminates. @fig-proj presents the final projections found by the two
optimisers, broken down by 10th quantile, faceted by the data dimension.
In the 6-dimensional data scenario, JSO consistently identifies a clear
pipe shape. The search-better optimiser also finds the pipe shape but
with a wide rim, suggesting a further polish search may be required.
With increasing dimensions, JSO may not always identify the pipe shape
due to random sampling, but it still presents a pipe shape in over 50%
of cases. When compared to the search-better optimiser, JSO reaches
higher index values and clearer pipe shapes across all quantiles.

In the second experiment, fifty simulations are conducted at each
hyper-parameter combination to analyse its effects on the success rate
of JSO . The hyper-parameters tested include: 1) 20, 50, and 100
jellyfishes, and 2) 50 and 100 maximum number of tries. In each
simulation, the success rate is calculated as defined in @sec-vis.
@fig-proportion presents the proportion of success for each
hyper-parameter combinations. As the number of jellyfishes and maximum
tries increase, the success rate also increases. For simpler problems (6
dimensions), small parameter values (20 jellyfishes and a maximum number
of tries of 50) can already result in high success rate, while for
higher-dimensional problems (8, 10, and 12 dimensions), a combination of
100 jellyfishes and 100 maximum number of tries is necessary for at
least half of the jellyfishes to find the optimal projection.

## Factors affecting JSO success rate: optimisation properties and jellyfish hyper-parameters {#sec-app-2}

The optimisation properties, including smoothness, squintability, and
speed, offer numerical metrics to characterise the complexity of a
projection pursuit optimisation problem. This example investigates how
these metrics, along with the jellyfish search hyper-parameters (the
number of jellyfishes and the maximum number of tries), affect the
success of JSO. Simulations are conducted to obtain the success rate
across various projection pursuit problems, characterised by
shape-to-find, data dimension, and index function used, as well as
different hyper-parameter combinations. Smoothness and squintability are
calculated for each projection pursuit problem as outlined in
@sec-theory. A generalised linear model is used to construct the
relationship between the success rate, jellyfish hyper-parameters and
optimisation properties.

In addition to the pipe-finding problem, a new shape, sine wave, is
investigated in 6D and 8D spaces with six indices considered:
`dcor2d_2`, `loess2d`, `MIC`, `TIC`, `spline`, and `stringy`. Combining
with two jellyfish hyper-parameters, a total of `r nrow(sim_df)` cases
is produced, comprising of `r nrow(filter(sim_df, index == "holes"))`
pipe-finding cases and `r nrow(filter(sim_df, index != "holes"))`
sine-wave finding cases. For each case, JSO is run fifty times and the
success rate is calculated as in @sec-app-1.

Smoothness and squintability are computed for each case, following the
procedures outlined in @sec-smoothness and @sec-squintability. To
calculate smoothness, three hundred random bases are simulated. Index
values and projection distance (to the optimal basis) are calculated for
each random basis before fitting them into a Gaussian process model. For
squintability, fifty random bases are sampled and interpolated to the
optimal basis with a step size of 0.005. For these interpolated bases,
index values and projection distances to the optimal basis are
calculated and averaged with a bin width of 0.005. The squintability
measure is then calculated from fitting a 4-parameter scaled logistic
function of index value against projection distance, estimated by
non-linear least squares.

@tbl-smoothness-squintability presents the parameters estimated from the
Gaussian process (variance, range, smooth, and nugget) and the scaled
logistic function ($\theta_1$ to $\theta_4$) for each case. The column
"smooth" is used as the smoothness measure and the column "squint" is
calculated as \[TODO: equation reference\] as squintability. The low
squint value of MIC and TIC index is due to its convex shape of the
index value against the projection distance, as shown in
@fig-idx-proj-dist. Comparing to other concave shapes, the maximum first
derivative happens at a smaller projection distance ($\theta_2$),
requiring the optimiser to get closer to the optimal to see a
significant change in the index value, hence a small squintability
measure.

```{r fig-idx-proj-dist}
#| fig.width = 9,
#| fig.height = 5,
#| fig.cap = "Index values versus projection distance for the 12 pipe/sine-wave finding problem, after the binning procedure for calculating the squintability measure. The index values, averaged at bin width of 0.005, are scaled from 0 to 1 for comparison (holes, TIC, and stringy). The `MIC` and `TIC` index curves are convex while others are concave. The stringy curve shows an instantaneous jump to the optimum when aproaching the best basis." 
sq_basis_dist_idx |>
  ggplot(aes(x = dist, y = index, group = index_name)) +
  geom_line() + 
  facet_nested_wrap(~index_name + n, nrow = 3, labeller = label_both) + 
  xlab("projection distance") + ylab("index value") 
```

```{r}
#| label: tbl-smoothness-squintability
#| tbl-cap: Parameters estimated from the Gaussian process (including variance, range, smoothness, and nugget) and scaled logistic function ($\theta_1$ to $\theta_4$) for the pipe-finding and sine-wave finding problems.  The column "smooth" and "squint" represent the smoothness and squintability measures. 
dt <- tibble(shape = c(rep("pipe", 4), rep("sine", 8)), smoothness) |>
  left_join(squintability) |> 
  rename(d = n, smooth = smoothness) |> 
  mutate(index = ifelse(index == "dcor2d_2", "dcor2d", index)) 
dt |> 
  knitr::kable(
    digits = 3, format = "latex", 
    col.names = c(
      "shape", "index", "d",
      "variance", "range", "smooth", "nugget",
      "$\\theta_1$", "$\\theta_2$", "$\\theta_3$", "$\\theta_4$", "squint"),
    booktabs = T, escape = FALSE, row.names = TRUE) |> 
  column_spec(1, border_left = TRUE) |> 
  column_spec(c(4, 8, 13), border_right = TRUE) |> 
  kable_styling(font_size = 7) 
```

```{r}
#| label: tbl-mod-data
#| tbl-cap: The first 7 rows of the datasets processed for modelling. 
sim_df |> 
  dplyr::select(index, d, smoothness, squintability, n_jellies, max_tries, 
                -I_max, P_J, time) |> 
  arrange(index, d, n_jellies, max_tries) |> 
  head(7) |> 
  kable(digits = 3,
        col.names = c("Index", "D", "Smoothness", "Squintability", 
                      "Num. of jellyfish", "Max. Num. of tries", 
                      "Prop. of success", "Time"), 
        booktabs = T) |> 
  kable_styling(latex_options = "hold_position", font_size = 7)
```

```{r}
#| label: tbl-mod-output
#| tbl-cap: Model estimates of proportion of jellyfish success on optimisation properties and jellyfish hyper-parameters from the generalised linear model with a binomial family and a logit link function. The variable smoothness, squintability, number of jellyfish and maximum number of tries are positively associated with JSO success rate while data dimension and being flagged as long runtime are negatively associated with the success rate. The variable squintability and dimension are significant, suggesting their importance relative to jellyfish hyper-parameters in the optimisation success.
sim_df2 <- sim_df |>   
  mutate(n_jellies = n_jellies/10,
         max_tries = max_tries/10,
         long_time = ifelse(time > 20, 1, 0),
         P_J = ifelse(index == "stringy", 0, P_J))

mod1 <- glm(P_J ~ smoothness + squintability + d + long_time + n_jellies + max_tries, data = sim_df2, family = binomial)
broom::tidy(mod1) |> 
  mutate(term = c("Intercept", "Smoothness", "Squintability", "dimension (d)", 
                  "long time", "number of jellyfish", "maximum number of tries")) |>
  kable(digits = 3) |> 
  column_spec(c(1,5), border_right = TRUE) |> 
  column_spec(1, border_left = TRUE) |> 
  kable_styling(font_size = 7) 
```


@tbl-mod-data combines all the variables calculated above (the JSO success rate, JSO hyper-parameters, and optimisation
properties) into one table. Three preprocessing steps are applied to the
data: 1) scaling the JSO hyper-parameters by a factor of 10 for
interpretation, 2) creating a new binary variable `long_time` to
indicate cases with an average run time over 30 seconds, and 3) encoding
the success rate for the stringy case as 0, because upon visual inspection of the final projection, none of the 50
simulations identified the sine-wave shape. A generalised linear model
with a binomial family and a logit link function is used to fit the data
and @tbl-mod-output presents the model outputs. The model suggests that
the JSO success rate is positively associated with the two JSO
hyper-parameters, as well as with the optimisation properties -- smoothness and squintability. Specifically, using 10 more jellyfish and 10 more tries will increase the odd ratio of success by `r round(100 * (exp(broom::tidy(mod1)$estimate[6])-1), 2)`% and `r round(100 * (exp(broom::tidy(mod1)$estimate[7])-1), 2)`%, respectively. However,
being flagged with long runtime and an increase of data dimension will
reduce the success rate by `r round(exp(broom::tidy(mod1)$estimate[5]) * 100, 2)`% and `r round(exp(broom::tidy(mod1)$estimate[4]) * 100, 2)`%, respectively.
The variable `squintability` and `dimension`
are significant, suggesting their importance relative to JSO
hyper-parameters in the optimisation success. Users can increase the 
number of jellyfish and the number of tries to improve the success rate, until JSO exceeds a 30-second runtime.

# Conclusion \[Di and Sherry\] {#sec-conclusion}

# References
