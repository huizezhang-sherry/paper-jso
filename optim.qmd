---
title: Studying the Performance of the Jellyfish Optimiser for the Application of Projection Pursuit
header-includes:
   - \usepackage{algorithm}
   - \usepackage{algpseudocode}
author:
  - name: Alice Anonymous
    email: alice@example.com
    affiliations: 
        - id: some-tech
          name: Some Institute of Technology
          department: Department Name
          address: Street Address
          city: City
          state: State
          postal-code: Postal Code
    attributes:
        corresponding: true
  - name: Bob Security
    email: bob@example.com
    affiliations:
        - id: another-u
          name: Another University
          department: Department Name
          address: Street Address
          city: City
          state: State
          postal-code: Postal Code
  - name: Cat Memes
    email: cat@example.com
    affiliations:
        - ref: another-u
  - name: Derek Zoolander
    email: derek@example.com
    affilations:
        - ref: some-tech
abstract: |
  This is the abstract. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum augue turpis, dictum non malesuada a, volutpat eget velit. Nam placerat turpis purus, eu tristique ex tincidunt et. Mauris sed augue eget turpis ultrices tincidunt. Sed et mi in leo porta egestas. Aliquam non laoreet velit. Nunc quis ex vitae eros aliquet auctor nec ac libero. Duis laoreet sapien eu mi luctus, in bibendum leo molestie. Sed hendrerit diam diam, ac dapibus nisl volutpat vitae. Aliquam bibendum varius libero, eu efficitur justo rutrum at. Sed at tempus elit.
keywords: 
  - projection pursuit
  - optimization
  - jellyfish optimiser
  - data visualisation
  - high-dimensional data
date: last-modified
bibliography: bibliography.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal of Multivariate Analysis
      formatting: preprint
      model: 3p
      cite-style: numbername
editor: 
  markdown: 
    wrap: 72
---

*Let's use British English ("American or British usage is accepted, but
not a mixture of these")*

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(ggh4x)
library(broom)
library(kableExtra)
library(ferrn)
load(here::here("data/sim_pipe.rda"))
load(here::here("data/pipe_better.rda"))
load(here::here("data/pipe_jellyfish.rda"))
load(here::here("data/smoothness.rda"))
load(here::here("data/squintability.rda"))
load(here::here("data/sq_basis_dist_idx.rda"))
load(here::here("data/sim_df.rda"))
```

# Introduction \[Nicolas and Jessica\]

The artificial jellyfish search (JS) algorithm [@chou_novel_2021] is a
swarm-based metaheuristic optimisation algorithm inspired by the search
behaviour of jellyfish in the ocean. It is one of the newest swarm
intelligence algorithms [@rajwar_exhaustive_2023], which was shown to
have stronger search ability and faster convergence with few algorithmic
parameters compared to classic optimization methods
[@chou_novel_2021]-[@chou_recent_2022].

Effective optimisation is an important aspect of many methods employed
for visualising high-dimensional data ($X$). Here we are concerned about
computing informative linear projections of high-dimensional ($p$) data
using projection pursuit (PP) (@kr69, @FT74). This involves optimising a
function (e.g. @hall1989polynomial, @cook1993projection,
@lee2010projection, @Loperfido2018, @Loperfido2020), called the
projection pursuit index (PPI), that defines what is interesting or
informative in a projection.

These PPI are defined on projections ($XA$), which means that there is a
constraint that needs to be considered when optimising. A projection of
data is defined by a $p\times d$ orthonormal matrix $A$, and this
imposes the constraint on the elements of $A$, that columns need have
norm equal to 1 and the product of columns need to sum to zero.

@cook1995grand introduced the PP guided tour, which enabled interactive
visualisation of the optimisation in order to visually explore
high-dimensional data. It is implemented in the R [@R] package `tourr`
[@tourr]. The optimisation that is implemented is fairly basic, and
potential problems were highlighted by @RJ-2021-105. Implementing better
optimisation functionality is a goal, but it needs to be kept in mind
that the guided tour also has places importance on watching the
projected data as the optimisation progresses.

Here we explore the potential for a jellyfish optimisation to be
integrated with the guided tour. @sec-background explains the
optimisation that is used in the current the projection pursuit guided
tour. @sec-theory provides more details on the jellyfish optimiser and
formalises several characteristics of projection pursuit indexes that
are help to measure optimiser performance. @sec-simulation describes a
simulation study on performance of the jellyfish for several types of
data and index functions. @sec-conclusion summarises the work and
provides suggestions for future directions.

# Projection pursuit, index functions and optimisation \[Di and Sherry\] {#sec-background}

<!-- Need to assume that the special issue will have the general introduction to PP, so the focus here will be on background needed for this paper, tours, guidance using PP, optimisation, visualisation, ... -->

A tour on high-dimensional data is constructed by geodesically
interpolating between pairs of planes. Any plane is described by an
orthonormal basis, $A_t$, where $t$ represents time in the sequence. The
term "geodesic" refers to maintaining the orthonormality constraint so
that each view shown is correctly a projection of the data. The PP
guided tour operates by geodesically interpolating to target planes
(projections) which have high PP index values, as provided by the
optimiser. The geodesic interpolation means that the viewer sees a
continuous sequence of projections of the data, so they can watch
patterns of interest forming as the function is optimised. There are
five optimisation methods implemented in the `tourr` package:

-   `search_geodesic()`: provides a pseudo-derivative optimisation. It
    searches locally for the best direction, based on differencing the
    index values for very close projections. Then it follows the
    direction along the geodesic path between planes, stopping when the
    next index value fails to increase.
-   `search_better()`: is a brute-force optimisation searching randomly
    for projections with higher index values.
-   `search_better_random()`: is essentially simulated annealing
    [@Bertsimas93] where the search space is reduced as the optimisation
    progresses.
-   `search_posse()`: implements the algorithm described in @posse95.
-   `search_polish()`: is a very localised search, to take tiny steps to
    get closer to the local maximum.

There are several PP index functions available: `holes()` and `cmass()`
[@cook1993projection]; `lda_pp()` [@lee2005projection]; `pda_pp()`
[@lee2010projection]; `dcor2d()` and `splines2d()` [@Grimm2016];
`norm_bin()` and `norm_kol()` [@huber85]; `slice_index()`
[@Laa:2020wkm]. Most are relatively simply defined, for any projection
dimension, and implemented because they are relatively easy to optimise.
A goal is to be able to incorporate more complex PP indexes, for example
based on scagnostics (@scag, @WW08).

An initial investigation of PP indexes, and the potential for
scagnostics is described in @laa_using_2020. To be useful here an
optimiser needs to be able to handle functions which are not very
smooth. In addition, because data structures might be relatively fine,
the optimiser needs to be able to find maxima that occur with a small
squint angle, that can only be seen from very close by. One last aspect
that is useful is for an optimiser to return local maxima in addition to
global because data can contain many different and interesting features.

# The jellyfish optimiser and properties of PP indexes \[Nicolas and Jessica\] {#sec-theory}

The jellyfish optimiser (JSO) mimics the natural movements of jellyfish,
which include passive and active motions driven by ocean currents and
their swimming patterns, respectively. In the context of optimization,
these movements are abstracted to explore the search space in a way that
balances exploration (searching new areas) and exploitation (focusing on
promising areas). The algorithm aims to find the optimal solution by
adapting the jellyfish's behaviour to navigate towards the best solution
over iterations [@chou_novel_2021].

To understand what the jellyfish optimizer is doing in the context of
Projection Pursuit, we first start with a current projection (the
starting point). Then, we evaluate this projection using an index
function, which tells us how good the current projection is. We then
move the projection in a direction determined by the 'best jelly' and
random factors, influenced by how far along we are in the optimization
process (the trial $i$ and `max.tries`). Occasionally, we might explore
completely new directions like a jellyfish might with ocean currents.
Then, we compare new potential projections to our current one. If
they're better, we adopt them; if not, we stick with our current
projection. This process continues and iteratively improves the
projection, until we reach the maximum number of trials.

::: {.callout-note icon="false"}
## Algorithm: Jellyfish Optimizer Pseudo Code

**Input**: `current_projections`, `index_function`, `tries`, `max_tries`

**Output**: `optimized_projection`

**Initialize** `best_jelly` as the projection with the best index value
from `current_projections`, and `current_index` as the array of index
values for each projection in `current_projections`

**for** each try in 1 to max_tries **do**

> Calculate $c_t$ based on the current try and max_tries

> **if** $c_t$ is greater than or equal to $0.5$ **then**
>
> > Define trend based on the best jelly and current projections
>
> > Update each projection towards the trend using a random factor and
> > orthonormalisation
>
> **else**
>
> > **if** a random number is greater than $1 - c_t$ **then**
> >
> > > Slightly adjust each projection with a small random factor (Type A
> > > passive)
> >
> > **else**
> >
> > > For each projection, compare with a random jelly and adjust
> > > towards or away from it (Type B active)
>
> Update the orientation of each projection to maintain consistency
>
> Evaluate the new projections using the index function

> **if** any new projection is worse than the current, revert to the
> `current_projection` for that case
>
> > Determine the projection with the best index value as the new
> > best_jelly

> **if** the try is the last one, print the final best projection and
> **exit**

**return** the set of projections with the updated best_jelly as the
optimized_projection
:::

<!-- Var names in the function and in the text needs to be consistent??? -->

The JSO implementation involves several key parameters that control its
search process in optimization problems. These parameters are designed
to guide the exploration and exploitation phases of the algorithm. While
the specific implementation details can vary depending on the version of
the algorithm or its application, we focus on two main parameters that
are most relevant to our application: the number of jellyfish and drift.

@laa_using_2020 has proposed five criteria for assessing projection
pursuit indexes (smoothness, squintability, flexibility, rotation
invariance, and speed). Since not all the properties affects the
execution of the optimisation, here we consider the three relevant
properties (smoothness, squintability, and speed), and propose three
metrics to evaluate these three properties.

## Smoothness {#sec-smoothness}

```{=html}
<!--
An intuitive way to measure smoothness of a function would be to count
how many continuous derivatives exist. 
To help define smoothness in our
context, we can make use of the Sobolev spaces: functions $f$ in the
Sobolev space $W^{p,\infty}$ have all derivatives of order less than $p$
continuous. Smoothness would then be the highest $p$ such that the index
function belongs to $W^{p,\infty}$. We can relax this and consider
$W^{p,q}$ Sobolev spaces.

Consider the following definition of partial derivative. Let $U$ be an
open subset of $\mathbf{R}^n$ and $f: U \rightarrow \mathbf{R}$. The
partial derivative of function $f$ at the point
$\mathbf{a} = (a_1, \ldots, a_n) \in U$ with respective to the $i$-th
variable $x_i$ is defined as

$$
\frac{\delta}{\delta x_i} f(\mathbf{a}) =  \lim_{h \rightarrow 0} \frac{f(a_1, \ldots, a_{i-1}, a_i +h, a_{i+1}, \ldots, a_n) - f(a_1 , \ldots, a_i, \ldots, a_n)}{h} = \lim_{h\rightarrow 0} \frac{f(\mathbf{a} + h\mathbf{e_i}) - f(\mathbf{a})}{h}.
$$ where $\mathbf{e_i}$ is the unit vector of the $i$-th variable $x_i$.

If the derivative is not well-defined, we propose to approximate the
derivative using the following expression:

$$\frac{1}{n_i}\sum_{i} \frac{\|f(\mathbf{a} + h_i\mathbf{e_i}) - f(\mathbf{a})\|^p}{h_i}$$

where $p \in [0,1]$. The choice of $p$ reflects the penalty behaviour.
For the application in this paper, we choose $p = 1$.

-   $h_i$ is an neighbourhood area around $a$. We can insert different
    values of $h$ to approximate the above quantity and observe how this
    quantity changes.
-->
```
If we evaluate the index function at some random points (like the random
initialization of the jellyfish optimizer), then we can interpret these
random index values as a random field, indexed by a space parameter: the
random projection angle. This analogy suggests to use this random
training sample to fit a spatial model, a simple one being a (spatial)
Gaussian process.

```{=html}
<!--
There exist R packages to fit the hyperparameters of a Gaussian process, for example ExaGeoStatR (https://github.com/ecrc/exageostatR), which is an R wrapper for the ExaGeoStat library (https://arxiv.org/pdf/1708.02835.pdf).
-->
```
How can we define a measure of smoothness from this? The distribution of
a Gaussian process is fully determined by its mean function and
covariance function. The way the covariance function is defined is where
smoothness comes into play: if an index is very smooth, then two close
projection angles should produce close index values (strong
correlation); by contrast, if an index is not smooth, then two close
projection angles might give very different index values (fast decay of
correlations with respect to distance between angles).

Popular covariance functions are parametric positive semi-definite
functions, some of which have a parameter to capture the smoothness of
the Gaussian field. In particular, consider the Matérn class of
covariance functions, defined by

$$
K(u):=\frac{(\sqrt{2\nu}u)^{\nu}}{\Gamma(\nu)2^{\nu-1}}\mathcal{K}_{\nu}(\sqrt{2\nu}u)
$$

where $\nu>0$ is the smoothness parameter and where $\mathcal{K}_\nu$ is
the modified Bessel function. The Matérn covariance function can be
expressed analytically when $\nu$ is a half-integer, the most popular
values in the literature being $1/2$, $3/2$ and $5/2$ . The parameter
$\nu$, called smoothness parameter, controls the decay of the covariance
function. As such, it is an appropriate measure of smoothness of a
random field.

In our context, we suggest to use this parameter as a measure of the
smoothness of the index function by fitting a Gaussian process prior
with Matérn covariance on a dataset generated by random evaluations of
the index function, as in the initial stage of the jellyfish random
search. There exist several R packages, such as GpGp or ExaGeoStatR, to
fit the hyperparameters of a GP covariance function on data. In this
project, we make use of the GpGp package.

The fitted value $\nu>0$ can be interpreted as follows: the higher
$\nu$, the smoother the index function.

## Squintability {#sec-squintability}

From the literature, it is commonly understood that a large squint angle
implies that the objective function value is close to optimal even when
we are not very close to the perfect view to see the structure. A small
squint angle means that index function value improves substantially only
when we are very close to the perfect view. As such, low squintability
implies rapid improvement in the index value when near the perfect view.

In this study, we propose two metrics to capture the notion of
squintability.

\[We generate random points that is beyond 1.5 projection distance and
interpolate. Then we fit a kernel or use nonlinear least squares.\]

First, parametric model.

\[Nicolas's pdf\]

Second, we consider the product of the largest absolute magnitude of
rate of change of $f$ and the corresponding projection angle as a second
measure of squintability. Since $f$ is decreasing, the rate of change of
$f$ is negative and thus $|\underset{x}{\min} f('x)|$ gives the absolute
magnitude of the most negative rate of change.

\[Nicolas's pdf\]

To the best of our knowledge, this is the first attempt to measure the
notion of squintability.

## Speed

The speed of optimizing an index function can be calculated/measured
using the computational complexity (in big O notation, with respect to
the sample size) of computing the index function.

# Visualisation of jellyfish optimiser

Information of the jellyfish optimiser is available in a tabular format
and below is an example data collected from finding the sine-wave
structure in 6D data using a distance correlation index (`docr2d_2`):

```{r}
load(here::here("data/sim_example.rda"))
```

```{r}
dplyr::glimpse(sim_example |> select(-id)) 
```

Information recorded can be categorised into the following categories:

-   projection pursuit variables: the index function used (`idx_f`), the
    data dimension (`d`)
-   jellyfish optimiser parameters: the number of jellies (`n_jellies`),
    the maximum number of tries (`max_tries`)
-   simulation variables: the simulation number (`sim`), the seed used
    (`seed`)
-   optimisation variables: the projection basis in a matrix format
    (`basis`), the index value (`index_val`), a description of the
    status - one of "initiation", "current_best", and "jellyfish_update"
    (`info`), current iteration ID (`tries`), current jelly ID (`loop`),
    and the time taken to find the optimum (`time`).

The basis column records every basis *visited* by the jellyfish
optimiser prior to comparing with the current basis. In each iteration,
if the index value of a visited basis is smaller than that of the
current one, the jellyfish optimiser will retain the current basis for
the next iteration, while still documenting the visited basis.

Numerical information to compute:

-   angular distance between the projection basis and the theoretical
    best basis,
-   the proportion of simulation that found the optimal basis,
-   the proportion of jellies, within each simulation, that found the
    optimal basis,

Visualisation to inspect:

-   inspect the basis visited by each jellyfish in the reduced PCA
    space,
-   inspect the final 2D projections reached by each jellyfish,
-   plot the index value against the angular distance between the
    projection basis and the theoretical best basis

Plotting the basis in the space and the projected data can help to
understand 1) whether each simulation finds the same optimum or some
simulations find local optima; and 2) whether the index function used
can detect the structure in the data and the projection contains the
structure of interest.

The visualisations above can be faceted by the projection pursuit
variables and jellyfish optimiser parameters to compare the performance
of different indexes to detect the same structure and how the jellyfish
optimiser parameters affect the optimisation process.

\[example plots\]

```{r}
load(here::here("data/sine_dcor2d_best.rda"))
set.seed(123456)
sin1000 <- spinebil::sinData(6, 1000) %>% scale() %>% as_tibble()
colnames(sin1000) <- paste0("V", 1:6)


proj_dt <- map_dfr(sine_dcor2d_best |> pull(basis), 
              ~as_tibble(as.matrix(sin1000) %*% .x), .id = "sim") 

p1 <- proj_dt |>
  ggplot(aes(x = V1, y = V2)) + 
  geom_point(size = 0.1) + 
  xlim(-4, 4) + ylim(-4, 4) +
  facet_wrap(~as.numeric(sim), ncol = 10) + 
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())
```

```{r}
#| fig.height: 5 
#| fig.width: 10
#| fig.cap: sdfjsflk
p1 
```

```{r eval = FALSE}
theo_best <- matrix(c(rep(0, 8), 1, 0, 0, 1), nrow = 6, byrow = TRUE)
angle_dcor2d <- sine_dcor2d_5050 |>
    rowwise() |>
    mutate(angle = tourr::proj_dist(theo_best, basis))

p2 <- angle_dcor2d |>
  ggplot(aes(x = angle, y = index_val)) +
  geom_point(size = 0.1) +
  theme(aspect.ratio = 1) 

```

# Application \[Di and Sherry\] {#sec-simulation}


[maybe a leading paragraph here to summarise the two examples]

## Performance of jellyfish search optimiser in pipe-finding problems

The jellyfish search optimiser introduces a new swarm-based search strategy to optimise the projection pursuit problems. In this example, the performance of jellyfish search optimiser is investigated in two folds: 1) compare the jellyfish search optimiser with a commonly used optimiser: the search better optimiser [@RJ-2021-105; @laa_using_2020], and 2) examine the jellyfish search optimiser under different hyper-parameters: the number of jellyfish (20, 50, and 100) and the maximum number of tries (50 and 100). The projection pursuit problem investigated is finding the pipe shape using the holes index in 6, 8, 10, and 12-dimensional data, as investigated by @laa_using_2020. 

```{r} 
#| fig.height = 7.5,
#| fig.width = 10,
#| label = fig-proj,
#| fig.align = "center",
#| fig.cap = "Projections found by the jellyfish and search better optimisers at each 10th quantile across 50 simulations. The projection pursuit problem is to find the pipe shape using the holes index in the 6, 8, 10, and 12-dimensional spaces. The jellyfish optimiser uses 100 jellyfish and 100 maximum number of tries, while the search better optimiser uses a maximum 1000 tries in each step of random sampling. The index value found by the jellyfish optimiser finds the pipe shape across all simulations in the 6-dimensional space and reaches a higher index value and clearer pipe shape in higher dimensions than the search better optimiser."

aaa <- map_dfr(
  list(pipe1000, pipe1000_8d, pipe1000_10d, pipe1000_12d), 
  ~{
    d_dim = dim(.x)[2]
    pipe_jellyfish |> 
      filter(d == d_dim) |> 
      arrange(index_val) |>
      filter(row_number() %in% c(1, seq(5, 50, 5))) |> 
      mutate(id = factor(
        paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
        levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"))) |> 
      compute_projection(data = .x, col = c("d", "index_val", "id"))
  }
)

p1 <- aaa |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = aaa |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x =0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

bbb <- pipe_better |> 
  group_by(dim) |> 
  arrange(index_val) |> 
  filter(row_number() %in% c(1, seq(5, 50, 5))) |>  
  mutate(id = factor(
    paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th"),
    levels = paste0(c(0, seq(5, 50, 5)) / 50 * 100, "th")), 
    d= 4 + 2 * as.numeric(dim)) |> 
  unnest(proj)

p2 <- bbb |> 
  ggplot() + 
  geom_point(aes(x = V1, y = V2), size = 0.05) + 
  geom_text(data = bbb |> select(-V1, -V2) |> unique(), 
            aes(label = round(index_val,3)), x =0, y = 3.7, 
            size = 3) + 
  facet_grid(d ~ id) + 
  xlim(-4, 4) + ylim(-4, 4) +
  theme_bw() +
  theme(aspect.ratio = 1, axis.ticks = element_blank(),
        axis.text = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank())

(p1 + ggtitle("The jellyfish optimiser"))/(p2 + ggtitle("The search better optimiser"))
```

```{r}
#| label = fig-proportion,
#| fig.width = 6,
#| fig.height = 3,
#| fig.align = "center",
#| fig.cap = "Proportion of jellyfishes reaches near-optimal index values in the pipe-finding problems. The proportion is calculated as the number of jellyfish, out of the total number of jellyfish across 50 simulations, that have an index value within 0.05 of the best jellyfish's index value across all simulations. As the dimensionality increases, the proportion of jellyfishes reaching the optimal index value decreases. Increasing the jellyfish hyperparameters (number of jellyfish` and maximum number of tries) improves the proportion."
pipe_run_summ <- sim_pipe |>
  mutate(idx_f = "holes") |>
  group_by(id, n_jellies, max_tries, d) |> 
  summarise(I_max = max(index_val)) 
  
pipe_sim_best <- pipe_run_summ |> 
  group_by(n_jellies, max_tries, d) |>
  summarise(I_best = max(I_max))
  
pipe_res <- pipe_run_summ |> 
  left_join(pipe_sim_best) |> 
  mutate(diff = abs(I_max - I_best) < 0.05) |> 
  group_by(n_jellies, max_tries, d) |> 
  summarise(proportion = sum(diff) / n()) |> 
  mutate(d = as.factor(d), n_jellies = as.factor(n_jellies))

pipe_res |> 
  rename(`max # of tries` = max_tries) |>
  ggplot(aes(x = n_jellies, y = proportion, 
             group = interaction(d, `max # of tries`),
             color = d)) + 
  geom_line() + 
  scale_color_brewer(palette = "Dark2", name = "dimension") +
  facet_wrap(vars(`max # of tries`), 
             labeller = "label_both") + 
  theme_bw() + 
  xlab("Number of jellyfish") + 
  ylab("Proportion of success")

```

In the first experiment, fifty simulations for both the jellyfish and the search better optimiser, across the four dimensions and @fig-proj presents the final projections found by the two optimisers, broken down by 10th quantile. The jellyfish optimiser uses 100 number of jellyfishes and 100 maximum number of tries, while a maximum of 1000 samples are allowed before the search better optimiser terminates, at each iteration of sampling. In the 6-dimensional data scenario, the jellyfish optimiser finds the clear pipe shape at all the 50 simulations, while the shape found by the search better optimiser begin to blur in worse case scenarios. As the dimension increases, while the jellyfish optimiser may not find the pipe shape due to random sampling, the pipe shape can be clearly seen more than 50% of the time across all dimensions. Compared with the search better optimiser, the jellyfish optimiser achieves higher index values and clearer pipe shapes. 

In the second experiment, fifty simulations are conducted for each combination of jellyfish hyper-parameters:  50 and 100 maximum number of tries and 20, 50, and 100 jellyfishes to understand the effect of hyper-parameters on the performance of the jellyfish optimiser. A optimisation run is considered successfully finding the target shape if its final index value is within 0.05 of the best index value found under the same scenario across all simulations. For example, in the 8-dimensional data scenario with 50 maximum number of tries, the best optimiser achieved an index value of `r pipe_sim_best |> filter(n_jellies == 50, max_tries == 50, d == 8) |> pull(I_best) |> round(3)`. Out of the 50 simulations, `r pipe_run_summ |> filter(n_jellies == 50, max_tries == 50, d == 8) |> filter(I_max >= 0.997 - 0.05) |> nrow() -> aa; aa` found an index value within 0.05 of the best value, resulting in a successful rate of `r pipe_run_summ |> filter(n_jellies == 50, max_tries == 50, d == 8) |> mutate(a = I_max >= 0.997 - 0.05) |> pull(a) -> bb; round(sum(bb)/length(bb), 3)` (`r aa`/ 50). @fig-proportion presents the proportion of successful optimiser for each scenario. As the number of jellyfish and maximum tries increase, the proportion of jellyfish finding the optimal projection also increases. For simpler problems (6 dimensions), small parameter values can result in high successful rate, while for higher-dimensional problems (8, 10, and 12 dimensions), a combination of 100 jellyfishes and 100 maximum number of tries is necessary for at least half of the jellyfishes to find the optimal projection. 

## Connecting optimisation success with its properties and jellyfish parameters

The optimisation properties (smoothness, squintability, and speed) proposed in the @sec-theory provide numerical measures to characterise the difficulty of the optimisation for a given projection pursuit problem. This example investigates how these measures, combined with the jellyfish parameters, affect the success of the jellyfish optimisers and how they can guide the decisions made in hyperparameter tuning with the jellyfish optimiser. Simulations are conducted with different jellyfish optimisers to obtain the proportion of successful optimisation across a collection of scenarios, for which smoothness, squintability, and speed are calculated. A generalised linear model is used to link the proportion of success rate with the jellyfish parameters and the optimisation properties.


In addition to the pipe-finding problem, also investigated is the sine-wave finding problem in 6D and 8D spaces with six indices considered: `dcor2d_2`, `loess2d`, `MIC`, `TIC`, `spline`, and `stringy`. Combining with different jellyfish optimiser parameters (`n_jellies` and `max_tries`), a total of `r nrow(sim_df)` cases is produced, comprising of `r nrow(filter(sim_df, index == "holes"))` pipe-finding cases  and `r nrow(filter(sim_df, index != "holes"))` sine-wave finding cases. For each case, the jellyfish optimiser is run fifty (50) times and summary statistics are calculated on the best index value found across all 50 runs and the proportion of runs that find a close (with a difference less than 0.05) best index value (`P_J_hat`).

Smoothness and squintability are computed for each case, following the procedures outlined in @sec-smoothness and @sec-squintability. To calculate smoothness, three hundred random bases are simulated. Index values and projection distance to the optimal basis are calculated for each basis to fit a Gaussian process model. For squintability, fifty random bases are sampled and interpolated to the optimal basis with a step size of 0.005. The interpolated bases are first binned with a bin width of 0.005 to average the index values before feeding into a 4-parameter scaled logistic function, estimated by non-linear least square, to obtain the squintability measure.

@tbl-smoothness-squintability presents the parameters estimated for each case from the Gaussian process (variance, range, smooth, and nugget) for smoothness and the scaled logistic function (theta1 to theta4, and squint) for squintability. The column "smooth" and "squint" are used as the smoothness and squintability measure. The low squint value of MIC and TIC index is due to its convex shape of the index value against the projection distance, as shown in @fig-idx-proj-dist. Comparing to other concave shapes, the maximum first derivative happens at a smaller projection distance ($\theta2$), requiring the optimiser to get closer to the optimal to see a significant change in the index value, hence a small squintability measure. [TODO: the stringy squintability doens't make sense]

@tbl-mod-data presents the processed data for modelling after augmenting the jellyfish parameters (`n_jellies` and `max_tries`) and the average time spent on each run (`time`) for each case. A generalised linear model is fitted with a binomial family and a logit link function to the data to investigate the relationship between the proportion of jellyfish success and jellyfish parameters and the optimisation properties. Pre-processing of the data include 1) scale the jellyfish parameters (`n_jellies` and `max_tries`) by 10 for interpretation, and 2) create a binary variable `long_time` to flag the cases with average time spent on each run greater than 30 seconds. @tbl-mod-output presents the model output with the estimated coefficients. The model suggests a positive relationship between the proportion of jellyfish success and the jellyfish parameters, `n_jellies` and `max_tries`, optimisation property parameter, `smoothness` and `squintability`, and a negative relationship with the average time spent on each run (`long_time`) and the data dimension (`d`). The variable `squintability` and `d` are significant, suggesting their importance over jellyfish parameters to the success of the optimisation.

<!-- @tbl-all-sim displays the first five rows after combining all calculations.  -->

```{r}
#| label: tbl-smoothness-squintability
#| tbl-cap: Parameters estimated from the Gaussian process (including variance, range, smoothness, and nugget) and scaled logistic function (theta1 to theta4) for the pipe-finding and sine-wave finding problems. The squint column is calculated as $\frac{\theta1 \theta2 \theta3}{4}$, as described in @sec-squintability. The "smooth" and "squint" column represent the smoothness and squintability measures. 
tibble(shape = c(rep("pipe", 4), rep("sine", 8)), smoothness) |>
  left_join(squintability) |> 
  rename(d = n, smooth = smoothness) |> 
  knitr::kable(digits = 3, booktabs = T) |> 
  column_spec(1, border_left = TRUE) |> 
  column_spec(c(3, 7, 12), border_right = TRUE) |> 
  kable_styling(latex_options = "hold_position", font_size = 7)
```

```{r fig-idx-proj-dist}
#| fig.width = 9,
#| fig.height = 5,
#| fig.cap = "Index values against projection distance for the 12 pipe/sine-wave finding problem after the binning procedure during the estimation of the squintability measure. The index value is averaged at each bin width of 0.005 and the `TIC` index is scaled to 0-1 for comparison. When finding the sine-wave structure using the `MIC` and `TIC` index, a convex curved is observed, as opposed to the pipe-finding problem or the sine-wave finding problem with the `dcor2d_2`, `loess2d`, and `splines2d` index. When finding the sine-wave with the stringy index, the index shows an instantaneous jump to the optimum when close to the best basis."
sq_basis_dist_idx |>
  ggplot(aes(x = dist, y = idx, group = index)) +
  geom_line() +
  facet_nested_wrap(~index + n, nrow = 3, labeller = label_both, scales = "free_y") + 
  xlab("projection distance") + ylab("index value")

```

```{r}
#| label: tbl-mod-data
#| tbl-cap: The first few rows of the datasets processed for modelling. The smoothness and squintability variable are uniquely characterised by the index function used and the data dimension, and thus do not vary across `n_jellies` and `max_tries`. The variable `P_J_hat`, and time are calculated at each observation.
sim_df |> 
  dplyr::select(index, d, smoothness, squintability, n_jellies, max_tries, 
                -I_max_max, P_J_hat, time) |> 
  arrange(index, d, n_jellies, max_tries) |> 
  head(7) |> 
  kable(digits = 3)
```


```{r}
#| label: tbl-mod-output
#| tbl-cap: Model estimates of proportion of jellyfish success (`P_J_hat`) on optimisation properties (`smoothness`, `squintability`, and `d`) and jellyfish parameters (`long_time`, `n_jellies`, and `max_tries`) from the generalised linear model with a binomial family and a logit link function. The variable `long_time` is derived to flag the cases with average time spent on each run greater than 30 seconds and the variable `n_jellies` and `max_tries` are scaled by 10 for interpretation.
sim_df2 <- sim_df |> 
  mutate(n_jellies = n_jellies/10,
         max_tries = max_tries/10,
         long_time = ifelse(time > 30, 1, 0),
         P_J_hat = ifelse(index == "stringy", 0, P_J_hat))

mod1 <- glm(P_J_hat ~ smoothness + squintability + d + long_time + n_jellies + max_tries, data = sim_df2, family = binomial)
broom::tidy(mod1) |> 
  mutate(term = ifelse(term == "(Intercept)", "intercept", term)) |>
  kable(digits = 3) |> 
  column_spec(c(1,5), border_right = TRUE) |> 
  column_spec(1, border_left = TRUE)
```


# Conclusion \[Di and Sherry\] {#sec-conclusion}

# References
