% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  number,
  preprint,
  3p]{elsarticle}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{algorithm}
\usepackage{algpseudocode}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\journal{Journal of Multivariate Analysis}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{elsarticle-num-names}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Studying the Performance of the Jellyfish Optimiser for the Application of Projection Pursuit},
  pdfauthor={Alice Anonymous; Bob Security; Cat Memes; Derek Zoolander},
  pdfkeywords={projection pursuit, optimization, jellyfish
optimiser, data visualisation, high-dimensional data},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\setlength{\parindent}{6pt}
\begin{document}

\begin{frontmatter}
\title{Studying the Performance of the Jellyfish Optimiser for the
Application of Projection Pursuit}
\author[1]{Alice Anonymous%
\corref{cor1}%
}
 \ead{alice@example.com} 
\author[2]{Bob Security%
%
}
 \ead{bob@example.com} 
\author[2]{Cat Memes%
%
}
 \ead{cat@example.com} 
\author[]{Derek Zoolander%
%
}
 \ead{derek@example.com} 

\affiliation[1]{organization={Some Institute of Technology, Department
Name},addressline={Street Address},city={City},postcode={Postal
Code},postcodesep={}}
\affiliation[2]{organization={Another University, Department
Name},addressline={Street Address},city={City},postcode={Postal
Code},postcodesep={}}

\cortext[cor1]{Corresponding author}




        
\begin{abstract}
This is the abstract. Lorem ipsum dolor sit amet, consectetur adipiscing
elit. Vestibulum augue turpis, dictum non malesuada a, volutpat eget
velit. Nam placerat turpis purus, eu tristique ex tincidunt et. Mauris
sed augue eget turpis ultrices tincidunt. Sed et mi in leo porta
egestas. Aliquam non laoreet velit. Nunc quis ex vitae eros aliquet
auctor nec ac libero. Duis laoreet sapien eu mi luctus, in bibendum leo
molestie. Sed hendrerit diam diam, ac dapibus nisl volutpat vitae.
Aliquam bibendum varius libero, eu efficitur justo rutrum at. Sed at
tempus elit.
\end{abstract}





\begin{keyword}
    projection pursuit \sep optimization \sep jellyfish
optimiser \sep data visualisation \sep 
    high-dimensional data
\end{keyword}
\end{frontmatter}
    
\emph{Let's use British English (``American or British usage is
accepted, but not a mixture of these'')}

\begin{verbatim}
Warning: package 'ggplot2' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'tidyr' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'ggh4x' was built under R version 4.3.2
\end{verbatim}

\section{Introduction {[}Nicolas and
Jessica{]}}\label{introduction-nicolas-and-jessica}

The artificial jellyfish search (JS) algorithm \citep{chou_novel_2021}
is a swarm-based metaheuristic optimisation algorithm inspired by the
search behaviour of jellyfish in the ocean. It is one of the newest
swarm intelligence algorithms \citep{rajwar_exhaustive_2023}, which was
shown to have stronger search ability and faster convergence with few
algorithmic parameters compared to classic optimization methods
\citep{chou_novel_2021}-\citep{chou_recent_2022}.

Effective optimisation is an important aspect of many methods employed
for visualising high-dimensional data (\(X\)). Here we are concerned
about computing informative linear projections of high-dimensional
(\(p\)) data using projection pursuit (PP) (\citet{kr69}, \citet{FT74}).
This involves optimising a function (e.g. \citet{hall1989polynomial},
\citet{cook1993projection}, \citet{lee2010projection},
\citet{Loperfido2018}, \citet{Loperfido2020}), called the projection
pursuit index (PPI), that defines what is interesting or informative in
a projection.

These PPI are defined on projections (\(XA\)), which means that there is
a constraint that needs to be considered when optimising. A projection
of data is defined by a \(p\times d\) orthonormal matrix \(A\), and this
imposes the constraint on the elements of \(A\), that columns need have
norm equal to 1 and the product of columns need to sum to zero.

\citet{cook1995grand} introduced the PP guided tour, which enabled
interactive visualisation of the optimisation in order to visually
explore high-dimensional data. It is implemented in the R \citep{R}
package \texttt{tourr} \citep{tourr}. The optimisation that is
implemented is fairly basic, and potential problems were highlighted by
\citet{RJ-2021-105}. Implementing better optimisation functionality is a
goal, but it needs to be kept in mind that the guided tour also has
places importance on watching the projected data as the optimisation
progresses.

Here we explore the potential for a jellyfish optimisation to be
integrated with the guided tour. Section~\ref{sec-background} explains
the optimisation that is used in the current the projection pursuit
guided tour. Section~\ref{sec-theory} provides more details on the
jellyfish optimiser and formalises several characteristics of projection
pursuit indexes that are help to measure optimisaer performance.
Section~\ref{sec-simulation} describes a simulation study on performance
of the jellyfish for several types of data and index functions.
Section~\ref{sec-conclusion} summarises the work and provides
suggestions for future directions.

\section{Projection pursuit, index functions and optimisation {[}Di and
Sherry{]}}\label{sec-background}

A tour on high-dimensional data is constructed by geodesically
interpolating between pairs of planes. Any plane is described by an
orthonormal basis, \(A_t\), where \(t\) represents time in the sequence.
The term ``geodesic'' refers to maintaining the orthonormality
constraint so that each view shown is correctly a projection of the
data. The PP guided tour operates by geodesically interpolating to
target planes (projections) which have high PP index values, as provided
by the optimiser. The geodesic interpolation means that the viewer sees
a continuous sequence of projections of the data, so they can watch
patterns of interest forming as the function is optimised. There are
five optimisation methods implemented in the \texttt{tourr} package:

\begin{itemize}
\tightlist
\item
  \texttt{search\_geodesic()}: provides a pseudo-derivative
  optimisation. It searches locally for the best direction, based on
  differencing the index values for very close projections. Then it
  follows the direction along the geodesic path between planes, stopping
  when the next index value fails to increase.
\item
  \texttt{search\_better()}: is a brute-force optimisation searching
  randomly for projections with higher index values.
\item
  \texttt{search\_better\_random()}: is essentially simulated annealing
  \citep{Bertsimas93} where the search space is reduced as the
  optimisation progresses.
\item
  \texttt{search\_posse()}: implements the algorithm described in
  \citet{posse95}.
\item
  \texttt{search\_polish()}: is a very localised search, to take tiny
  steps to get closer to the local maximum.
\end{itemize}

There are several PP index functions available: \texttt{holes()} and
\texttt{cmass()} \citep{cook1993projection}; \texttt{lda\_pp()}
\citep{lee2005projection}; \texttt{pda\_pp()} \citep{lee2010projection};
\texttt{dcor2d()} and \texttt{splines2d()} \citep{Grimm2016};
\texttt{norm\_bin()} and \texttt{norm\_kol()} \citep{huber85};
\texttt{slice\_index()} \citep{Laa:2020wkm}. Most are relatively simply
defined, for any projection dimension, and implemented because they are
relatively easy to optimise. A goal is to be able to incorporate more
complex PP indexes, for example based on scagnostics (\citet{scag},
\citet{WW08}).

An initial investigation of PP indexes, and the potential for
scagnostics is described in \citet{laa_using_2020}. To be useful here an
optimiser needs to be able to handle functions which are not very
smooth. In addition, because data structures might be relatively fine,
the optimiser needs to be able to find maxima that occur with a small
squint angle, that can only be seen from very close by. One last aspect
that is useful is for an optimiser to return local maxima in addition to
global because data can contain many different and interesting features.

\section{The jellyfish optimiser and properties of PP indexes {[}Nicolas
and Jessica{]}}\label{sec-theory}

The jellyfish optimiser (JSO) mimics the natural movements of jellyfish,
which include passive and active motions driven by ocean currents and
their swimming patterns, respectively. In the context of optimization,
these movements are abstracted to explore the search space in a way that
balances exploration (searching new areas) and exploitation (focusing on
promising areas). The algorithm aims to find the optimal solution by
adapting the jellyfish's behaviour to navigate towards the best solution
over iterations \citep{chou_novel_2021}.

To understand what the jellyfish optimizer is doing in the context of
Projection Pursuit, we first start with a current projection (the
starting point). Then, we evaluate this projection using an index
function, which tells us how good the current projection is. We then
move the projection in a direction determined by the `best jelly' and
random factors, influenced by how far along we are in the optimization
process (the trial \(i\) and \texttt{max.tries}). Occasionally, we might
explore completely new directions like a jellyfish might with ocean
currents. Then, we compare new potential projections to our current one.
If they're better, we adopt them; if not, we stick with our current
projection. This process continues and iteratively improves the
projection, until we reach the maximum number of trials.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, title={Algorithm: Jellyfish Optimizer Pseudo Code}, toptitle=1mm, toprule=.15mm, colback=white, colframe=quarto-callout-note-color-frame, arc=.35mm, breakable, leftrule=.75mm, opacitybacktitle=0.6, bottomtitle=1mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, bottomrule=.15mm, left=2mm, opacityback=0]

\textbf{Input}: \texttt{current\_projections}, \texttt{index\_function},
\texttt{tries}, \texttt{max\_tries}

\textbf{Output}: \texttt{optimized\_projection}

\textbf{Initialize} \texttt{best\_jelly} as the projection with the best
index value from \texttt{current\_projections}, and
\texttt{current\_index} as the array of index values for each projection
in \texttt{current\_projections}

\textbf{for} each try in 1 to max\_tries \textbf{do}

\begin{quote}
Calculate \(c_t\) based on the current try and max\_tries
\end{quote}

\begin{quote}
\textbf{if} \(c_t\) is greater than or equal to \(0.5\) \textbf{then}

\begin{quote}
Define trend based on the best jelly and current projections
\end{quote}

\begin{quote}
Update each projection towards the trend using a random factor and
orthonormalisation
\end{quote}

\textbf{else}

\begin{quote}
\textbf{if} a random number is greater than \(1 - c_t\) \textbf{then}

\begin{quote}
Slightly adjust each projection with a small random factor (Type A
passive)
\end{quote}

\textbf{else}

\begin{quote}
For each projection, compare with a random jelly and adjust towards or
away from it (Type B active)
\end{quote}
\end{quote}

Update the orientation of each projection to maintain consistency

Evaluate the new projections using the index function
\end{quote}

\begin{quote}
\textbf{if} any new projection is worse than the current, revert to the
\texttt{current\_projection} for that case

\begin{quote}
Determine the projection with the best index value as the new
best\_jelly
\end{quote}
\end{quote}

\begin{quote}
\textbf{if} the try is the last one, print the final best projection and
\textbf{exit}
\end{quote}

\textbf{return} the set of projections with the updated best\_jelly as
the optimized\_projection

\end{tcolorbox}

The JSO implementation involves several key parameters that control its
search process in optimization problems. These parameters are designed
to guide the exploration and exploitation phases of the algorithm. While
the specific implementation details can vary depending on the version of
the algorithm or its application, we focus on two main parameters that
are most relevant to our application: the number of jellyfish and drift.

\citet{laa_using_2020} has proposed five criteria for assessing
projection pursuit indexes (smoothness, squintability, flexibility,
rotation invariance, and speed). Since not all the properties affects
the execution of the optimisation, here we consider the three relevant
properties (smoothness, squintability, and speed), and propose three
metrics to evaluate these three properties.

\subsection{Smoothness}\label{smoothness}

If we evaluate the index function at some random points (like the random
initialization of the jellyfish optimizer), then we can interpret these
random index values as a random field, indexed by a space parameter: the
random projection angle. This analogy suggests to use this random
training sample to fit a spatial model, a simple one being a (spatial)
Gaussian process.

How can we define a measure of smoothness from this? The distribution of
a Gaussian process is fully determined by its mean function and
covariance function. The way the covariance function is defined is where
smoothness comes into play: if an index is very smooth, then two close
projection angles should produce close index values (strong
correlation); by contrast, if an index is not smooth, then two close
projection angles might give very different index values (fast decay of
correlations with respect to distance between angles).

Popular covariance functions are parametric positive semi-definite
functions, some of which have a parameter to capture the smoothness of
the Gaussian field. In particular, consider the Matérn class of
covariance functions, defined by

\[
K(u):=\frac{(\sqrt{2\nu}u)^{\nu}}{\Gamma(\nu)2^{\nu-1}}\mathcal{K}_{\nu}(\sqrt{2\nu}u)
\]

where \(\nu>0\) is the smoothness parameter and where
\(\mathcal{K}_\nu\) is the modified Bessel function. The Matérn
covariance function can be expressed analytically when \(\nu\) is a
half-integer, the most popular values in the literature being \(1/2\),
\(3/2\) and \(5/2\) . The parameter \(\nu\), called smoothness
parameter, controls the decay of the covariance function. As such, it is
an appropriate measure of smoothness of a random field.

In our context, we suggest to use this parameter as a measure of the
smoothness of the index function by fitting a Gaussian process prior
with Matérn covariance on a dataset generated by random evaluations of
the index function, as in the initial stage of the jellyfish random
search. There exist several R packages, such as GpGp or ExaGeoStatR, to
fit the hyperparameters of a GP covariance function on data. In this
project, we make use of the GpGp package.

The fitted value \(\nu>0\) can be interpreted as follows: the higher
\(\nu\), the smoother the index function.

\subsection{Squintability}\label{squintability}

From the literature, it is commonly understood that a large squint angle
implies that the objective function value is close to optimal even when
we are not very close to the perfect view to see the structure. A small
squint angle means that index function value improves substantially only
when we are very close to the perfect view. As such, low squintability
implies rapid improvement in the index value when near the perfect view.

In this study, we propose two metrics to capture the notion of
squintability.

{[}We generate random points that is beyond 1.5 projection distance and
interpolate. Then we fit a kernel or use nonlinear least squares.{]}

First, parametric model.

{[}Nicolas's pdf{]}

Second, we consider the product of the largest absolute magnitude of
rate of change of \(f\) and the corresponding projection angle as a
second measure of squintability. Since \(f\) is decreasing, the rate of
change of \(f\) is negative and thus \(|\underset{x}{\min} f('x)|\)
gives the absolute magnitude of the most negative rate of change.

{[}Nicolas's pdf{]}

To the best of our knowledge, this is the first attempt to measure the
notion of squintability.

\subsection{Speed}\label{speed}

The speed of optimizing an index function can be calculated/measured
using the computational complexity (in big O notation, with respect to
the sample size) of computing the index function.

\section{Visualisation of jellyfish
optimiser}\label{visualisation-of-jellyfish-optimiser}

Information of the jellyfish optimiser is available in a tabular format
and below is an example data collected from finding the sine-wave
structure in 6D data using a distance correlation index
(\texttt{docr2d\_2}):

\begin{verbatim}
Rows: 275,000
Columns: 13
$ idx_f     <chr> "dcor2d_2", "dcor2d_2", "dcor2d_2", "dcor2d_2", "dcor2d_2", ~
$ d         <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~
$ n_jellies <dbl> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, ~
$ max_tries <dbl> 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, ~
$ sim       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
$ seed      <int> 3462, 3462, 3462, 3462, 3462, 3462, 3462, 3462, 3462, 3462, ~
$ basis     <list> <<matrix[6 x 2]>>, <<matrix[6 x 2]>>, <<matrix[6 x 2]>>, <<~
$ index_val <dbl> 0.0247212373, 0.0033938502, 0.0463398915, 0.0486230801, -0.0~
$ info      <chr> "initiation", "initiation", "initiation", "initiation", "ini~
$ method    <chr> "search_jellyfish", "search_jellyfish", "search_jellyfish", ~
$ tries     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
$ loop      <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1~
$ time      <drtn> 35.46031 secs, 35.46031 secs, 35.46031 secs, 35.46031 secs,~
\end{verbatim}

Information recorded can be categorised into the following categories:

\begin{itemize}
\tightlist
\item
  projection pursuit variables: the index function used
  (\texttt{idx\_f}), the data dimension (\texttt{d})
\item
  jellyfish optimiser parameters: the number of jellies
  (\texttt{n\_jellies}), the maximum number of tries
  (\texttt{max\_tries})
\item
  simulation variables: the simulation number (\texttt{sim}), the seed
  used (\texttt{seed})
\item
  optimisation variables: the projection basis in a matrix format
  (\texttt{basis}), the index value (\texttt{index\_val}), a description
  of the status - one of ``initiation'', ``current\_best'', and
  ``jellyfish\_update'' (\texttt{info}), current iteration ID
  (\texttt{tries}), current jelly ID (\texttt{loop}), and the time taken
  to find the optimum (\texttt{time}).
\end{itemize}

The basis column records every basis \emph{visited} by the jellyfish
optimiser prior to comparing with the current basis. In each iteration,
if the index value of a visited basis is smaller than that of the
current one, the jellyfish optimiser will retain the current basis for
the next iteration, while still documenting the visited basis.

Numerical information to compute:

\begin{itemize}
\tightlist
\item
  angular distance between the projection basis and the theoretical best
  basis,
\item
  the proportion of simulation that found the optimal basis,
\item
  the proportion of jellies, within each simulation, that found the
  optimal basis,
\end{itemize}

Visualisation to inspect:

\begin{itemize}
\tightlist
\item
  inspect the basis visited by each jellyfish in the reduced PCA space,
\item
  inspect the final 2D projections reached by each jellyfish,
\item
  plot the index value against the angular distance between the
  projection basis and the theoretical best basis
\end{itemize}

Plotting the basis in the space and the projected data can help to
understand 1) whether each simulation finds the same optimum or some
simulations find local optima; and 2) whether the index function used
can detect the structure in the data and the projection contains the
structure of interest.

The visualisations above can be faceted by the projection pursuit
variables and jellyfish optimiser parameters to compare the performance
of different indexes to detect the same structure and how the jellyfish
optimiser parameters affect the optimisation process.

{[}example plots{]}

\begin{figure}[H]

{\centering \includegraphics{optim_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\caption{sdfjsflk}

\end{figure}%

\section{Application {[}Di and Sherry{]}}\label{sec-simulation}

The jellyfish optimiser has been implemented in the tourr package
\citep{wickham_tourr_2011} and we will use the diagnostic plots proposed
in the ferrn package \citep{RJ-2021-105} to visualise the optimisation
process.

\subsection{Going beyond 10D}\label{going-beyond-10d}

The pipe-finding problem is initially used to investigate indexes and
optimisers in \citet{laa_using_2020}, and we extend it from a 6D problem
to a 12D problem.

Jellyfish optimiser, as a multi-start algorithm, is efficient in
{[}\ldots{]} for high-dimensional problems

\begin{figure}[H]

{\centering \includegraphics{optim_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\caption{sthis sdfaksdlf}

\end{figure}%

\begin{figure}[H]

{\centering \includegraphics{optim_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\caption{sthis sdfaksdlf}

\end{figure}%

\subsection{On skewness and kurtosis
index}\label{on-skewness-and-kurtosis-index}

\subsection{Another data example}\label{another-data-example}

construct a relationship between jellyfish success and jellyfish
parameters and the optimisation properties defined in
Section~\ref{sec-theory}.

This can imform the choice of jellyfish parameters for a given
optimisation problem.

In addition to the pipe-finding problem, we also consider the problem of
finding the sine wave structure in the 6D space and six indexes
(\texttt{dcor2d\_2}, \texttt{loess2d}, \texttt{MIC}, \texttt{TIC},
\texttt{spline}, and \texttt{stringy}) are considered. Combined with the
jellyfish optimisers (\texttt{n\_jellies} = 20/50/100,
\texttt{max\_tries} = 50/100), we obtain additional 27 setups for this
investigation.

\begin{longtable}[]{@{}lrrrrr@{}}
\toprule\noalign{}
index & n & variance & range & smoothness & nugget \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
dcor2d\_2 & 6 & 0.034 & 0.167 & 2.663 & 0.114 \\
loess2d & 6 & 0.083 & 0.307 & 2.194 & 0.292 \\
MIC & 6 & 0.016 & 0.100 & 2.394 & 0.087 \\
TIC & 6 & 0.124 & 0.104 & 2.471 & 0.086 \\
stringy & 6 & 0.000 & 1173.035 & 1.031 & 17608.047 \\
splines2d & 6 & 0.040 & 0.189 & 2.606 & 0.104 \\
MIC & 8 & 0.016 & 0.100 & 2.394 & 0.087 \\
TIC & 8 & 0.124 & 0.104 & 2.471 & 0.086 \\
holes & 6 & 0.002 & 0.408 & 2.364 & 0.212 \\
holes & 8 & 0.000 & 0.259 & 2.373 & 0.613 \\
holes & 10 & 0.000 & 0.144 & 2.317 & 1.831 \\
holes & 12 & 0.000 & 0.254 & 2.173 & 0.879 \\
\end{longtable}

\begin{longtable}[]{@{}lrrrrr@{}}
\toprule\noalign{}
index & n & theta1 & theta2 & theta3 & theta4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
holes & 6 & 1.001 & 0.860 & 3.368 & 0.823 \\
holes & 8 & 1.001 & 0.869 & 3.264 & 0.811 \\
holes & 10 & 1.000 & 0.885 & 3.151 & 0.806 \\
holes & 12 & 1.000 & 0.878 & 3.345 & 0.806 \\
MIC & 6 & 0.894 & 0.571 & 1.623 & -0.024 \\
MIC & 8 & 0.932 & 0.328 & 1.314 & -0.030 \\
TIC & 6 & 0.951 & 0.536 & 1.719 & -0.025 \\
TIC & 8 & 0.945 & 0.564 & 1.723 & -0.027 \\
dcor2d\_2 & 6 & 0.954 & 1.039 & 2.742 & -0.019 \\
loess2d & 6 & 1.016 & 1.039 & 2.648 & 0.080 \\
splines2d & 6 & 1.014 & 1.051 & 2.730 & -0.009 \\
stringy & 6 & 1.011 & 0.011 & 254.734 & 0.727 \\
\end{longtable}

\begin{figure}[H]

{\centering \includegraphics{optim_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\caption{sdkflskdfjl}

\end{figure}%

\section{Conclusion {[}Di and Sherry{]}}\label{sec-conclusion}


\renewcommand\refname{References}
  \bibliography{bibliography.bib}


\end{document}
